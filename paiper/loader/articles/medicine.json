{
   "name":"medicine",
   "articles":[
      {
         "id":29935653,
         "title":"Personalized medicine: motivation, challenges, and progress.",
         "abstract":"There is a great deal of hype surrounding the concept of personalized medicine. Personalized medicine is rooted in the belief that since individuals possess nuanced and unique characteristics at the molecular, physiological, environmental exposure, and behavioral levels, they may need to have interventions provided to them for diseases they possess that are tailored to these nuanced and unique characteristics. This belief has been verified to some degree through the application of emerging technologies such as DNA sequencing, proteomics, imaging protocols, and wireless health monitoring devices, which have revealed great inter-individual variation in disease processes. In this review, we consider the motivation for personalized medicine, its historical precedents, the emerging technologies that are enabling it, some recent experiences including successes and setbacks, ways of vetting and deploying personalized medicines, and future directions, including potential ways of treating individuals with fertility and sterility issues. We also consider current limitations of personalized medicine. We ultimately argue that since aspects of personalized medicine are rooted in biological realities, personalized medicine practices in certain contexts are likely to be inevitable, especially as relevant assays and deployment strategies become more efficient and cost-effective.",
         "relevant":true
      },
      {
         "id":23642060,
         "title":"Induced pluripotent stem cells (iPSCs) derived from different cell sources and their potential for regenerative and personalized medicine",
         "abstract":"Human induced pluripotent stem cells (hiPSCs) have great potential as a robust source of progenitors for regenerative medicine. The novel technology also enables the derivation of patient-specific cells for applications to personalized medicine, such as for personal drug screening and toxicology. However, the biological characteristics of iPSCs are not yet fully understood and their similarity to human embryonic stem cells (hESCs) is still unresolved. Variations among iPSCs, resulting from their original tissue or cell source, and from the experimental protocols used for their derivation, significantly affect epigenetic properties and differentiation potential. Here we review the potential of iPSCs for regenerative and personalized medicine, and assess their expression pattern, epigenetic memory and differentiation capabilities in relation to their parental tissue source. We also summarize the patient-specific iPSCs that have been derived for applications in biological research and drug discovery; and review risks that must be overcome in order to use iPSC technology for clinical applications.",
         "relevant":true
      },
      {
         "id":"10.3390/ijms20225760",
         "title":"Stem Cells: The Game Changers of Human Cardiac Disease Modelling and Regenerative Medicine",
         "abstract":"A comprehensive understanding of the molecular basis and mechanisms underlying cardiac diseases is mandatory for the development of new and effective therapeutic strategies. The lack of appropriate in vitro cell models that faithfully mirror the human disease phenotypes has hampered the understanding of molecular insights responsible of heart injury and disease development. Over the past decade, important scientific advances have revolutionized the field of stem cell biology through the remarkable discovery of reprogramming somatic cells into induced pluripotent stem cells (iPSCs). These advances allowed to achieve the long-standing ambition of modelling human disease in a dish and, more interestingly, paved the way for unprecedented opportunities to translate bench discoveries into new therapies and to come closer to a real and effective stem cell-based medicine. The possibility to generate patient-specific iPSCs, together with the new advances in stem cell differentiation procedures and the availability of novel gene editing approaches and tissue engineering, has proven to be a powerful combination for the generation of phenotypically complex, pluripotent stem cell-based cellular disease models with potential use for early diagnosis, drug screening, and personalized therapy. This review will focus on recent progress and future outcome of iPSCs technology toward a customized medicine and new therapeutic options.",
         "relevant":true
      },
      {
         "id":"10.3389/fendo.2020.00430",
         "title":"Prospect of Stem Cell Therapy and Regenerative Medicine in Osteoporosis",
         "abstract":"The field of cell therapy and regenerative medicine can hold the promise of restoring normal tissues structure and function. Additionally, the main targets of stem cell-based therapies are chronic diseases and lifelong disabilities without definite cures such as osteoporosis. Osteoporosis as one of the important causes of morbidity in older men and post-menopausal women is characterized by reduced bone quantity or skeletal tissue atrophy that leads to an increased risk of osteoporotic fractures. The common therapeutic methods for osteoporosis only can prevent the loss of bone mass and recover the bone partially. Nevertheless, stem cell-based therapy is considered as a new approach to regenerate the bone tissue. Herein, mesenchymal stem cells as pivotal candidates for regenerative medicine purposes especially bone regeneration are the most common type of cells with anti-inflammatory, immune-privileged potential, and less ethical concerns than other types of stem cells which are investigated in osteoporosis. Based on several findings, the mesenchymal stem cells effectiveness near to a great extent depends on their secretory function. Indeed, they can be involved in the establishment of normal bone remodeling via initiation of specific molecular signaling pathways. Accordingly, the aim herein was to review the effects of stem cell-based therapies in osteoporosis.",
         "relevant":true
      },
      {
         "id":"10.3390/jpm10010008",
         "title":"Harnessing the Potential of Stem Cells for Disease Modeling: Progress and Promises",
         "abstract":"Ex vivo cell/tissue-based models are an essential step in the workflow of pathophysiology studies, assay development, disease modeling, drug discovery, and development of personalized therapeutic strategies. For these purposes, both scientific and pharmaceutical research have adopted ex vivo stem cell models because of their better predictive power. As matter of a fact, the advancing in isolation and in vitro expansion protocols for culturing autologous human stem cells, and the standardization of methods for generating patient-derived induced pluripotent stem cells has made feasible to generate and investigate human cellular disease models with even greater speed and efficiency. Furthermore, the potential of stem cells on generating more complex systems, such as scaffold-cell models, organoids, or organ-on-a-chip, allowed to overcome the limitations of the two-dimensional culture systems as well as to better mimic tissues structures and functions. Finally, the advent of genome-editing/gene therapy technologies had a great impact on the generation of more proficient stem cell-disease models and on establishing an effective therapeutic treatment. In this review, we discuss important breakthroughs of stem cell-based models highlighting current directions, advantages, and limitations and point out the need to combine experimental biology with computational tools able to describe complex biological systems and deliver results or predictions in the context of personalized medicine.",
         "relevant":true
      },
      {
         "id":"10.1177/1557988315595693",
         "title":"Precision Medicine and Men's Health",
         "abstract":"Precision medicine can greatly benefit men's health by helping to prevent, diagnose, and treat prostate cancer, benign prostatic hyperplasia, infertility, hypogonadism, and erectile dysfunction. For example, precision medicine can facilitate the selection of men at high risk for prostate cancer for targeted prostate-specific antigen screening and chemoprevention administration, as well as assist in identifying men who are resistant to medical therapy for prostatic hyperplasia, who may instead require surgery. Precision medicine-trained clinicians can also let couples know whether their specific cause of infertility should be bypassed by sperm extraction and in vitro fertilization to prevent abnormalities in their offspring. Though precision medicine's role in the management of hypogonadism has yet to be defined, it could be used to identify biomarkers associated with individual patients' responses to treatment so that appropriate therapy can be prescribed. Last, precision medicine can improve erectile dysfunction treatment by identifying genetic polymorphisms that regulate response to medical therapies and by aiding in the selection of patients for further cardiovascular disease screening.",
         "relevant":true
      },
      {
         "id":"10.1007/978-3-030-16391-4_11",
         "title":"Artificial Intelligence and Personalized Medicine",
         "abstract":"The development of high-throughput, data-intensive biomedical research assays and technologies has created a need for researchers to develop strategies for analyzing, integrating, and interpreting the massive amounts of data they generate. Although a wide variety of statistical methods have been designed to accommodate 'big data,' experiences with the use of artificial intelligence (AI) techniques suggest that they might be particularly appropriate. In addition, the results of the application of these assays reveal a great heterogeneity in the pathophysiologic factors and processes that contribute to disease, suggesting that there is a need to tailor, or 'personalize,' medicines to the nuanced and often unique features possessed by individual patients. Given how important data-intensive assays are to revealing appropriate intervention targets and strategies for treating an individual with a disease, AI can play an important role in the development of personalized medicines. We describe many areas where AI can play such a role and argue that AI's ability to advance personalized medicine will depend critically on not only the refinement of relevant assays, but also on ways of storing, aggregating, accessing, and ultimately integrating, the data they produce. We also point out the limitations of many AI techniques in developing personalized medicines as well as consider areas for further research.",
         "relevant":true
      },
      {
         "id":"10.1558/cam.v12i1.25993",
         "title":"Using Chinese medicine in a Western way’: Negotiating integrative Chinese medicine treatment for Type 2 Diabetes",
         "abstract":"Type 2 diabetes affects Chinese Americans at an alarming rate and many Chinese Americans use Chinese medicine principles to deal with their diabetes. In this article, we examine interviews with Chinese medicine practitioners about the best ways to treat diabetes and xiaoke (Chinese medicine’s closest equivalent to diabetes). These interviews were conducted to examine how practitioners would promote a particular form of integrative medicine – in this case, using Chinese medicinal principles to suggest food treatments for diabetes or xiaoke. Our research expands understandings of integrative medicine and Chinese medicine recognizing that in practice, the static categories of Chinese and Western diagnosis and treatment are not very useful for understanding how integration occurs. Instead, Chinese medicine practitioners negotiate between the poles of individual and standardized on one dimension and physical and energetic on another dimension as a way of practicing Chinese medicine and enregistering their professional identities here in the U.S. Examining these interviews from a language and social interaction perspective, we present integration as a performance and enactment of social personae rather than a product. We highlight the need to attend to differences in what oftentimes is treated as a monolithic community of Chinese medicine in the U.S.",
         "relevant":true
      },
      {
         "id":22159338,
         "title":"[Phytomedicine in otorhinolaryngology - evidence-based medicine with medicinal plants]",
         "abstract":"Phytomedicine has become an increasingly important treatment option for patients in the western world. Patients who experienced failure or adverse reactions with conventional western medicine often switch to natural and holistic methods. In eastern countries, with their long history of traditional medicine, patients often resort to herbal preparations as the majority of western medicines are unaffordable. The desire of western physicians for evidence-based medicine also applies in the sector of phytomedicine. A serious perception of natural products in scientific medicine can therefore only be based on data from prospective, controlled, randomized double-blind clinical trials. In order to illuminate the present scientific foundation of effective and reliable phytomedicine, a literature search in PubMed (Medline) was conducted based on inclusion and exclusion criteria. The main focus was on the field of otorhinolaryngology. Besides the presentation of selected, reliable studies and the evaluation of the efficacy of various medicinal plants, shortcomings of selected publications are discussed.",
         "relevant":true
      },
      {
         "id":"10.1155/2019/8304260",
         "title":"Human Systems Biology and Metabolic Modelling: A Review-From Disease Metabolism to Precision Medicine",
         "abstract":"In cell and molecular biology, metabolism is the only system that can be fully simulated at genome scale. Metabolic systems biology offers powerful abstraction tools to simulate all known metabolic reactions in a cell, therefore providing a snapshot that is close to its observable phenotype. In this review, we cover the 15 years of human metabolic modelling. We show that, although the past five years have not experienced large improvements in the size of the gene and metabolite sets in human metabolic models, their accuracy is rapidly increasing. We also describe how condition-, tissue-, and patient-specific metabolic models shed light on cell-specific changes occurring in the metabolic network, therefore predicting biomarkers of disease metabolism. We finally discuss current challenges and future promising directions for this research field, including machine/deep learning and precision medicine. In the omics era, profiling patients and biological processes from a multiomic point of view is becoming more common and less expensive. Starting from multiomic data collected from patients and N-of-1 trials where individual patients constitute different case studies, methods for model-building and data integration are being used to generate patient-specific models. Coupled with state-of-the-art machine learning methods, this will allow characterizing each patient's disease phenotype and delivering precision medicine solutions, therefore leading to preventative medicine, reduced treatment, and in silico clinical trials.",
         "relevant":true
      },
      {
         "id":"10.1073/pnas.1706096114",
         "title":"Precision medicine screening using whole-genome sequencing and advanced imaging to identify disease risk in adults",
         "abstract":"Reducing premature mortality associated with age-related chronic diseases, such as cancer and cardiovascular disease, is an urgent priority. We report early results using genomics in combination with advanced imaging and other clinical testing to proactively screen for age-related chronic disease risk among adults. We enrolled active, symptom-free adults in a study of screening for age-related chronic diseases associated with premature mortality. In addition to personal and family medical history and other clinical testing, we obtained whole-genome sequencing (WGS), noncontrast whole-body MRI, dual-energy X-ray absorptiometry (DXA), global metabolomics, a new blood test for prediabetes (Quantose IR), echocardiography (ECHO), ECG, and cardiac rhythm monitoring to identify age-related chronic disease risks. Precision medicine screening using WGS and advanced imaging along with other testing among active, symptom-free adults identified a broad set of complementary age-related chronic disease risks associated with premature mortality and strengthened WGS variant interpretation. This and other similarly designed screening approaches anchored by WGS and advanced imaging may have the potential to extend healthy life among active adults through improved prevention and early detection of age-related chronic diseases (and their risk factors) associated with premature mortality.",
         "relevant":true
      },
      {
         "id":"10.1073/pnas.1909378117",
         "title":"Precision medicine integrating whole-genome sequencing, comprehensive metabolomics, and advanced imaging",
         "abstract":"Genome sequencing has established clinical utility for rare disease diagnosis. While increasing numbers of individuals have undergone elective genome sequencing, a comprehensive study surveying genome-wide disease-associated genes in adults with deep phenotyping has not been reported. Here we report the results of a 3-y precision medicine study with a goal to integrate whole-genome sequencing with deep phenotyping. A cohort of 1,190 adult participants (402 female [33.8%]; mean age, 54 y [range 20 to 89+]; 70.6% European) had whole-genome sequencing, and were deeply phenotyped using metabolomics, advanced imaging, and clinical laboratory tests in addition to family/medical history. Of 1,190 adults, 206 (17.3%) had at least 1 genetic variant with pathogenic (P) or likely pathogenic (LP) assessment that suggests a predisposition of genetic risk. A multidisciplinary clinical team reviewed all reportable findings for the assessment of genotype and phenotype associations, and 137 (11.5%) had genotype and phenotype associations. A high percentage of genotype and phenotype associations (>75%) was observed for dyslipidemia (n = 24), cardiomyopathy, arrhythmia, and other cardiac diseases (n = 42), and diabetes and endocrine diseases (n = 17). A lack of genotype and phenotype associations, a potential burden for patient care, was observed in 69 (5.8%) individuals with P/LP variants. Genomics and metabolomics associations identified 61 (5.1%) heterozygotes with phenotype manifestations affecting serum metabolite levels in amino acid, lipid and cofactor, and vitamin pathways. Our descriptive analysis provides results on the integration of whole-genome sequencing and deep phenotyping for clinical assessments in adults.",
         "relevant":true
      },
      {
         "id":"10.1146/annurev-med-041316-090905",
         "title":"Precision Medicine: Functional Advancements",
         "abstract":"Precision medicine was conceptualized on the strength of genomic sequence analysis. High-throughput functional metrics have enhanced sequence interpretation and clinical precision. These technologies include metabolomics, magnetic resonance imaging, and I rhythm (cardiac monitoring), among others. These technologies are discussed and placed in clinical context for the medical specialties of internal medicine, pediatrics, obstetrics, and gynecology. Publications in these fields support the concept of a higher level of precision in identifying disease risk. Precise disease risk identification has the potential to enable intervention with greater specificity, resulting in disease prevention-an important goal of precision medicine.",
         "relevant":true
      },
      {
         "id":"10.3109/10408363.2014.997930",
         "title":"Genomic medicine and risk prediction across the disease spectrum",
         "abstract":"Genomic medicine is based on the knowledge that virtually every medical condition, disease susceptibility or response to treatment is caused, regulated or influenced by genes. Genetic testing may therefore add value across the disease spectrum, ranging from single-gene disorders with a Mendelian inheritance pattern to complex multi-factorial diseases. The critical factors for genomic risk prediction are to determine: (1) where the genomic footprint of a particular susceptibility or dysfunction resides within this continuum, and (2) to what extent the genetic determinants are modified by environmental exposures. Regarding the small subset of highly penetrant monogenic disorders, a positive family history and early disease onset are mostly sufficient to determine the appropriateness of genetic testing in the index case and to inform pre-symptomatic diagnosis in at-risk family members. In more prevalent polygenic non-communicable diseases (NCDs), the use of appropriate eligibility criteria is required to ensure a balance between benefit and risk. An additional screening step may therefore be necessary to identify individuals most likely to benefit from genetic testing. This need provided the stimulus for the development of a pathology-supported genetic testing (PSGT) service as a new model for the translational implementation of genomic medicine in clinical practice. PSGT is linked to the establishment of a research database proven to be an invaluable resource for the validation of novel and previously described gene-disease associations replicated in the South African population for a broad range of NCDs associated with increased cardio-metabolic risk. The clinical importance of inquiry concerning family history in determining eligibility for personalized genotyping was supported beyond its current limited role in diagnosing or screening for monogenic subtypes of NCDs. With the recent introduction of advanced microarray-based breast cancer subtyping, genetic testing has extended beyond the genome of the host to also include tumor gene expression profiling for chemotherapy selection. The decreasing cost of next generation sequencing over recent years, together with improvement of both laboratory and computational protocols, enables the mapping of rare genetic disorders and discovery of shared genetic risk factors as novel therapeutic targets across diagnostic boundaries. This article reviews the challenges, successes, increasing inter-disciplinary integration and evolving strategies for extending PSGT towards exome and whole genome sequencing (WGS) within a dynamic framework. Specific points of overlap are highlighted between the application of PSGT and exome or WGS, as the next logical step in genetically uncharacterized patients for whom a particular disease pattern and/or therapeutic failure are not adequately accounted for during the PSGT pre-screen. Discrepancies between different next generation sequencing platforms and low concordance among variant-calling pipelines caution against offering exome or WGS as a stand-alone diagnostic approach. The public reference human genome sequence (hg19) contains minor alleles at more than 1 million loci and variant calling using an advanced major allele reference genome sequence is crucial to ensure data integrity. Understanding that genomic risk prediction is not deterministic but rather probabilistic provides the opportunity for disease prevention and targeted treatment in a way that is unique to each individual patient.",
         "relevant":true
      },
      {
         "id":"10.1016/j.revmed.2016.12.008",
         "title":"[Evolutionary medicine: A new look on health and disease]",
         "abstract":"Evolutionary medicine represents an innovative approach deriving from evolutionary biology. It includes the initial Darwin's view, its actualization in the light of progresses in genetics and also dissident theories (i.e. non gene-based) particularly epigenetics. This approach enables us to reconsider the pathophysiology of numerous diseases, as for instance, infection, and our so-called diseases of civilization especially obesity, type 2 diabetes, allergy or cancer. Evolutionary medicine may also improve our knowledge regarding inter-individual variation in susceptibility to disease or drugs. Furthermore, it points out the impact of our behaviors and environment on the genesis of a series of diseases.",
         "relevant":true
      },
      {
         "id":"10.1016/j.biocel.2018.12.009",
         "title":"RNA splicing analysis in genomic medicine",
         "abstract":"High-throughput next-generation sequencing technologies have led to a rapid increase in the number of sequence variants identified in clinical practice via diagnostic genetic tests. Current bioinformatic analysis pipelines fail to take adequate account of the possible splicing effects of such variants, particularly where variants fall outwith canonical splice site sequences, and consequently the pathogenicity of such variants may often be missed. The regulation of splicing is highly complex and as a result, in silico prediction tools lack sufficient sensitivity and specificity for reliable use. Variants of all kinds can be linked to aberrant splicing in disease and the need for correct identification and diagnosis grows ever more crucial as novel splice-switching antisense oligonucleotide therapies start to enter clinical usage. RT-PCR provides a useful targeted assay of the splicing effects of identified variants, while minigene assays, massive parallel reporter assays and animal models can also be used for more detailed study of a particular splicing system, given enough time and resources. However, RNA-sequencing (RNA-seq) has the potential to be used as a rapid diagnostic tool in genomic medicine. By utilising data science approaches and machine learning, it may prove possible to finally understand and interpret the 'splicing code' and apply this knowledge in human disease diagnostics.",
         "relevant":true
      },
      {
         "id":"10.1038/s41540-019-0115-2",
         "title":"A network-based approach to uncover microRNA-mediated disease comorbidities and potential pathobiological implications",
         "abstract":"Disease-disease relationships (e.g., disease comorbidities) play crucial roles in pathobiological manifestations of diseases and personalized approaches to managing those conditions. In this study, we develop a network-based methodology, termed meta-path-based Disease Network (mpDisNet) capturing algorithm, to infer disease-disease relationships by assembling four biological networks: disease-miRNA, miRNA-gene, disease-gene, and the human protein-protein interactome. mpDisNet is a meta-path-based random walk to reconstruct the heterogeneous neighbors of a given node. mpDisNet uses a heterogeneous skip-gram model to solve the network representation of the nodes. We find that mpDisNet reveals high performance in inferring clinically reported disease-disease relationships, outperforming that of traditional gene/miRNA-overlap approaches. In addition, mpDisNet identifies network-based comorbidities for pulmonary diseases driven by underlying miRNA-mediated pathobiological pathways (i.e., hsa-let-7a- or hsa-let-7b-mediated airway epithelial apoptosis and pro-inflammatory cytokine pathways) as derived from the human interactome network analysis. The mpDisNet offers a powerful tool for network-based identification of disease-disease relationships with miRNA-mediated pathobiological pathways.",
         "relevant":true
      },
      {
         "id":"10.16288/j.yczz.15-239",
         "title":"Use of genome editing tools in human stem cell-based disease modeling and precision medicine",
         "abstract":"Precision medicine emerges as a new approach that takes into account individual variability. The successful conduct of precision medicine requires the use of precise disease models. Human pluripotent stem cells (hPSCs), as well as adult stem cells, can be differentiated into a variety of human somatic cell types that can be used for research and drug screening. The development of genome editing technology over the past few years, especially the CRISPR/Cas system, has made it feasible to precisely and efficiently edit the genetic background. Therefore, disease modeling by using a combination of human stem cells and genome editing technology has offered a new platform to generate \" personalized \" disease models, which allow the study of the contribution of individual genetic variabilities to disease progression and the development of precise treatments. In this review, recent advances in the use of genome editing in human stem cells and the generation of stem cell models for rare diseases and cancers are discussed.",
         "relevant":true
      },
      {
         "id":"10.1007/978-3-319-63904-8_7",
         "title":"Target Discovery for Precision Medicine Using High-Throughput Genome Engineering",
         "abstract":"Over the past few years, programmable RNA-guided nucleases such as the CRISPR/Cas9 system have ushered in a new era of precision genome editing in diverse model systems and in human cells. Functional screens using large libraries of RNA guides can interrogate a large hypothesis space to pinpoint particular genes and genetic elements involved in fundamental biological processes and disease-relevant phenotypes. Here, we review recent high-throughput CRISPR screens (e.g. loss-of-function, gain-of-function, and targeting noncoding elements) and highlight their potential for uncovering novel therapeutic targets, such as those involved in cancer resistance to small molecular drugs and immunotherapies, tumor evolution, infectious disease, inborn genetic disorders, and other therapeutic challenges.",
         "relevant":true
      },
      {
         "id":"10.1016/j.semcancer.2018.04.001",
         "title":"CRISPR/Cas9 for cancer research and therapy",
         "abstract":"CRISPR/Cas9 has become a powerful method for making changes to the genome of many organisms. First discovered in bacteria as part of an adaptive immune system, CRISPR/Cas9 and modified versions have found a widespread use to engineer genomes and to activate or to repress the expression of genes. As such, CRISPR/Cas9 promises to accelerate cancer research by providing an efficient technology to dissect mechanisms of tumorigenesis, identify targets for drug development, and possibly arm cells for cell-based therapies. Here, we review current applications of the CRISPR/Cas9 technology for cancer research and therapy. We describe novel Cas9 variants and how they are used in functional genomics to discover novel cancer-specific vulnerabilities. Furthermore, we highlight the impact of CRISPR/Cas9 in generating organoid and mouse models of cancer. Finally, we provide an overview of the first clinical trials that apply CRISPR/Cas9 as a therapeutic approach against cancer.",
         "relevant":true
      },
      {
         "id":"10.1093/bfgp/ely011",
         "title":"The application of CRISPR-Cas9 genome editing tool in cancer immunotherapy",
         "abstract":"Clustered regularly interspaced short palindromic repeats (CRISPR)-associated protein 9 (CRISPR-Cas9) system was originally discovered in prokaryotes functioned as a part of the adaptive immune system. Because of its high efficiency and easy operability, CRISPR-Cas9 system has been developed to be a powerful and versatile gene editing tool shortly after its discovery. Given that multiple genetic alterations are the main factors that drive genesis and development of tumor, CRISPR-Cas9 system has been applied to correct cancer-causing gene mutations and deletions and to engineer immune cells, such as chimeric antigen receptor T (CAR T) cells, for cancer immunotherapeutic applications. Recently, CRISPR-Cas9-based CAR T-cell preparation has been an important breakthrough in antitumor therapy. Here, we summarize the mechanism, delivery and the application of CRISPR-Cas9 in gene editing, and discuss the challenges and future directions of CRISPR-Cas9 in cancer immunotherapy.",
         "relevant":true
      },
      {
         "id":"10.1016/j.canlet.2019.01.017",
         "title":"CRISPR-Cas9 for cancer therapy: Opportunities and challenges",
         "abstract":"Cancer is a genetic disease stemming from cumulative genetic/epigenetic aberrations. Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-Cas9-mediated genome editing technology has been extensively applied in various cell types and organisms, both in vitro and in vivo, for efficient gene disruption and gene modification. CRISPR-Cas9 has shown great promise for the treatment of cancer. However, despite its advantages and tremendous potential, numerous challenges, such as fitness of edited cells, editing efficiency, delivery methods and potential off-target effects, remain to be solved for completely clinical application. Here, we present the potential applications and recent advances of CRISPR-Cas9 in cancer therapy, and discuss the challenges that might be encountered in clinical applications.",
         "relevant":true
      },
      {
         "id":"10.1007/s13238-017-0410-x",
         "title":"Advancing chimeric antigen receptor T cell therapy with CRISPR/Cas9",
         "abstract":"The clustered regularly interspaced short palindromic repeats (CRISPR)/CRISPR-associated 9 (CRISPR/Cas9) system, an RNA-guided DNA targeting technology, is triggering a revolution in the field of biology. CRISPR/Cas9 has demonstrated great potential for genetic manipulation. In this review, we discuss the current development of CRISPR/Cas9 technologies for therapeutic applications, especially chimeric antigen receptor (CAR) T cell-based adoptive immunotherapy. Different methods used to facilitate efficient CRISPR delivery and gene editing in T cells are compared. The potential of genetic manipulation using CRISPR/Cas9 system to generate universal CAR T cells and potent T cells that are resistant to exhaustion and inhibition is explored. We also address the safety concerns associated with the use of CRISPR/Cas9 gene editing and provide potential solutions and future directions of CRISPR application in the field of CAR T cell immunotherapy. As an integration-free gene insertion method, CRISPR/Cas9 holds great promise as an efficient gene knock-in platform. Given the tremendous progress that has been made in the past few years, we believe that the CRISPR/Cas9 technology holds immense promise for advancing immunotherapy.",
         "relevant":true
      },
      {
         "id":"10.1007/s12272-018-1029-z",
         "title":"High-throughput genetic screens using CRISPR-Cas9 system",
         "abstract":"The CRISPR-Cas9 system is a powerful tool for genome engineering, and its programmability and simplicity have enabled various types of gene manipulation such as gene disruption and transcriptional and epigenetic perturbation. Particularly, CRISPR-based pooled libraries facilitate high-throughput screening for functional regulatory elements in the human genome. In this review, we describe recent advances in CRISPR-Cas9 technology and its use in high-throughput genetic screening. We also discuss its potential for drug target discovery and current challenges of this technique in biomedical research.",
         "relevant":true
      },
      {
         "id":"10.1016/j.gde.2018.06.001",
         "title":"Decoding the noncoding genome via large-scale CRISPR screens",
         "abstract":"Large portions of the human genome harbor functional noncoding elements, which can regulate a variety of biological processes and have important implications for disease risk and therapeutic outcomes. However, assigning specific functions to noncoding sequences remains a major challenge. Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-CRISPR-associated protein (Cas) systems have emerged as a powerful approach for targeted genome and epigenome perturbation. CRISPR systems are now harnessed for high-throughput screening of the noncoding genome to uncover functional regulatory elements and to define their precise functions with superior speed. Here, we summarize the various tools developed for such screens in mammalian systems and discuss screening methods and technical considerations. We further highlight screens that are already transforming our understanding of gene regulation and disease mechanisms, consider the impact of such discoveries on the development of new therapeutics, and provide our viewpoint on the challenges for future development of the field.",
         "relevant":true
      },
      {
         "id":"10.14423/SMJ.0000000000000195",
         "title":"Fixed drug eruptions: presentation, diagnosis, and management",
         "abstract":"Fixed drug eruption (FDE) is a well-defined, circular, hyperpigmenting plaque that recurs as one or a few lesions always in fixed locations upon ingestion of a drug. FDE commonly occurs on the genitals, lips, trunk, and hands. Although the lesions are distinctive, the diagnosis of FDE often is missed because it shares none of the characteristics of more common morbilliform drug rashes. The diagnosis can be confirmed by histopathologic examination of a small punch biopsy specimen. Drug avoidance is the mainstay of treatment, and antihistamines can reduce associated pruritus. Raising awareness of this condition will increase the likelihood of prompt diagnosis leading to resolution within days to weeks after the offending drug is discontinued.",
         "relevant":true
      },
      {
         "id":"10.2174/1389450117666160401124624",
         "title":"Smart Drug Delivery Systems in Cancer Therapy",
         "abstract":"Background: Smart nanocarriers have been designed for tissue-specific targeted drug delivery, sustained or triggered drug release and co-delivery of synergistic drug combinations to develop safer and more efficient therapeutics.\n\nObjective: Advances in drug delivery systems provide reduced side effects, longer circulation half-life and improved pharmacokinetics.\n\nResults: Smart drug delivery systems have been achieved successfully in the case of cancer. These nanocarriers can serve as an intelligent system by considering the differences of tumor microenvironment from healthy tissue, such as low pH, low oxygen level, or high enzymatic activity of matrix metalloproteinases.\n\nConclusion: The performance of anti-cancer agents used in cancer diagnosis and therapy is improved by enhanced cellular internalization of smart nanocarriers and controlled drug release. Here, we review targeting, cellular internalization; controlled drug release and toxicity of smart drug delivery systems. We are also emphasizing the stimulus responsive controlled drug release from smart nanocarriers.\n\nKeywords: Smart drug delivery; nanocarriers; smart nanocarriers; stimulus responsive drug release; targeted cancer therapy; targeted durg delivery.",
         "relevant":true
      },
      {
         "id":"10.7150/thno.23459",
         "title":"Endogenous pH-responsive nanoparticles with programmable size changes for targeted tumor therapy and imaging applications",
         "abstract":"Nanotechnology-based antitumor drug delivery systems, known as nanocarriers, have demonstrated their efficacy in recent years. Typically, the size of the nanocarriers is around 100 nm. It is imperative to achieve an optimum size of these nanocarriers which must be designed uniquely for each type of delivery process. For pH-responsive nanocarriers with programmable size, changes in pH (~6.5 for tumor tissue, ~5.5 for endosomes, and ~5.0 for lysosomes) may serve as an endogenous stimulus improving the safety and therapeutic efficacy of antitumor drugs. This review focuses on current advanced pH-responsive nanocarriers with programmable size changes for anticancer drug delivery. In particular, pH-responsive mechanisms for nanocarrier retention at tumor sites, size reduction for penetrating into tumor parenchyma, escaping from endo/lysosomes, and swelling or disassembly for drug release will be highlighted. Additional trends and challenges of employing these nanocarriers in future clinical applications are also addressed.",
         "relevant":true
      },
      {
         "id":"10.1016/j.biomaterials.2015.02.001",
         "title":"Hierarchical targeted hepatocyte mitochondrial multifunctional chitosan nanoparticles for anticancer drug delivery",
         "abstract":"The overwhelming majority of drugs exert their pharmacological effects after reaching their target sites of action, however, these target sites are mainly located in the cytosol or intracellular organelles. Consequently, delivering drugs to the specific organelle is the key to achieve maximum therapeutic effects and minimum side-effects. In the work reported here, we designed, synthesized, and evaluated a novel mitochondrial-targeted multifunctional nanoparticles (MNPs) based on chitosan derivatives according to the physiological environment of the tumor and the requirement of mitochondrial targeting drug delivery. The intelligent chitosan nanoparticles possess various functions such as stealth, hepatocyte targeting, multistage pH-response, lysosomal escape and mitochondrial targeting, which lead to targeted drug release after the progressively shedding of functional groups, thus realize the efficient intracellular delivery and mitochondrial localization, inhibit the growth of tumor, elevate the antitumor efficacy, and reduce the toxicity of anticancer drugs. It provides a safe and efficient nanocarrier platform for mitochondria targeting anticancer drug delivery.",
         "relevant":true
      },
      {
         "id":"10.1016/j.ejps.2014.05.021",
         "title":"Synthesis of Doxorubicin loaded magnetic chitosan nanoparticles for pH responsive targeted drug delivery",
         "abstract":"Targeted drug delivery is a promising alternative to overcome the limitations of classical chemotherapy. In an ideal targeted drug delivery system carrier nanoparticles would be directed to the tumor tissue and selectively release therapeutic molecules. As a novel approach, chitosan coated magnetic nanoparticles (CS MNPs) maintain a pH dependent drug delivery which provides targeting of drugs to the tumor site under a magnetic field. Among various materials, chitosan has a great importance as a pH sensitive, natural, biodegradable, biocompatible and bioadhesive polymer. The aim of this study was to obtain an effective targeted delivery system for Doxorubicin, using chitosan coated MNPs. Different sized CS MNPs were produced by in situ synthesis method. The anti-cancer agent Doxorubicin was loaded onto CS MNPs which were characterized previously. Doxorubicin loading was confirmed by FTIR. Drug loading and release characteristics, and stability of the nanoparticles were investigated. Our results showed that the CS MNPs have pH responsive release characteristics. The cellular internalization of Doxorubicin loaded CS MNPs were visualized by fluorescent microscopy. Doxorubicin loaded CS MNPs are efficiently taken up by MCF-7 (MCF-7/S) and Doxorubicin resistant MCF-7 (MCF-7/1 μM) breast cancer cells, which increases the efficacy of drug and also maintains overcoming the resistance of Doxorubicin in MCF-7/Dox cells. Consequently, CS MNPs synthesized at various sizes can be effectively used for the pH dependent release of Doxorubicin in cancer cells. Results of this study can provide new insights in the development of pH responsive targeted drug delivery systems to overcome the side effects of conventional chemotherapy.",
         "relevant":true
      },
      {
         "id":"10.1097/MJT.0000000000000066",
         "title":"Polyhydroxybutyrate-coated magnetic nanoparticles for doxorubicin delivery: cytotoxic effect against doxorubicin-resistant breast cancer cell line",
         "abstract":"In this study, polyhydroxybutyrate (PHB)-coated magnetic nanoparticles (MNPs) were prepared by coprecipitation of iron salts (Fe and Fe) by ammonium hydroxide. Characterizations of PHB-coated MNPs were performed by Fourier transform infrared spectroscopy, x-ray diffraction, dynamic light scattering, thermal gravimetric analysis, vibrating sample magnetometry, and transmission electron microscopy analyses. Doxorubicin was loaded onto PHB-MNPs, and the release efficiencies at different pHs were studied under in vitro conditions. The most efficient drug loading concentration was found about 87% at room temperature in phosphate-buffered saline (pH 7.2). The drug-loaded MNPs were stable up to 2 months in neutral pH for mimicking physiological conditions. The drug release studies were performed with acetate buffer (pH 4.5) that mimics endosomal pH. Doxorubicin (60%) released from PHB-MNPs within 65 hours. Doxorubicin-loaded PHB-MNPs were about 2.5-fold more cytotoxic as compared with free drug on resistant Michigan Cancer Foundation-7 (human breast adenocarcinoma, MCF-7) cell line (1 μM doxorubicin) in vitro. Therefore, doxorubicin-loaded PHB-MNPs lead to overcome the drug resistance.",
         "relevant":true
      },
      {
         "id":"10.1016/j.ijbiomac.2019.01.137",
         "title":"Novel chitosan-quinoline nanoparticles as anticancer drug nanocarriers were prepared using 2-chloro-3-formylquinoline and 3-formylquinolin-2(1H)-one as non-toxic modifying agents via oil-in-water nanoemulsion technique. Chitosan-quinoline nanoparticles were characterized by FT-IR, UV-vis spectrophotometry, XRD, SEM, AFM and DLS techniques. The morphological and particle size studies demonstrated that drug-loaded chitosan-quinoline nanoparticles have a regular nanorod shape and monolithic structure with the desired particle size of 141 to 174.8 nm and a negative zeta potential of -2.4 to -14.1 mV. Drug loading capacity (LC) and encapsulation efficiency (EE) were achieved using quercetin as a hydrophobic anticancer drug and were about 4.8-9.6% and 65.8-77%, respectively. The in vitro release studies displayed great pH-sensitive release behavior. Evaluation of the anticancer efficacy of quercetin loaded chitosan-quinoline nanoparticles using the in vitro cytotoxicity studies against HeLa cells indicated that the chitosan nanoparticles are a promising candidate for the anticancer drugs delivery.",
         "abstract":"Novel chitosan-quinoline nanoparticles as anticancer drug nanocarriers were prepared using 2-chloro-3-formylquinoline and 3-formylquinolin-2(1H)-one as non-toxic modifying agents via oil-in-water nanoemulsion technique. Chitosan-quinoline nanoparticles were characterized by FT-IR, UV-vis spectrophotometry, XRD, SEM, AFM and DLS techniques. The morphological and particle size studies demonstrated that drug-loaded chitosan-quinoline nanoparticles have a regular nanorod shape and monolithic structure with the desired particle size of 141 to 174.8 nm and a negative zeta potential of -2.4 to -14.1 mV. Drug loading capacity (LC) and encapsulation efficiency (EE) were achieved using quercetin as a hydrophobic anticancer drug and were about 4.8-9.6% and 65.8-77%, respectively. The in vitro release studies displayed great pH-sensitive release behavior. Evaluation of the anticancer efficacy of quercetin loaded chitosan-quinoline nanoparticles using the in vitro cytotoxicity studies against HeLa cells indicated that the chitosan nanoparticles are a promising candidate for the anticancer drugs delivery.",
         "relevant":true
      },
      {
         "id":"10.2147/IJN.S236927",
         "title":"Nanocarrier-Based Therapeutics and Theranostics Drug Delivery Systems for Next Generation of Liver Cancer Nanodrug Modalities",
         "abstract":"The development of therapeutics and theranostic nanodrug delivery systems have posed a challenging task for the current researchers due to the requirement of having various nanocarriers and active agents for better therapy, imaging, and controlled release of drugs efficiently in one platform. The conventional liver cancer chemotherapy has many negative effects such as multiple drug resistance (MDR), high clearance rate, severe side effects, unwanted drug distribution to the specific site of liver cancer and low concentration of drug that finally reaches liver cancer cells. Therefore, it is necessary to develop novel strategies and novel nanocarriers that will carry the drug molecules specific to the affected cancerous hepatocytes in an adequate amount and duration within the therapeutic window. Therapeutics and theranostic systems have advantages over conventional chemotherapy due to the high efficacy of drug loading or drug encapsulation efficiency, high cellular uptake, high drug release, and minimum side effects. These nanocarriers possess high drug accumulation in the tumor area while minimizing toxic effects on healthy tissues. This review focuses on the current research on nanocarrier-based therapeutics and theranostic drug delivery systems excluding the negative consequences of nanotechnology in the field of drug delivery systems. However, clinical developments of theranostics nanocarriers for liver cancer are considered outside of the scope of this article. This review discusses only the recent developments of nanocarrier-based drug delivery systems for liver cancer therapy and diagnosis. The negative consequences of individual nanocarrier in the drug delivery system will also not be covered in this review.",
         "relevant":true
      },
      {
         "id":"10.1016/j.drudis.2018.02.001",
         "title":"Cell membrane-coated nanocarriers: the emerging targeted delivery system for cancer theranostics",
         "abstract":"Cancer is a leading cause of death worldwide. The use of nanocarriers (NCs) has generated significant interest to improve cancer therapy by targeted delivery. However, conventional NCs in general lack specificity and have poor biodistribution, resulting in low efficacy in cancer therapy. To circumvent this problem, there has been an increasing focus on cancer cell membrane-coated NCs (CCMCNCs), which can deliver therapeutics directly to tumor cells. CCMCNCs comprise active cancer cell surface adhesive molecules combined with other functional proteins, and offer extended blood circulation with robust cell-specific targeting, ensuring enhanced intratumoral penetration and higher tumor-specific accumulation of NCs. In this review, we discuss the preparation, homologous targeting mechanisms, and application of CCMCNCs in targeted cancer therapy.",
         "relevant":true
      },
      {
         "id":"10.1016/j.ejpb.2015.03.018",
         "title":"Advanced targeted therapies in cancer: Drug nanocarriers, the future of chemotherapy",
         "abstract":"Cancer is the second worldwide cause of death, exceeded only by cardiovascular diseases. It is characterized by uncontrolled cell proliferation and an absence of cell death that, except for hematological cancers, generates an abnormal cell mass or tumor. This primary tumor grows thanks to new vascularization and, in time, acquires metastatic potential and spreads to other body sites, which causes metastasis and finally death. Cancer is caused by damage or mutations in the genetic material of the cells due to environmental or inherited factors. While surgery and radiotherapy are the primary treatment used for local and non-metastatic cancers, anti-cancer drugs (chemotherapy, hormone and biological therapies) are the choice currently used in metastatic cancers. Chemotherapy is based on the inhibition of the division of rapidly growing cells, which is a characteristic of the cancerous cells, but unfortunately, it also affects normal cells with fast proliferation rates, such as the hair follicles, bone marrow and gastrointestinal tract cells, generating the characteristic side effects of chemotherapy. The indiscriminate destruction of normal cells, the toxicity of conventional chemotherapeutic drugs, as well as the development of multidrug resistance, support the need to find new effective targeted treatments based on the changes in the molecular biology of the tumor cells. These novel targeted therapies, of increasing interest as evidenced by FDA-approved targeted cancer drugs in recent years, block biologic transduction pathways and/or specific cancer proteins to induce the death of cancer cells by means of apoptosis and stimulation of the immune system, or specifically deliver chemotherapeutic agents to cancer cells, minimizing the undesirable side effects. Although targeted therapies can be achieved directly by altering specific cell signaling by means of monoclonal antibodies or small molecules inhibitors, this review focuses on indirect targeted approaches that mainly deliver chemotherapeutic agents to molecular targets overexpressed on the surface of tumor cells. In particular, we offer a detailed description of different cytotoxic drug carriers, such as liposomes, carbon nanotubes, dendrimers, polymeric micelles, polymeric conjugates and polymeric nanoparticles, in passive and active targeted cancer therapy, by enhancing the permeability and retention or by the functionalization of the surface of the carriers, respectively, emphasizing those that have received FDA approval or are part of the most important clinical studies up to date. These drug carriers not only transport the chemotherapeutic agents to tumors, avoiding normal tissues and reducing toxicity in the rest of the body, but also protect cytotoxic drugs from degradation, increase the half-life, payload and solubility of cytotoxic agents and reduce renal clearance. Despite the many advantages of all the anticancer drug carriers analyzed, only a few of them have reached the FDA approval, in particular, two polymer-protein conjugates, five liposomal formulations and one polymeric nanoparticle are available in the market, in contrast to the sixteen FDA approval of monoclonal antibodies. However, there are numerous clinical trials in progress of polymer-protein and polymer-drug conjugates, liposomal formulations, including immunoliposomes, polymeric micelles and polymeric nanoparticles. Regarding carbon nanotubes or dendrimers, there are no FDA approvals or clinical trials in process up to date due to their unresolved toxicity. Moreover, we analyze in detail the more promising and advanced preclinical studies of the particular case of polymeric nanoparticles as carriers of different cytotoxic agents to active and passive tumor targeting published in the last 5 years, since they have a huge potential in cancer therapy, being one of the most widely studied nano-platforms in this field in the last years. The interest that these formulations have recently achieved is stressed by the fact that 90% of the papers based on cancer therapeutics with polymeric nanoparticles have been published in the last 6 years (PubMed search).",
         "relevant":true
      },
      {
         "id":"10.2147/IJN.S194596",
         "title":"Needle-shaped amphoteric calix[4]arene as a magnetic nanocarrier for simultaneous delivery of anticancer drugs to the breast cancer cells",
         "abstract":"Background: Chemotherapy as an important tool for cancer treatment faces many obstacles such as multidrug resistance and adverse toxic effects on healthy tissues. Drug delivery systems has opened a new window to overcome these problems. There has been a strong interest development of new platform and system for delivof chemotherapeutic agents.\n\nPurpose: In the present study, a green synthesis method was chosen and performed for preparation of a novel amphoteric calix[4]arene (Calix) macrocycle with low toxicity to the human body.\n\nMaterials and methods: The amphoteric Calix was coated on the surface of Fe3O4 magnetic nanoparticles and used as a magnetic nanocarrier for simultaneous delivery of two anticancer agents, doxorubicin and methotrexate, against MCF7 cancer cells. Several chemical characterizations were done for validation of prepared nanocarrier, and in vitro loading and release studies of drugs were performed with good encapsulation efficiency.\n\nResults: In vitro biological studies including hemolysis assay, erythrocytes sedimentation rate, red blood cells aggregation, cyto cellular internalization, and apoptosis evaluations were performed. Based on results, the developed nanocarrier has many advantages and capability for an efficient codelivery of DOX and MTX, which has a highly potent ability to kill cancer cells.\n\nConclusion: All these results persuade us, this nanocarrier could be effectively used for cancer therapy of MCF7 breast cancer cells and is suitable for use in further animal studies in future investigations.\n\nKeywords: MCF7 cells; amphoteric calix[4]arene; combination therapy; drug delivery; green synthesis.",
         "relevant":true
      },
      {
         "id":"10.1016/j.tips.2014.09.008",
         "title":"Prodrug-based nanoparticulate drug delivery strategies for cancer therapy",
         "abstract":"Despite the rapid developments in nanotechnology and biomaterials, the efficient delivery of chemotherapeutic agents is still challenging. Prodrug-based nanoassemblies have many advantages as a potent platform for anticancer drug delivery, such as improved drug availability, high drug loading efficiency, resistance to recrystallization upon encapsulation, and spatially and temporally controllable drug release. In this review, we discuss prodrug-based nanocarriers for cancer therapy, including nanosystems based on polymer-drug conjugates, self-assembling small molecular weight prodrugs and prodrug-encapsulated nanoparticles (NPs). In addition, we discuss new trends in the field of prodrug-based nanoassemblies that enhance the delivery efficiency of anticancer drugs, with special emphasis on smart stimuli-triggered drug release, hybrid nanoassemblies, and combination drug therapy.",
         "relevant":true
      },
      {
         "id":"10.1517/17425247.2012.679927",
         "title":"Nanoparticles as delivery carriers for anticancer prodrugs",
         "abstract":" Introduction: Prodrugs are inactive compounds which are metabolized in the body to produce parent active agents. It has been shown that prodrugs hold some advantages over conventional drugs, such as increased solubility, improved permeability and bioavailability, reduced adverse effects and prolonged half-lives. Optimization of the vehicles used is very important in order to employ the advantages of prodrugs. Nanocarriers are currently being widely used as prodrug vehicles because of their ability to enhance storage stability, modulate prodrug release and tumor-targeted delivery and protect against enzymatic attack. This combined approach of prodrugs and nanoparticles has a particular attraction for developing anticancer therapies.\n\nAreas covered: This paper discusses liposomes, polymeric nanoparticles and lipid nanoparticles, which are all carriers commonly used for prodrug encapsulation. Macromolecular prodrugs can spontaneously form self-assembled nanoparticles with no intervention of other additives. This review also describes recent developments in prodrug delivery using nanoparticulate strategies. Pharmacokinetic, pharmacodynamic and cytotoxicity evaluations of anticancer prodrugs are systematically elucidated in this review.\n\nExpert opinion: More profiles involved in animal and clinical studies will encourage the future applicability of prodrug nanocarrier therapy. The possible toxicity associated with nanoparticles is a concern for development of prodrug delivery.",
         "relevant":true
      },
      {
         "id":"10.1016/j.colsurfb.2013.08.021",
         "title":"Nanoassemblies containing a fluorouracil/zidovudine glyceryl prodrug with phospholipase A2-triggered drug release for cancer treatment",
         "abstract":"Secretory phospholipase A2 (sPLA2), which is overexpressed in many tumors, cleaves ester bonds at the sn-2 position of phospholipids. A PLA2-sensitive amphiphilic prodrug, 1-O-octadecyl-2-(5-fluorouracil)-N-acetyl-3-zidovudine-phosphorylglycerol (OFZG), was synthesized and used to prepare nanoassemblies through the injection of a mixture of OFZG/cholesterol/Tween 80 (2:1:0.1, mol:mol:mol) into water. Cholesterol and Tween 80 was incorporated into the OFZG monolayers at the air/water interface to yield nanoassemblies. The resulting nanoassemblies exhibited a narrow size distribution with a mean size of 77.8nm and were stable due to their high surface charges. The in vitro experiments showed that PLA2 degraded OFZG. The nanoassemblies exhibited higher anticancer activity than the parent drug 5-fluorouracil (5-FU) in COLO205, HT-28, and HCT-116 cells. The intravenous (i.v.) administration of the nanoassemblies into mice resulted in the rapid elimination of OFZG from the circulation and its distribution mainly in the liver, lung, spleen, and kidney. After their injection into tumor-bearing mice, the nanoassemblies exhibited anticancer efficiency comparable to that of 5-FU, even though the nanoassemblies contained concentrations of only 1/10 of the molar amount of 5-FU. The lessons learned from the study and methods for the design of PLA2-sensitive amphiphilic prodrugs are also discussed. Enzyme-sensitive amphiphilic combinatorial prodrugs and prodrug-loaded nanoassemblies may represent a new strategy for anticancer drug design.",
         "relevant":true
      },
      {
         "id":"10.3390/molecules21070965",
         "title":"Targeting Epithelial-Mesenchymal Transition (EMT) to Overcome Drug Resistance in Cancer",
         "abstract":"Epithelial-mesenchymal transition (EMT) is known to play an important role in cancer progression, metastasis and drug resistance. Although there are controversies surrounding the causal relationship between EMT and cancer metastasis, the role of EMT in cancer drug resistance has been increasingly recognized. Numerous EMT-related signaling pathways are involved in drug resistance in cancer cells. Cells undergoing EMT show a feature similar to cancer stem cells (CSCs), such as an increase in drug efflux pumps and anti-apoptotic effects. Therefore, targeting EMT has been considered a novel opportunity to overcome cancer drug resistance. This review describes the mechanism by which EMT contributes to drug resistance in cancer cells and summarizes new advances in research in EMT-associated drug resistance.",
         "relevant":true
      },
      {
         "id":"10.1016/j.drup.2015.11.004",
         "title":"Lysosomes as mediators of drug resistance in cancer",
         "abstract":"Drug resistance remains a leading cause of chemotherapeutic treatment failure and cancer-related mortality. While some mechanisms of anticancer drug resistance have been well characterized, multiple mechanisms remain elusive. In this respect, passive ion trapping-based lysosomal sequestration of multiple hydrophobic weak-base chemotherapeutic agents was found to reduce the accessibility of these drugs to their target sites, resulting in a markedly reduced cytotoxic effect and drug resistance. Recently we have demonstrated that lysosomal sequestration of hydrophobic weak base drugs triggers TFEB-mediated lysosomal biogenesis resulting in an enlarged lysosomal compartment, capable of enhanced drug sequestration. This study further showed that cancer cells with an increased number of drug-accumulating lysosomes are more resistant to lysosome-sequestered drugs, suggesting a model of drug-induced lysosome-mediated chemoresistance. In addition to passive drug sequestration of hydrophobic weak base chemotherapeutics, other mechanisms of lysosome-mediated drug resistance have also been reported; these include active lysosomal drug sequestration mediated by ATP-driven transporters from the ABC superfamily, and a role for lysosomal copper transporters in cancer resistance to platinum-based chemotherapeutics. Furthermore, lysosomal exocytosis was suggested as a mechanism to facilitate the clearance of chemotherapeutics which highly accumulated in lysosomes, thus providing an additional line of resistance, supplementing the organelle entrapment of chemotherapeutics away from their target sites. Along with these mechanisms of lysosome-mediated drug resistance, several approaches were recently developed for the overcoming of drug resistance or exploiting lysosomal drug sequestration, including lysosomal photodestruction and drug-induced lysosomal membrane permeabilization. In this review we explore the current literature addressing the role of lysosomes in mediating cancer drug resistance as well as novel modalities to overcome this chemoresistance.",
         "relevant":true
      },
      {
         "id":"10.18632/oncotarget.15155",
         "title":"Lysosomal accumulation of anticancer drugs triggers lysosomal exocytosis",
         "abstract":"We have recently shown that hydrophobic weak base anticancer drugs are highly sequestered in acidic lysosomes, inducing TFEB-mediated lysosomal biogenesis and markedly increased lysosome numbers per cell. This enhanced lysosomal sequestration of chemotherapeutics, away from their intracellular targets, provoked cancer multidrug resistance. However, little is known regarding the fate of lysosome-sequestered drugs. While we suggested that sequestered drugs might be expelled from cancer cells via lysosomal exocytosis, no actual drug-induced lysosomal exocytosis was demonstrated. By following the subcellular localization of lysosomes during exposure to lysosomotropic chemotherapeutics, we herein demonstrate that lysosomal drug accumulation results in translocation of lysosomes from the perinuclear zone towards the plasma membrane via movement on microtubule tracks. Furthermore, following translocation to the plasma membrane in drug-treated cells, lysosomes fused with the plasma membrane and released their cargo to the extracellular milieu, as also evidenced by increased levels of the lysosomal enzyme cathepsin D in the extracellular milieu. These findings suggest that lysosomal exocytosis of chemotherapeutic drug-loaded lysosomes is a crucial component of lysosome-mediated cancer multidrug resistance. We further argue that drug-induced lysosomal exocytosis bears important implications on tumor progression, as several lysosomal enzymes were found to play a key role in tumor cell invasion, angiogenesis and metastasis.",
         "relevant":true
      },
      {
         "id":"10.3390/ijms21124392",
         "title":"Drug Sequestration in Lysosomes as One of the Mechanisms of Chemoresistance of Cancer Cells and the Possibilities of Its Inhibition",
         "abstract":"Resistance to chemotherapeutics and targeted drugs is one of the main problems in successful cancer therapy. Various mechanisms have been identified to contribute to drug resistance. One of those mechanisms is lysosome-mediated drug resistance. Lysosomes have been shown to trap certain hydrophobic weak base chemotherapeutics, as well as some tyrosine kinase inhibitors, thereby being sequestered away from their intracellular target site. Lysosomal sequestration is in most cases followed by the release of their content from the cell by exocytosis. Lysosomal accumulation of anticancer drugs is caused mainly by ion-trapping, but active transport of certain drugs into lysosomes was also described. Lysosomal low pH, which is necessary for ion-trapping is achieved by the activity of the V-ATPase. This sequestration can be successfully inhibited by lysosomotropic agents and V-ATPase inhibitors in experimental conditions. Clinical trials have been performed only with lysosomotropic drug chloroquine and their results were less successful. The aim of this review is to give an overview of lysosomal sequestration and expression of acidifying enzymes as yet not well known mechanism of cancer cell chemoresistance and about possibilities how to overcome this form of resistance.",
         "relevant":true
      },
      {
         "id":"10.1074/jbc.M113.514091",
         "title":"P-glycoprotein mediates drug resistance via a novel mechanism involving lysosomal sequestration",
         "abstract":"Localization of the drug transporter P-glycoprotein (Pgp) to the plasma membrane is thought to be the only contributor of Pgp-mediated multidrug resistance (MDR). However, very little work has focused on the contribution of Pgp expressed in intracellular organelles to drug resistance. This investigation describes an additional mechanism for understanding how lysosomal Pgp contributes to MDR. These studies were performed using Pgp-expressing MDR cells and their non-resistant counterparts. Using confocal microscopy and lysosomal fractionation, we demonstrated that intracellular Pgp was localized to LAMP2-stained lysosomes. In Pgp-expressing cells, the Pgp substrate doxorubicin (DOX) became sequestered in LAMP2-stained lysosomes, but this was not observed in non-Pgp-expressing cells. Moreover, lysosomal Pgp was demonstrated to be functional because DOX accumulation in this organelle was prevented upon incubation with the established Pgp inhibitors valspodar or elacridar or by silencing Pgp expression with siRNA. Importantly, to elicit drug resistance via lysosomes, the cytotoxic chemotherapeutics (e.g. DOX, daunorubicin, or vinblastine) were required to be Pgp substrates and also ionized at lysosomal pH (pH 5), resulting in them being sequestered and trapped in lysosomes. This property was demonstrated using lysosomotropic weak bases (NH4Cl, chloroquine, or methylamine) that increased lysosomal pH and sensitized only Pgp-expressing cells to such cytotoxic drugs. Consequently, a lysosomal Pgp-mediated mechanism of MDR was not found for non-ionizable Pgp substrates (e.g. colchicine or paclitaxel) or ionizable non-Pgp substrates (e.g. cisplatin or carboplatin). Together, these studies reveal a new mechanism where Pgp-mediated lysosomal sequestration of chemotherapeutics leads to MDR that is amenable to therapeutic exploitation.",
         "relevant":true
      },
      {
         "id":"10.1016/j.drup.2020.100719",
         "title":"FDA approved drugs with pharmacotherapeutic potential for SARS-CoV-2 (COVID-19) therapy",
         "abstract":"In December 2019, a novel SARS-CoV-2 coronavirus emerged, causing an outbreak of life-threatening pneumonia in the Hubei province, China, and has now spread worldwide, causing a pandemic. The urgent need to control the disease, combined with the lack of specific and effective treatment modalities, call for the use of FDA-approved agents that have shown efficacy against similar pathogens. Chloroquine, remdesivir, lopinavir/ritonavir or ribavirin have all been successful in inhibiting SARS-CoV-2 in vitro. The initial results of a number of clinical trials involving various protocols of administration of chloroquine or hydroxychloroquine mostly point towards their beneficial effect. However, they may not be effective in cases with persistently high viremia, while results on ivermectin (another antiparasitic agent) are not yet available. Interestingly, azithromycin, a macrolide antibiotic in combination with hydroxychloroquine, might yield clinical benefit as an adjunctive. The results of clinical trials point to the potential clinical efficacy of antivirals, especially remdesivir (GS-5734), lopinavir/ritonavir, and favipiravir. Other therapeutic options that are being explored involve meplazumab, tocilizumab, and interferon type 1. We discuss a number of other drugs that are currently in clinical trials, whose results are not yet available, and in various instances we enrich such efficacy analysis by invoking historic data on the treatment of SARS, MERS, influenza, or in vitro studies. Meanwhile, scientists worldwide are seeking to discover novel drugs that take advantage of the molecular structure of the virus, its intracellular life cycle that probably elucidates unfolded-protein response, as well as its mechanism of surface binding and cell invasion, like angiotensin converting enzymes-, HR1, and metalloproteinase inhibitors.",
         "relevant":true
      },
      {
         "id":"10.1016/j.fcl.2019.04.004",
         "title":"Nonsurgical Treatment Options for Insertional Achilles Tendinopathy",
         "abstract":"Most nonoperative treatments for insertional Achilles tendinopathy (IAT) have insufficient evidence to support treatment recommendations. Exercise has the highest level of evidence supporting the ability of this treatment option to reduce IAT pain. The effects of exercise may be enhanced by a wide variety of other treatments, including soft tissue treatment, nutritional supplements, iontophoresis, education, stretching, and heel lifts. When exercise is unsuccessful, extracorporeal shock wave therapy seems to be the next best nonoperative treatment option to reduce IAT pain. After other nonoperative treatment options have been exhausted, injections may be considered, particularly to facilitate participation in an exercise program.",
         "relevant":true
      },
      {
         "id":"10.1177/1938640019826673",
         "title":"Shockwave Therapy Associated With Eccentric Strengthening for Achilles Insertional Tendinopathy: A Prospective Study",
         "abstract":"Background. The usual initial treatment for insertional Achilles tendinopathy is nonsurgical. Yet there is no standard conservative treatment for Achilles insertional tendinopathy. Shockwave therapy (SWT) has become a reliable option for the management of this illness over the past years. The aim of this study is to report the effectiveness of low-energy SWT associated with an eccentric strengthening protocol in 19 consecutive patients. Methods. This is a prospective study with 19 patients aged between 26 and 72 years diagnosed with insertional Achilles tendinopathy. The protocol consisted of SWT associated with eccentric exercises for 12 weeks. All patients were evaluated on the first day and after 24 weeks (final follow-up) with the Victorian Institute of Sports Assessment-Achilles (VISA-A) score, visual analogue scale (VAS), American Orthopaedic Foot and Ankle Society (AOFAS) questionnaire, and by algometry. At the last follow-up, patients were also assessed for adherence to the protocol, complications and final outcome (in their perception as success or fail). Results and Conclusion. Fifteen (79%) patients were fully adherent to the Alfredson protocol, and 13 (68%) patients considered the treatment protocol successful. At the last evaluation, patients demanded higher pressure on calcaneus to trigger pain (algometry 1), reported less pain when the algometer was applied with 3 kg (algometry 2), had less global pain (VAS), and had higher AOFAS and VISA-A scores. This study evidences that eccentric loading associated with SWT can dramatically improve patients' symptoms. We can conclude that eccentric loading associated with SWT is an effective treatment for Achilles insertional tendinopathy. Levels of Evidence: Therapeutic, Level III: Prospective cohort.",
         "relevant":true
      },
      {
         "id":"10.1007/s40122-020-00157-5",
         "title":"Injection Techniques for Common Chronic Pain Conditions of the Foot: A Comprehensive Review",
         "abstract":"Purpose of review: This is a comprehensive literature review of the available evidence and techniques of foot injections for chronic pain conditions. It briefly describes common foot chronic pain syndromes and then reviews available injection techniques for each of these syndromes, weighing the available evidence and comparing the available approaches.\n\nRecent findings: Foot and ankle pain affects 20% of the population over 50 and significantly impairs mobility and ability to participate in activities of daily living (ADLs), as well as increases fall risk. It is commonly treated with costly surgery, at times with questionable efficacy. Injection therapy is challenging when the etiology is anatomical or compressive. Morton's neuroma is a budging of the interdigital nerve. Steroid, alcohol, and capsaicin injections provide some benefit, but it is short lived. Hyaluronic acid (HA) injection provided long-term relief and could prove to be a viable treatment option. Achilles tendinopathy (AT) is most likely secondary to repeat tendon stress-platelet-rich-plasma (PRP) and prolotherapy have been trialed for this condition, but more evidence is required to show efficacy. Similar injections were trials for plantar fasciitis and achieved only short-term relief; however, some evidence suggests that PRP injections reduce the frequency of required therapy. Tarsal tunnel syndrome, a compressive neuropathy carries a risk of permanent neural injury if left untreated. Injection therapy can provide a bridge to surgery; however, surgical decompression remains the definitive therapy. When the etiology is inflammatory, steroid injection is more likely to provide benefit. This has been shown in several studies for gout, as well as osteoarthritis of the foot and ankle and treatment-refractory rheumatoid arthritis. HA showed similar benefit, possibly due to anti-inflammatory effects. Stem cell injections may provide the additional benefit of structure restoration. Chronic foot pain is common in the general population and has significant associated morbidity and disability. Traditionally treated with surgery, these are costly and only somewhat effective. Injections provide an effective alternative financially and some evidence exists that they are effective in pain alleviation. However, current evidence is limited and the benefit described from injection therapy has been short-lived in most cases. Further studies in larger populations are required to evaluate the long-term effects of these treatments.\n\nKeywords: Achilles tendinopathy; Ankle arthritis; Foot pain; Gout; Injection techniques; Morton neuroma; Plantar fasciitis; Tarsal tunnel syndrome.",
         "relevant":true
      },
      {
         "id":"10.5704/MOJ.1911.002",
         "title":"Is Platelet-rich Plasma Injection more Effective than Steroid Injection in the Treatment of Chronic Plantar Fasciitis in Achieving Long-term Relief?",
         "abstract":"Introduction: Plantar fasciitis is characterised by pain in the heel, which is aggravated on weight bearing after prolonged rest. Many modalities of treatment are commonly used in the management of plantar fasciitis including steroid injection. Many studies show that steroid injection provides pain relief in the short term but not long lasting. Recent reports show autologous platelet-rich plasma (PRP) injection promotes healing, resulting in better pain relief in the short as well as long term. The present study was undertaken to compare the effects of local injection of platelet-rich plasma and Corticosteroid in the treatment of chronic plantar fasciitis. Materials and methods: Patients with the clinical diagnosis of chronic plantar fasciitis (heel pain of more than six weeks) after failed conservative treatment and plantar fascia thickness more than 4mm were included in the study. Patients with previous surgery for plantar fasciitis, active bilateral plantar fasciitis, vascular insufficiency or neuropathy related to heel pain, hypothyroidism and diabetes mellitus were excluded from the study. In this prospective double-blind study, 60 patients who fulfilled the criteria were divided randomly into two groups. Patients in Group A received PRP injection and those in Group B received steroid injection. Patients were assessed with visual analog scale (VAS) and American Orthopedic Foot and Ankle Society (AOFAS) score. Assessment was done before injection, at six weeks, three months and six months follow-up after injection. Plantar fascia thickness was assessed before the intervention and six months after treatment using sonography. Results: Mean VAS in Group A decreased from 7.14 before injection to 1.41 after injection and in Group B decreased from 7.21 before injection to 1.93 after injection, at final follow-up. Mean AOFAS score in Group A improved from 54 to 90.03 and in Group B from 55.63 to 74.67 at six months' follow-up. The improvements observed in VAS and AOFAS were statistically significant. At the end of six months' follow-up, plantar fascia thickness had reduced in both groups (5.78mm to 3.35mm in Group A and 5.6 to 3.75 in Group B) and the difference was statistically significant. Conclusion: Local injection of platelet-rich plasma is an effective treatment option for chronic plantar fasciitis when compared with steroid injection with long lasting beneficial effect.",
         "relevant":true
      },
      {
         "id":"10.1007/s00403-018-1808-x",
         "title":"Treatment preferences and treatment satisfaction among psoriasis patients: a systematic review",
         "abstract":"A critical gap exists in determining treatment preferences and treatment satisfaction from patient perspectives, which is paramount to achieving therapeutic success. The objective of this systematic review is to determine factors influencing treatment preferences and treatment satisfaction among psoriasis patients. PubMed, EMBASE, and Web of Science databases were searched between November 1, 2010, and December 1, 2017. Observational and interventional research studies published in the English language that discussed patient preferences and patient satisfaction in the treatment of psoriasis were reviewed and synthesized. We utilized data on treatment preferences and treatment satisfaction from 35,388 psoriasis patients based on 60 articles from the years 2010 to 2017. Treatment preferences were heterogeneous and changed over time among psoriasis patients. Across all treatment modalities, the most important treatment attributes were treatment location, probability of improvement, and delivery method. For biologics specifically, the most important attributes were risk of adverse events and probability of treatment benefit. Factors that influenced patients' preferences for certain treatments included age, sex, comorbidities, disease duration, and prior treatments. Notably, some psoriasis patients placed higher importance on a treatment's process attributes (e.g., access and delivery) over its outcome attributes (e.g., efficacy). Overall, patient satisfaction with existing therapies remains modest; however, those treated with biologic agents exhibited highest treatment satisfaction over oral therapy, phototherapy, and topical therapy.",
         "relevant":true
      },
      {
         "id":"10.1016/j.cvex.2018.01.012",
         "title":"Update on Cancer Treatment in Exotics",
         "abstract":"Treatment options for animals with cancer are rapidly expanding, including in exotic animal medicine. Limited information is available about treatment effects in exotic pet species beyond individual case reports. Most cancer treatment protocols in exotic animals are extrapolated from those described in humans, dogs, and cats. This review provides an update on cancer treatment in exotic animal species. The Exotic Species Cancer Research Alliance accumulates clinical cases in a central location with standardized clinical information, with resources to help clinicians find and enter their cases for the collective good of exotic clinicians and their patients.",
         "relevant":true
      },
      {
         "id":"10.1097/CCO.0000000000000573",
         "title":"Integrated treatment of brain metastases",
         "abstract":"Purpose of review: Optimal treatment of brain metastases has been limited to local treatment with few systemic options. Increasing use of systemic targeted therapies, chemotherapy and immunotherapy and combination of local and systemic treatments has resulted in plethora of publications. We review the existing evidence for individual treatments and new evidence for the integration of systemic and combination of local treatments.\n\nRecent findings: Encouraging efficacy of systemic therapies supports combination of systemic and local treatment albeit with little randomized trial data. Efficacy particularly of targeted agents provides an opportunity to delay local treatments including radiosurgery and whole brain radiotherapy. Randomized trials testing the integration of surgery, radiotherapy and radiosurgery are reviewed with emphasis on patient relevant endpoints to guide the clinician in the choice and sequence of treatments and integrating systemic and local therapies.\n\nSummary: There is increasing tendency to use focused radiation for single and oligometastases with or without surgery and decline in whole brain radiotherapy which is limited to multiple metastases in tumours without effective systemic options. Systemic therapies have promising intracranial efficacy and the sequence and combination with localized radiation is awaiting trials. Changes in practice with a move to primary systemic treatment for brain metastases without radiation, should be undertaken with caution and close monitoring.",
         "relevant":true
      },
      {
         "id":"10.1080/14737140.2017.1296764",
         "title":"Radiosurgery/stereotactic radiotherapy in combination with immunotherapy and targeted agents for melanoma brain metastases",
         "abstract":"The clinical landscape of advanced melanoma drastically changed after the introduction of both targeted therapies and immunotherapy. This rapid development in systemic therapies led to a change in the management of patients with brain metastases, with the subsequent need to re-assess the role of local therapies, in particular stereotactic radiosurgery (SRS). Areas covered: In this non-systematic review, we report on the current knowledge on the use of SRS in combination with immunotherapy and BRAF/MEK inhibitors for patients with melanoma brain metastases, as well as ongoing trials in this field. Expert commentary: It is now more common to observe patients with melanoma brain metastases with better performance status and prolonged life expectancy. A combination of targeted therapy and immunotherapy, in different sequences, has been shown to be feasible and well tolerable, on the basis of retrospective reports. Additional data from ongoing prospective trials are however needed to confirm or not these findings and better explore the efficacy of the combination.",
         "relevant":true
      },
      {
         "id":"10.18553/jmcp.2014.20.4.346",
         "title":"Current and future roles of targeted therapy and immunotherapy in advanced melanoma",
         "abstract":"Background: Melanoma is an aggressive disease that accounts for approximately 75% of skin cancer-related deaths. Historically, treatment options for patients with advanced stage melanoma have been limited by modest response rates and failure to improve overall survival. The treatment landscape for advanced stage melanoma was revolutionized in 2011 with the approval of ipilimumab and vemurafenib, both of which improved overall survival in phase III clinical trials. More recently, the targeted inhibitors dabrafenib and trametinib have demonstrated similar therapeutic profiles.\n\nObjectives: To (a) discuss emerging treatment options for advanced melanoma, specifically ilpilimumab, vemurafenib, dabrafenib, and trametinib, in the context of their mechanisms of action and their potential for long-term improvement in patient outcome, and (b) to consider the impact of these agents on the current treatment landscape.\n\nMethods: A literature search was conducted to collect data from clinical trials involving ipilimumab, vemurafenib, dabrafenib, and trametinib. Emphasis was placed on outcome measures related to long-term clinical benefit.\n\nResults: Ipilimumab, a fully human monoclonal antibody, exploits the natural ability of the immune system to eradicate primary cancer cells. It inhibits the binding of cytotoxic T-lymphocyte antigen-4 to its ligands, thereby potentiating T-cell response and antitumor immunity. In a phase III clinical trial, ipilimumab at 3 mg/kg improved overall survival in previously treated patients with metastatic melanoma. Components of the mitogen-activated protein kinase (MAPK) pathway are particularly relevant in melanoma and have been targeted by small molecular inhibitors. Vemurafenib and dabrafenib inhibit the BRAF V600 mutation, which prevents oncogenic activities such as uncheck proliferation and evasion of immune response. Data from phase III clinical trials suggest that both vemurafenib and dabrafenib improve patient outcomes, with vemurafenib showing an overall survival benefit and dabrafenib showing improved median progression-free survival. The targeted-therapy approach in melanoma continued to gain momentum with the development of trametinib, which inhibits the MEK protein, the only known substrate of the BRAF V600 protein. Inhibition of MEK leads to decreased cell signaling and proliferation in cancer cells. In phase III trials, trametinib demonstrated significant improvement in median progression-free survival and median overall survival compared with chemotherapy treatment, making this treatment a valuable addition to the current armamentarium. The adverse events associated with these new treatments are generally tolerable and mild to moderate in severity; however, care should be taken when selecting a therapy, since the specific adverse events associated with these treatments are unique, and serious events have been reported.\n\nConclusions: The immunotherapy ipilimumab and the MAPK-targeted inhibitors vemurafenib, dabrafenib, and trametinib have forever changed the treatment landscape for melanoma. Indeed, these new therapies have demonstrated long-term improvement in patient outcome, a benefit not afforded by traditional therapeutics. Important research continues on the molecular basis of melanoma, and new targets are likely to emerge. Other areas of work include optimization of sequencing and/or combination of current treatments, which may increase the number of patients who experience clinical benefit.",
         "relevant":true
      },
      {
         "id":"10.3389/fphar.2020.00722",
         "title":"PD-1/PD-L1 Based Combinational Cancer Therapy: Icing on the Cake",
         "abstract":"Cancer has been a major global health problem due to its high morbidity and mortality. While many chemotherapy agents have been studied and applied in clinical trials or in clinic, their application is limited due to its toxic side effects and poor tolerability. Monoclonal antibodies specific to the PD-1 and PD-L1 immune checkpoints have been approved for the treatment of various tumors. However, the application of PD-1/PD-L1 inhibitors remains suboptimal and thus another strategy comes in to our sight involving the combination of checkpoint inhibitors with other agents, enhancing the therapeutic efficacy. Various novel promising approaches are now in clinical trials, just as icing on the cake. This review summarizes relevant investigations on combinatorial therapeutics based on PD-1/PD-L1 inhibition.",
         "relevant":true
      },
      {
         "id":30003719,
         "title":"Photodynamic combinational therapy in cancer treatment",
         "abstract":"Photodynamic therapy (PDT) has attracted widespread attention in recent years as a non-invasive and highly selective approach for cancer treatment. PDT involves the activation of a photosensitizer by an appropriate wavelength of light, generating transient levels of reactive oxygen species (ROS). However, the utilization of PDT against deep tumors has been greatly limited by insufficient luminous flux and the occurrence of peripheral tissue damage. Therefore, experts have begun to explore whether the combination of PDT with other treatments can improve its efficacy. In this review, we have collected articles about experiments (in vitro and in vivo) and clinical research on photodynamic combination therapies in recent years, roughly divided into four parts corresponding to PDT combined with chemotherapy, radiotherapy, immunotherapy and other therapies, to compare the therapeutic effects of the combination therapy and monotherapy. The results showed that photodynamic combination treatments, in general, perform better than single treatment modalities. Thus, the increased therapeutic effects, reduced side effects and coordination treatment effects of PDT are worth of further exploration.",
         "relevant":true
      },
      {
         "id":"10.3390/polym11122085",
         "title":"Perspectives of Molecularly Imprinted Polymer-Based Drug Delivery Systems in Cancer Therapy",
         "abstract":"Despite the considerable effort made in the past decades, multiple aspects of cancer management remain a challenge for the scientific community. The severe toxicity and poor bioavailability of conventional chemotherapeutics, and the multidrug resistance have turned the attention of researchers towards the quest of drug carriers engineered to offer an efficient, localized, temporized, and doze-controlled delivery of antitumor agents of proven clinical value. Molecular imprinting of chemotherapeutics is very appealing in the design of drug delivery systems since the specific and selective binding sites created within the polymeric matrix turn these complex structures into value-added carriers with tunable features, notably high loading capacity, and a good control of payload release. Our work aims to summarize the present state-of-the art of molecularly imprinted polymer-based drug delivery systems developed for anticancer therapy, with emphasis on the particularities of the chemotherapeutics' release and with a critical assessment of the current challenges and future perspectives of these unique drug carriers.",
         "relevant":true
      },
      {
         "id":"10.3109/03639045.2014.948451",
         "title":"Development of ocular drug delivery systems using molecularly imprinted soft contact lenses",
         "abstract":"Recently, significant advances have been made in order to optimize drug delivery to ocular tissues. The main problems in ocular drug delivery are poor bioavailability and uncontrollable drug delivery of conventional ophthalmic preparations (e.g. eye drops). Hydrogels have been investigated since 1965 as new ocular drug delivery systems. Increase of hydrogel loading capacity, optimization of drug residence time on the ocular surface and biocompatibility with the eye tissue has been the main focus of previous studies. Molecular imprinting technology provided the opportunity to fulfill the above-mentioned objectives. Molecularly imprinted soft contact lenses (SCLs) have high potentials as novel drug delivery systems for the treatment of eye disorders. This technique is used for the preparation of polymers with specific binding sites for a template molecule. Previous studies indicated that molecular imprinting technology could be successfully applied for the preparation of SCLs as ocular drug delivery systems. Previous research, particularly in vivo studies, demonstrated that molecular imprinting is a versatile and effective method in optimizing the drug release behavior and enhancing the loading capacity of SCLs as new ocular drug delivery systems. This review highlights various potentials of molecularly imprinted contact lenses in enhancing the drug-loading capacity and controlling the drug release, compared to other ocular drug delivery systems. We have also studied the effects of contributing factors such as the type of comonomer, template/functional monomer molar ratio, crosslinker concentration in drug-loading capacity, and the release properties of molecularly imprinted hydrogels.",
         "relevant":true
      },
      {
         "id":"10.2174/1567201813666160101120238",
         "title":"Molecularly Imprinted Polymers: Novel Discovery for Drug Delivery",
         "abstract":"Background: Molecularly imprinted polymers (MIP) are novel carriers synthesized by imprinting of a template over a polymer. This paper presents the recent application of MIP for diagnostic and therapeutic drug delivery.\n\nOverview: MIP owing to their 3D polymeric structures and due to bond formation with the template serves as a reservoir of active causing stimuli sensitive, enantioselective, targetted and/or controlled release. The review elaborates about key factors for optimization of MIP, controlled release by MIP for various administration routes various forms like patches, contact lenses, nanowires along with illustrations. To overcome the limitation of organic solvent usage causing increased cost, water compatible MIP and use of supercritical fluid technology for molecular imprinting were developed. Novel methods for developing water compatible MIP like pickering emulsion polymerization, co-precipitation method, cyclodextrin imprinting, surface grafting, controlled/living radical chain polymerization methods are described with illustration in this review. Various protein imprinting methods like bulk, epitope and surface imprinting are described along with illustrations. Further, application of MIP in microdevices as biomimetic sensing element for personalized therapy is elaborated.\n\nConclusion: Although development and application of MIP in drug delivery is still at its infancy, constant efforts of researchers will lead to a novel intelligent drug delivery with commercial value. Efforts should be directed in developing solid oral dosage forms consisting of MIP for therapeutic protein and peptide delivery and targeted release of potent drugs addressing life threatening disease like cancer. Amalgamation of bio-engineering and pharmaceutical techniques can make these future prospects into reality.",
         "relevant":true
      },
      {
         "id":"10.1002/14651858.CD008123.pub4",
         "title":"Medical treatment for botulism",
         "abstract":"Background: Botulism is an acute paralytic illness caused by a neurotoxin produced by Clostridium botulinum. Supportive care, including intensive care, is key, but the role of other medical treatments is unclear. This is an update of a review first published in 2011.\n\nObjectives: To assess the effects of medical treatments on mortality, duration of hospitalization, mechanical ventilation, tube or parenteral feeding, and risk of adverse events in botulism.\n\nSearch methods: We searched the Cochrane Neuromuscular Specialised Register, CENTRAL, MEDLINE, and Embase on 23 January 2018. We reviewed bibliographies and contacted authors and experts. We searched two clinical trials registers, WHO ICTRP and clinicaltrials.gov, on 21 February 2019.\n\nSelection criteria: Randomized controlled trials (RCTs) and quasi-RCTs examining the medical treatment of any of the four major types of botulism (infant intestinal botulism, food-borne botulism, wound botulism, and adult intestinal toxemia). Potential medical treatments included equine serum trivalent botulism antitoxin, human-derived botulinum immune globulin intravenous (BIG-IV), plasma exchange, 3,4-diaminopyridine, and guanidine.\n\nData collection and analysis: We followed standard Cochrane methodology.Our primary outcome was in-hospital death from any cause occurring within four weeks from randomization or the beginning of treatment. Secondary outcomes were death from any cause occurring within 12 weeks, duration of hospitalization, duration of mechanical ventilation, duration of tube or parenteral feeding, and proportion of participants with adverse events or complications of treatment.\n\nMain results: A single RCT met the inclusion criteria. Our 2018 search update identified no additional trials. The included trial evaluated BIG-IV for the treatment of infant botulism and included 59 treatment participants and 63 control participants. The control group received a control immune globulin that did not have an effect on botulinum toxin. Participants were followed during the length of their hospitalization to measure the outcomes of interest. There was some violation of intention-to-treat principles, and possibly some between-treatment group imbalances among participants admitted to the intensive care unit and mechanically ventilated, but otherwise the risk of bias was low. There were no deaths in either group, making any treatment effect on mortality inestimable. There was a benefit in the treatment group on mean duration of hospitalization (BIG-IV: 2.60 weeks, 95% confidence interval (CI) 1.95 to 3.25; control: 5.70 weeks, 95% CI 4.40 to 7.00; mean difference (MD) -3.10 weeks, 95% CI -4.52 to -1.68; moderate-certainty evidence); mechanical ventilation (BIG-IV: 1.80 weeks, 95% CI 1.20 to 2.40; control: 4.40 weeks, 95% CI 3.00 to 5.80; MD -2.60 weeks, 95% CI -4.06 to -1.14; low-certainty evidence); and tube or parenteral feeding (BIG-IV: 3.60 weeks, 95% CI 1.70 to 5.50; control: 10.00 weeks, 95% CI 6.85 to 13.15; MD -6.40 weeks, 95% CI -10.00 to -2.80; moderate-certainty evidence), but not on proportion of participants with adverse events or complications (BIG-IV: 63.08%; control: 68.75%; risk ratio 0.92, 95% CI 0.72 to 1.18; absolute risk reduction 0.06, 95% CI 0.22 to -0.11; moderate-certainty evidence).\n\nAuthors' conclusions: We found low- and moderate-certainty evidence supporting the use of BIG-IV in infant intestinal botulism. A single RCT demonstrated that BIG-IV probably decreases the duration of hospitalization; may decrease the duration of mechanical ventilation; and probably decreases the duration of tube or parenteral feeding. Adverse events were probably no more frequent with immune globulin than with placebo. Our search did not reveal any evidence examining the use of other medical treatments including serum trivalent botulism antitoxin.",
         "relevant":true
      },
      {
         "id":"10.1002/14651858.CD011680.pub2",
         "title":"Interventions for necrotizing soft tissue infections in adults",
         "abstract":"Background: Necrotizing soft tissue infections (NSTIs) are severe and rapidly spreading soft tissue infections of the subcutaneous tissue, fascia, or muscle, which are mostly caused by bacteria. Associated rates of mortality and morbidity are high, with the former estimated at around 23%, and disability, sequelae, and limb loss occurring in 15% of patients. Standard management includes intravenous empiric antimicrobial therapy, early surgical debridement of necrotic tissues, intensive care support, and adjuvant therapies such as intravenous immunoglobulin (IVIG).\n\nObjectives: To assess the effects of medical and surgical treatments for necrotizing soft tissue infections (NSTIs) in adults in hospital settings.\n\nSearch methods: We searched the following databases up to April 2018: the Cochrane Skin Group Specialised Register, CENTRAL, MEDLINE, Embase, and LILACS. We also searched five trials registers, pharmaceutical company trial results databases, and the US Food and Drug Administration and the European Medicines Agency websites. We checked the reference lists of included studies and reviews for further references to relevant randomised controlled trials (RCTs).\n\nSelection criteria: RCTs conducted in hospital settings, that evaluated any medical or surgical treatment for adults with NSTI were eligible for inclusion. Eligible medical treatments included 1) comparisons between different antimicrobials or with placebo; 2) adjuvant therapies such as intravenous immunoglobulin (IGIV) therapy compared with placebo; no treatment; or other adjuvant therapies. Eligible surgical treatments included surgical debridement compared with amputation, immediate versus delayed intervention, or comparisons of number of interventions.RCTs of hyperbaric oxygen (HBO) therapy for NSTI were ineligible because HBO is the focus of another Cochrane Review.\n\nData collection and analysis: We used standard methodological procedures expected by Cochrane. The primary outcome measures were 1) mortality within 30 days, and 2) proportion of participants who experience a serious adverse event. Secondary outcomes were 1) survival time, and 2) assessment of long-term morbidity. We used GRADE to assess the quality of the evidence for each outcome.\n\nMain results: We included three trials randomising 197 participants (62% men) who had a mean age of 55 years. One trial compared two antibiotic treatments, and two trials compared adjuvant therapies with placebo. In all trials, participants concomitantly received standard interventions, such as intravenous empiric antimicrobial therapy, surgical debridement of necrotic tissues, intensive care support, and adjuvant therapies. All trials were at risk of attrition bias and one trial was not blinded.Moxifloxacin versus amoxicillin-clavulanate One trial included 54 participants who had a NSTI; it compared a third-generation quinolone, moxifloxacin, at a dose of 400 mg given once daily, against a penicillin, amoxicillin-clavulanate, at a dose of 3 g given three times daily for at least three days, followed by 1.5 g three times daily. Duration of treatment varied from 7 to 21 days. We are uncertain of the effects of these treatments on mortality within 30 days (risk ratio (RR) 3.00, 95% confidence interval (CI) 0.39 to 23.07) and serious adverse events at 28 days (RR 0.63, 95% CI 0.30 to 1.31) because the quality of the evidence is very low.AB103 versus placebo One trial of 43 randomised participants compared two doses, 0.5 mg/kg and 0.25 mg/kg, of an adjuvant drug, a CD28 antagonist receptor (AB103), with placebo. Treatment was given via infusion pump for 10 minutes before, after, or during surgery within six hours after the diagnosis of NSTI. We are uncertain of the effects of AB103 on mortality rate within 30 days (RR of 0.34, 95% CI 0.05 to 2.16) and serious adverse events measured at 28 days (RR 1.49, 95% CI 0.52 to 4.27) because the quality of the evidence is very low.Intravenous immunoglobulin (IVIG) versus placebo One trial of 100 randomised participants assessed IVIG as an adjuvant drug, given at a dose of 25 g/day, compared with placebo, given for three consecutive days. There may be no clear difference between IVIG and placebo in terms of mortality within 30 days (RR 1.17, 95% CI 0.42 to 3.23) (low-certainty evidence), nor serious adverse events experienced in the intensive care unit (ICU) (RR 0.73 CI 95% 0.32 to 1.65) (low-certainty evidence).Serious adverse events were only described in one RCT (the IVIG versus placebo trial) and included acute kidney injury, allergic reactions, aseptic meningitis syndrome, haemolytic anaemia, thrombi, and transmissible agents.Only one trial reported assessment of long-term morbidity, but the outcome was not defined in the way we prespecified in our protocol. The trial used the Short Form Health Survey (SF36). Data on survival time were provided upon request for the trials comparing amoxicillin-clavulanate versus moxifloxacin and IVIG versus placebo. However, even with data provided, it was not possible to perform survival analysis.\n\nAuthors' conclusions: We found very little evidence on the effects of medical and surgical treatments for NSTI. We cannot draw conclusions regarding the relative effects of any of the interventions on 30-day mortality or serious adverse events due to the very low quality of the evidence.The quality of the evidence is limited by the very small number of trials, the small sample sizes, and the risks of bias in the included trials. It is important for future trials to clearly define their inclusion criteria, which will help with the applicability of future trial results to a real-life population.Management of NSTI participants (critically-ill participants) is complex, involving multiple interventions; thus, observational studies and prospective registries might be a better foundation for future research, which should assess empiric antimicrobial therapy, as well as surgical debridement, along with the placebo-controlled comparison of adjuvant therapy. Key outcomes to assess include mortality (in the acute phase of the condition) and long-term functional outcomes, e.g. quality of life (in the chronic phase).",
         "relevant":true
      },
      {
         "id":"10.5582/bst.2015.01019",
         "title":"The advantages of using traditional Chinese medicine as an adjunctive therapy in the whole course of cancer treatment instead of only terminal stage of cancer",
         "abstract":"Recent studies indicate that Traditional Chinese medicine (TCM) can play an important role in the whole course of cancer treatment such as recovery stages of post-operative, radiotherapy or chemotherapy stages instead of only terminal stage of cancer. In this review, we have summarized current evidence for using TCM as adjuvant cancer treatment in different stages of cancer lesions. Some TCMs (e.g., TJ-41, Liu-jun-zi-tang, PHY906, Coumarin, and Aescine) are capable of improving the post-operative symptoms such as fatigue, pain, appetite, diarrhea, nausea, vomiting, and lymphedema. Some TCMs (e.g., Ginseng, Huang-Qi, BanZhiLian, TJ-48, Huachansu injection, Shenqi fuzheng injection, and Kanglaite injection) in combination with chemo- or radio-therapy are capable of enhancing the efficacy of and diminishing the side effects and complications caused by chemo- and radiotherapy. Taken together, they have great advantages in terms of suppressing tumor progression, relieving surgery complications, increasing the sensitivity of chemo- and radio- therapeutics, improving an organism's immune system function, and lessening the damage caused by surgery, chemo- or radio-therapeutics. They have significant effects on relieving breast cancer-related lymphedema, reducing cancer-related fatigue and pain, improving radiation pneumonitis and gastrointestinal side effects, protecting liver function, and even ameliorating bone marrow suppression. This review of those medicines should contribute to an understanding of Chinese herbal medicines as an adjunctive therapy in the whole course of cancer treatment instead of only terminal stage of cancer, by providing useful information for development of more effective anti-cancer drugs and making more patients \"survival with cancer\" for a long time.",
         "relevant":true
      },
      {
         "id":"10.1055/s-0035-1547302",
         "title":"[Hand Therapy in the Treatment of Patients with CRPS]",
         "abstract":"In the modern treatment of CRPS a multidisciplinary concept is firmly established (MMPT, multimodal pain therapy). Besides medical therapy and psychotherapy, physio- and occupational therapy count as basic treatment options. Although physio- and occupational therapy (in the following called hand therapy) are the most important basic treatments, the therapy is hardly standardised and there are few scientific investigations concerning their application. Therefore the purpose of this paper is to present the applied hand therapeutic techniques with regard to function/performance, application and effectiveness, and to derive a suitable treatment algorithm. The techniques used in hand therapy are presented and reviewed in regard to their effectiveness by means of a literature search. It turns out that exercise therapy, manual therapy, graded motor imaging, CO2 baths and occupational therapy have a proven benefit for the patients. Although for many of the treatments reliable evidence-based data are lacking a treatment algorithm was established but there is a strong need for further investigations concerning the therapeutic effectiveness in the treatment of CRPS.",
         "relevant":true
      },
      {
         "id":"10.1016/S0030-5898(03)00088-9",
         "title":"Effective physical treatment for chronic low back pain",
         "abstract":"It is now feasible to adopt an evidence-based approach when providing physical treatment for patients with chronic LBP. A summary of the efficacy of a range of physical treatments is provided in Table 1. The evidence-based primary care options are exercise, laser, massage, and spinal manipulation; however, the latter three have small or transient effects that limit their value as therapies for chronic LBP. In contrast, exercise produces large reductions in pain and disability, a feature that suggests that exercise should play a major role in the management of chronic LBP. Physical treatments, such as acupuncture, backschool, hydrotherapy, lumbar supports, magnets, TENS, traction, ultrasound, Pilates therapy, Feldenkrais therapy, Alexander technique, and craniosacral therapy are either of unknown value or ineffective and so should not be considered. Outside of primary care, multidisciplinary treatment or functional restoration is effective; however, the high cost probably means that these programs should be reserved for patients who do not respond to cheaper treatment options for chronic LBP. Although there are now effective treatment options for chronic LBP, it needs to be acknowledged that the problem of chronic LBP is far from solved. Though treatments can provide marked improvements in the patient's condition, the available evidence suggests that the typical chronic LBP patient is left with some residual pain and disability. Developing new, more powerful treatments and refining the current group of known effective treatments is the challenge for the future.",
         "relevant":true
      },
      {
         "id":"10.1586/14787210.2014.910112",
         "title":"A cure for HIV: is it in sight?",
         "abstract":"HIV is a devastating disease affecting millions of people worldwide despite the advent of successful antiretroviral therapy (ART). However, ART does not result in a cure and has to be taken for life. Accordingly, researchers are turning towards cure efforts, particularly in the light of two patients whose HIV has been seemingly eradicated. Numerous approaches and strategies have been considered for curing HIV, but no scalable and safe solution has yet been reached. With newly discovered difficulties in measuring the HIV reservoir, the main barrier to a cure, the only true test of cure is to stop ART and see whether the virus becomes detectable. However, it is possible that this treatment interruption may be associated with certain risks for patients. Here, we compare the current major approaches and recent advances for curing HIV, as well as discuss ways of evaluating HIV cure and the safety concerns involved.",
         "relevant":true
      },
      {
         "id":"10.1038/ni.3152",
         "title":"HIV reservoirs as obstacles and opportunities for an HIV cure",
         "abstract":"The persistence of HIV reservoirs remains a formidable obstacle to achieving sustained virologic remission in HIV-infected individuals after antiretroviral therapy (ART) is discontinued, even if plasma viremia has been successfully suppressed for prolonged periods of time. Numerous approaches aimed at eradicating the virus, as well as maintaining its prolonged suppression in the absence of ART, have had little success. A better understanding of the pathophysiologic nature of HIV reservoirs and the impact of various interventions on their persistence is essential for the development of successful therapeutic strategies against HIV or the long-term control of infection. Here, we discuss the persistent HIV reservoir as a barrier to cure as well as the current therapeutic strategies aimed at eliminating or controlling the virus in the absence of ART.",
         "relevant":true
      },
      {
         "id":"10.1186/s12981-017-0177-4",
         "title":"Eradication of HIV-1 latent reservoirs through therapeutic vaccination",
         "abstract":"Despite the significant success of combination anti-retroviral therapy to reduce HIV viremia and save lives, HIV-1 infection remains a lifelong infection that must be appropriately managed. Advances in the understanding of the HIV infection process and insights from vaccine development in other biomedical fields such as cancer, imaging, and genetic engineering have fueled rapid advancements in HIV cure research. In the last few years, several studies have focused on the development of \"Kick and Kill\" therapies to reverse HIV latency and kick start viral translational activity. This has been done with the aim that concomitant anti-retroviral treatment and the elicited immune responses will prevent de novo infections while eradicating productively infected cells. In this review, we describe our perspective on HIV cure and the new approaches we are undertaking to eradicate the established pro-viral reservoir.",
         "relevant":true
      },
      {
         "id":"10.1146/annurev.immunol.18.1.665",
         "title":"Reservoirs for HIV-1: mechanisms for viral persistence in the presence of antiviral immune responses and antiretroviral therapy",
         "abstract":"The success of combination antiretroviral therapy for HIV-1 infection has generated interest in mechanisms by which the virus can persist in the body despite the presence of drugs that effectively inhibit key steps in the virus life cycle. It is becoming clear that viral reservoirs established early in the infection not only prevent sterilizing immunity but also represent a major obstacle to curing the infection with the potent antiretroviral drugs currently in use. Mechanisms of viral persistence are best considered in the context of the dynamics of viral replication in vivo. Virus production in infected individuals is largely the result of a dynamic process involving continuous rounds of de novo infection of and replication in activated CD4(+) T cells with rapid turnover of both free virus and virus-producing cells. This process is largely, but not completely, interrupted by effective antiretroviral therapy. After a few months of therapy, plasma virus levels become undetectable in many patients. Analysis of viral decay rates initially suggested that eradication of the infection might be possible. However, there are several potential cellular and anatomical reservoirs for HIV-1 that may contribute to long-term persistence of HIV-1. These include infected cell in the central nervous system and the male urogenital tract. However, the most worrisome reservoir consists of latently infected resting memory CD4(+) T cells carrying integrated HIV-1 DNA. Definitive demonstration of the presence of this form of latency required development of methods for isolating extremely pure populations of resting CD4(+) T cells and for demonstrating that a small fraction of these cells contain integrated HIV-1 DNA that is competent for replication if the cells undergo antigen-driven activation. Most of the latent virus in resting CD4(+) T cells is found in cells of the memory phenotype. The half-life of this latent reservoir is extremely long (44 months). At this rate, eradication of this reservoir would require over 60 years of treatment. Thus, latently infected resting CD4(+) T cells provide a mechanism for life-long persistence of replication-competent forms of HIV-1, rendering unrealistic hopes of virus eradication with current antiretroviral regimens. The extraordinary stability of the reservoir may reflect gradual reseeding by a very low level of ongoing viral replication and/or mechanisms that contribute to the intrinsic stability of the memory T cell compartment. Given the substantial long-term toxicities of current combination therapy regimens, novel approaches to eradicating this latent reservoir are urgently needed.",
         "relevant":true
      },
      {
         "id":"10.1128/JVI.03331-13",
         "title":"HIV DNA subspecies persist in both activated and resting memory CD4+ T cells during antiretroviral therapy",
         "abstract":"The latent HIV reservoir is a major impediment to curing HIV infection. The contribution of CD4(+) T cell activation status to the establishment and maintenance of the latent reservoir was investigated by enumerating viral DNA components in a cohort of 12 individuals commencing antiretroviral therapy (ART) containing raltegravir, an integrase inhibitor. Prior to ART, the levels of total HIV DNA were similar across HLA-DR(+) and HLA-DR(-) (HLA-DR(±)) CD38(±) memory CD4(+) T cell phenotypes; episomal two-long terminal repeat (2-LTR) HIV DNA levels were higher in resting (HLA-DR(-) CD38(-)) cells, and this phenotype exhibited a significantly higher ratio of 2-LTR to integrated HIV DNA (P = 0.002). After 1 year of ART, there were no significant differences across each of the memory phenotypes of any HIV DNA component. The decay dynamics of integrated HIV DNA were slow within each subset, and integrated HIV DNA in the resting HLA-DR(-) CD38(-) subset per mm(3) of peripheral blood exhibited no significant decay (half-life of 25 years). Episomal 2-LTR HIV DNA decayed relative to integrated HIV DNA in resting cells with a half-life of 134 days. Surprisingly, from week 12 on, the decay rates of both total and episomal HIV DNA were lower in activated CD38(+) cells. By weeks 24 and 52, HIV RNA levels in plasma were most significantly correlated with the numbers of resting cells containing integrated HIV DNA. On the other hand, total HIV DNA levels in all subsets were significantly correlated with the numbers of HLA-DR(+) CD38(-) cells containing integrated HIV DNA. These results provide insights into the interrelatedness of cell activation and reservoir maintenance, with implications for the design of therapeutic strategies targeting HIV persistence.",
         "relevant":true
      },
      {
         "id":11813503,
         "title":"Study of the impact of HIV genotypic drug resistance testing on therapy efficacy",
         "abstract":"During recent years significant progress has been made in the treatment of HIV-1, at least in part due to the availability of potent antiretroviral drugs. The goal of the current treatment strategies is to inhibit the viral replication as completely as possible by using a combination of 3 or more antiretroviral drugs. This Highly Active Antiretroviral Therapy (HAART) has radically changed the clinical outcome of HIV, leading to decreased mortality and morbidity, at least in developed countries. Additionally to the advent of new and potent drugs, demonstrations of the prognostic value of the CD4 cell count and the plasma viral load were of major importance in the development of therapeutic strategies. Especially the ability of viral load assays to assess accurately the true level of viral replication, led to a better understanding of the pathogenesis of the disease. HIV proved to be a highly dynamic infection even during the period of clinical latency. The initial enthusiasm that HAART could radically change the outcome of HIV was cooled off in face of the difficulties in real life associated with the complex treatment strategies. Besides long-term side effects and suboptimal drug potency, the emergence of resistant virus and the necessity of perfect therapy adherence are major concerns for obtaining a sustained control of viral replication. In this study we focused on HIV resistance, which remains one of the major threats for a sustained response to antiretroviral therapy. HIV proved to be able to develop resistance to all currently used antiretroviral drugs. The high replication rate of the virus together with the low fidelity of the viral reverse transcriptase, from the basis for the presence of enormous amounts of viral variants. Whenever viral replication is ongoing in the presence of antiretroviral drugs, these variants that escape the inhibitory effects of the drugs will be selected. Although the knowledge in the field of HIV resistance has expanded enormously, many issues need to be answered. Genotypic and phenotypic resistance patterns are evolving continuously, due to changes in the treatment strategies. Moreover the relation between drug resistance and therapy failure needs further investigation, in order to prove the relevance of performing resistance testing in the follow-up of HIV-infected patients. The wide availability of antiretroviral drugs has led to the transmission of resistant HIV. Infection with HIV resistant to one or more antiretroviral drugs has been observed to occur through the different transmission routes. In a first study we assessed the prevalence of genotypic resistance to antiretroviral drugs in Belgian antiretroviral-naïve HIV-infected patients. We observed that HIV strains with resistance-related mutations to one or more classes of antiretroviral drugs are not uncommon in the Belgian naïve patients. Furthermore the inclusion of samples from patients visiting the Belgian hospitals for the first time in 1995, 1997 and 1998, showed that the overall prevalence of baseline genotypic resistance remains rather constant (26-30%). The increasing trend in genotypic baseline resistance to 3TC (2% to 6.3%) and PIs (4.4% to 9.9%) as well as the decrease in ZDV-resistance (13.3% to 5.4%), reflect the change in treatment strategies, and resistance to these drugs is most probably caused by transmission of variants with resistance mutations selected during therapy. The presence of NNRTI-related mutations (around 16%) is likely to reflect the occurrence of baseline polymorphisms, since NNRTI-related mutations can occur without a replication deficit for the virus. Moreover, despite the rather recent introduction of NNRTIs into the clinic from 1997 onwards, no clear trend in NNRTI baseline resistance over time is observed. The best current therapeutic strategy for HIV-infected patients is to start antiretroviral therapy with HAART in order to avoid the accumulation of resistance towards drugs in less suppressive regimens. In a second study, we showed that the start of HAART in antiretroviral-naive HIV-patients in daily clinical practice could prevent viral breakthrough for up to 44 months in 60% of patients (n = 25). Six of 10 patients with virologic failure developed resistance to the drugs included in their treatment regimens. In comparison to patients with a sustained virologic response, patients with virologic failure were in a later disease stage when starting therapy and showed lower PI drug-levels, what can be an indication of poor adherence. Despite a poor virologic response for some of them, a rise in CD4 cell count was observed for all patients during the study period. A large number of HIV-infected patients started treatment in the pre-HAART period. The use of NRTIs is mono- or bitherapy was not able to prevent the development of resistant virus. In a third study we studied the prevalence and characteristics of 2 patterns of multinucleoside resistance (MNR) in European patients (n = 755). In patients without NRTI-exposure or with exposure to only one NRTI, no MNR was observed. MNR was present in low prevalence (each pattern < 2%) in patients pretreated with multiple NRTIs. Despite this low prevalence, MNR should be closely monitored, since it results in broad cross-resistance to NRTIs in vitro and a poor therapy response in vivo. We also assessed the predictive value of baseline resistance on the virologic response to later added drugs. The genotype of patients starting or changing a therapy consisting solely of NRTIs was analyzed at baseline and 6 months later. In patients without genotypic mutations towards the added drug the virologic response was significantly better compared to patients with baseline resistance. At 6 months however, both patient groups showed a rise in viral load due to the accumulation of NRTI-related mutations under the presence of poorly suppressive regimens, although the difference between the two groups remained significant. In this study, and also in studies reported by others, the presence of baseline resistance has a high predictive value for therapy failure, while the absence of resistance is not predictive for therapy response. Suboptimal adherence may be one of the reasons of the poor predictive value of the absence of baseline resistance for a good therapy response. In a last observational study, we investigated the relation between adherence, the presence and development of genotypic resistance and the virologic response in patients during HAART therapy. Adherence to 1 protease inhibitor was monitored using Electronic Event Monitoring. Patients with perfect therapy adherence and in particular without drug holidays, can control viral replication provided that the activity of the drugs included in the combination is not entirely compromised by the presence of baseline resistance mutations. In our patient population, reduced adherence resulted in therapy failure, mostly associated with a subsequent accumulation of resistance mutations. In conclusion, the outcome of the HIV disease has been revolutionarily changed with the advent of HAART. Both resistance and treatment adherence are crucial factors in determining the therapy response. Retrospective studies, such as ours, and a limited number of prospective trials already proved the short-term benefit of therapy switch based on the results of resistance tests in addition to standard of care. To ultimately define the role of tools as resistance testing and adherence monitoring with eventual adherence interventions, more prospective trials are needed as well in treatment-naïve as in experienced patients.",
         "relevant":true
      },
      {
         "id":14640390,
         "title":"Predictors of the virological response to a change in the antiretroviral treatment regimen in HIV-1-infected patients enrolled in a randomized trial comparing genotyping, phenotyping and standard of care (Narval trial, ANRS 088)",
         "abstract":"Objective: To identify predictors of the virological response to antiretroviral therapy in patients in whom initial therapy has failed.\n\nMethods: The Narval trial was designed to compare phenotyping, genotyping and standard of care for the choice of antiretroviral therapy in patients in whom a protease inhibitor (PI)-containing regimen had failed. Virological success was defined as viral load below 200 copies/ml at week 12. Baseline variables including demographic, clinical and biological characteristics, HIV reverse transcriptase and protease mutations, the randomization arm, the drugs prescribed, as well as adherence to treatment and plasma concentrations of PIs and non-nucleoside reverse transcriptase inhibitors (NNRTIs) at week 12 were tested in the model. Variables that were significantly associated with virological success in univariate analysis were included in a logistic regression model.\n\nResults: Five-hundred-and-forty-one patients were randomized. Virological success at week 12 was obtained in 200 patients. In multivariate analysis, the following factors were significantly associated with virological success: prescription of efavirenz to NNRTI-naive patients (OR=4.37; 95% CI: 2.76-6.90), randomization to the genotyping arm (OR=2.13, 1.20-3.79), prescription of lamivudine (OR=1.69, 1.01-2.83) and prescription of abacavir to abacavir-naive patients (OR=1.66, 1.02-2.72). Factors significantly associated with virological failure were prescription of nelfinavir (OR=0.30, 0.13-0.68), a high baseline viral load (OR=0.37, 0.28-0.50), the presence of at least five PI mutations (OR=0.42, 0.26-0.66), the presence of at least three thymidine analogue mutations (OR=0.61, 0.39-0.97) and at least 30 months of prior PI exposure (OR=0.64, 0.41-0.99).\n\nConclusions: These results confirm that among heavily pretreated patients, prescription of efavirenz to NNRTI-naive patients is associated with a good virological response, while a high baseline viral load, a large number of PI mutations and nelfinavir prescription at baseline are associated with a poor virological response. Genotyping was found to be beneficial, while this was not the case for phenotyping. This work was presented at the XI International HIV Drug Resistance Workshop, Sevilla, Spain, July 3-6 2002 (Abstract N(o)133); and at the XIV International Conference on AIDS, Barcelona, Spain, July 7-11 2002 (Abstract N(o)ThOrB138).",
         "relevant":true
      },
      {
         "id":"10.1016/j.jcv.2018.05.008",
         "title":"Accumulated pre-switch resistance to more recently introduced one-pill-once-a-day antiretroviral regimens impacts HIV-1 virologic outcome",
         "abstract":"Background: One-pill-once-a-day regimens (OPODs) appeal to providers and patients. The impact of resistance to OPODs in routine clinical care is important yet unclear, particularly in treatment-experienced patients.\n\nObjectives: We hypothesized that resistance to any OPOD component impacts treatment success and that historical, vs. most recent, resistance better predicts it.\n\nStudy design: In the largest RI HIV Center, we identified all patients starting/switching to Complera/Stribild, evaluated their 12-month viral load (VL) suppression, and examined the impact of demographic, clinical and laboratory data on it, focusing on recent-only vs. accumulated significant resistance, defined as low-, intermediate- or high-level predicted resistance to any OPOD component. Associations with outcomes were evaluated using Fisher exact and Wilcoxon rank sum tests. Hypotheses were tested using logistic regression.\n\nResults: Of 1624 patients, 224 started/switched to Complera or Stribild, mean age 44 years, 8 years post-diagnosis, CD4 468 cells/μL; 183 treatment-experienced (140 with genotypes; 61% suppressed at switch). Significant OPOD-associated resistance was in 30% by recent-only genotypes, and 38% by all genotypes. 12-month VL suppression was in 83% of treatment-experienced participants: 96% of suppressed at switch, associated with older age, higher CD4, fewer prior genotypes, less accumulated resistance, and better adherence; and 61% of unsuppressed at switch, associated with better adherence. Accumulated resistance independently predicted 12-month failure, better than most-recent resistance only.\n\nConclusion: 12-month VL suppression with Complera/Stribild was high, suggesting that OPODs remain options even for experienced patients. Clinicians should consider resistance history before switching to OPODs and continue to focus on improving adherence.",
         "relevant":true
      },
      {
         "id":"10.1310/hct1202-79",
         "title":"Lower CD4 cell count and higher virus load, but not antiretroviral drug resistance, are associated with AIDS-defining events and mortality: an ACTG Longitudinal Linked Randomized Trials (ALLRT) analysis",
         "abstract":"Background: We hypothesized that drug resistance mutations would impact clinical outcomes associated with HIV-1 infection.\n\nMethods: A matched case-control study of participants in AIDS Clinical Trials Group Longitudinal Linked Randomized Trials (ALLRT). Cases experienced an AIDS-defining event (ADE) or mortality, and controls did not. One hundred thirty-four cases were identified and matched to a total of 266 controls by age, sex, treatment regimen, and length of follow-up. Both cases and controls had HIV RNA levels of ≥ 500 copies/mL within 24 weeks of an event. Population-based genotyping at or near the time of the event was used to evaluate the impact of resistance mutations on incidence of ADE and/or death using conditional logistic regression models.\n\nResults: One hundred four cases and 183 controls were analyzed. Median time to event was 99 weeks; 6 cases were deaths. At baseline, cases had lower CD4 (median 117 vs 235 cells/mm3; P < .0001) and higher HIV RNA levels (median 205,000 vs 57,000 copies/mL; P = .003). No significant differences in resistance were seen between cases and controls.\n\nConclusions: In this rigorously designed case-control study, lower CD4 cell counts and higher virus loads, not antiretroviral drug resistance, were strongly associated with ADE and mortality.",
         "relevant":true
      },
      {
         "id":17503744,
         "title":"Evolution of resistance mutations during low-level viral replication in HIV-1-infected patients treated with zidovudine/lamivudine/abacavir as a first-line regimen",
         "abstract":"Objective: Long-term evaluation of viral evolution in patients who continued first-line therapy with zidovudine/lamivudine/abacavir (Trizivir [TZV]) in the presence of low-level viral replication and assessment of the impact of mutational patterns selected under TZV on viral load (VL), CD4+ T-cell count (CD4) and subsequent therapeutic options.\n\nDesign: Analysis of viral evolution based on genotypic resistance tests (GRT) from samples collected during non-suppressive first-line therapy with TZV.\n\nMethods: Patients from the Frankfurt HIV cohort with at least 3 months uninterrupted first-line therapy with TZV in whom VL and CD4 measurements were performed at baseline and at follow up were identified. Criteria for virological failure (VF) were two consecutive VL >400 copies/ml. GRTs were required at baseline, VF and last visit (LV).\n\nResults: Initially, 23/119 patients were classified as VF; 4/23 were lost to follow up. Median time to VF was 48 weeks. Because of the observed virological and immunological benefit, patients continued TZV for a median of 87 weeks despite detectable viraemia. Median CD4 increase and VL reduction at LV were 120 cells/mm3 and 317,100 copies/ml, respectively, compared to baseline. After 54 weeks of treatment with detectable VL, three mutational patterns were observed: Group A (n=4) characterized by M184V without further regimen-associated mutations, group B (n=9) by M184V accompanied by one to three thymidine analogue mutations (TAMs), and group C (n=6) by M184V and four to six TAMs. No virological or CD4 parameters correlated with these patterns. Group A remained unchanged, thus preserving activity of most nucleoside analogues (NA). However, in the majority of patients (groups B and C) accumulation of mutations at different rates was observed, leading to a sequential loss of NA options.\n\nConclusions: Continuous treatment with TZV in the presence of viral replication is associated with a stepwise accumulation of resistance mutations. M184V was present in all cases, not followed by further selection of TAMs in a small, unpredictable subgroup of patients. However, in the majority of patients selection of M184V was associated with accumulation of TAMs at different rates leading to a substantial loss of active NAs, despite continuous virological and immunological benefit when compared with baseline.",
         "relevant":true
      },
      {
         "id":17310827,
         "title":"Antiretroviral efficacy and virological profile of a zidovudine/lamivudine/tenofovir disoproxil fumarate combination therapy in antiretroviral-naive patients",
         "abstract":"Objective: To study the antiviral efficacy and the mutations selected by a triple therapy with zidovudine (AZT), lamivudine (3TC) and tenofovir disoproxil fumarate (TDF).\n\nMethods: Antiretroviral-naive patients received 300 mg AZT/150 mg 3TC twice a day plus 300 mg TDF once a day in an open pilot study. Follow-up was assessed at baseline therapy (MO) and at months 1, 3, 6, 9 and 12. Reverse transcriptase (RT) genotypic resistance analysis and in selected cases, a recombinant drug susceptibility and replication capacity assay were performed from plasma RNA at baseline and in case of virological failure (VF); that is, rebound of viral load >50 copies/microl on therapy.\n\nResults: Twenty-four patients were included. At baseline, the median CD4+ T-cell count was 443 cells/microl and the median plasma viral load (VL) was 4.38 log10 copies/ml. RT resistance mutations were observed at MO in 4 patients. At M12, the proportion of patients with a VL <50 copies/ml reached 88% using an on-treatment analysis and 67% with an intent-to-treat analysis. The median increase in CD4+ T cells at M12 was 94 cells/microl. Four patients had a VF on therapy: two with wild-type viruses, one with selection of M184V and thymidine analogue mutations (TAMs) on a background of TAMs, and one with selection of K65R and M184V, with a replication capacity at 2.4%/o.\n\nConclusion: The virological response in our study demonstrates the antiviral efficacy of the AZT/3TC/TDF combination therapy, which needs further evaluation. The moderate frequency of selection of K65R could be due to the presence of AZT in the regimen.",
         "relevant":true
      },
      {
         "id":15000589,
         "title":"Genotypic resistance tests for the management of the HIV-infected pregnant woman",
         "abstract":"Witness for the prosecution: Recommendations for genotypic resistance testing in HIV-infected pregnant women are the same as for non-pregnant women: acute HIV infection, virological failure or suboptimal viral suppression after initiation of antiretroviral therapy, or high likelihood of exposure to resistant virus based on community prevalence or source characteristics. All pregnant women with detectable HIV-RNA levels should perform resistance testing to maximize the response to antiretrovirals in pregnancy. Currently there are no data on the value of drug resistance testing to prevent vertical transmission. Most studies show that the most important factor in the risk of transmission is the amount of HIV-RNA at the moment of delivery. A strategy to overcome this problem would be to use of resistance testing to select a regimen, which has the greatest potential to reduce viral load at the moment of delivery. We would also like to use the same information to select the regimen that would be used to provide prophylaxis to the newborn. It is currently unknown whether zidovudine (ZDV) prevents transmission through another mechanism(s) in addition to reducing viral load, so one could argue that even if ZDV resistance has been found in the mother, it should still be included in the regimen. Witness for the defence: To reduce the risk of HIV vertical transmission, prospective controlled trials on the use of antiretroviral prophylactic treatment in different schedules during pregnancy were conducted. These studies assessed the efficacy of short- or medium-term antiretroviral therapy in reducing vertical transmission, but highlighted the concerns about the selection of resistant variants (monotherapy prophylaxis or suboptimal regimens). The availability of recent more complex multidrug regimens increased the prevalence of drug resistance among the HIV-1-infected population; so, women of childbearing age are at risk of becoming infected with resistant virus and those on treatment, living in developed countries, could harbour resistant virus before pregnancy. Therefore, there are growing concern about the role of these resistant variants in mother-to-child HIV-1 transmission. Several studies documenting HIV-resistant variants in vertical transmission form a compelling basis for recommending the use of HIV-1 genotypic drug resistance tests during pregnancy. Owing to the availability of different genotypic HIV-1 tests at variable costs, the choice of the most appropriate assay could take into account the prevalence and incidence of drug-resistant mutations, the availability of drugs and the antiretroviral experience setting, to choose the best long-term effective antiretroviral therapy for the mother and to avoid the risk of transmission to the offspring.",
         "relevant":true
      },
      {
         "id":"10.1111/ecc.13139",
         "title":"Major patterns of cancer cure: Clinical implications",
         "abstract":"Introduction: This review aimed to classify major patterns of cancer cure and discuss clinical implications. Patterns of cancer cure were identified, in terms of long-term survival and life expectancy, by means of two recently estimated indicators: cure fraction (CF) and time to cure (TTC).\n\nMethods: We considered population-based studies reporting results for some cancer types on CF, defined as the proportion of patients who will reach the same life expectancy of the general population, or/and TTC, the time span necessary to experience a negligible excess mortality. TTC is obtained using conditional relative survival, which indicates the probability of surviving an additional y number of years, given that patients already survived x number of years.\n\nResults: Four major patterns of cancer types emerged from published studies: (a) cancers with a CF > 60% and a TTC < 5 years (e.g., testicular, thyroid); (b) cancers with a CF between 20% and 50% and a TTC < 10 years (colon, rectum); (c) cancers showing a CF of approximately 50% and TTC > 10 years (breast, prostate and bladder); (d) cancers with a CF < 20% and uncertain TTC (lung or pancreas).\n\nConclusion: Clinical and social impact of \"cancer cure\" categorisation are discussed in details. Recognising a cancer patient as cured represents an opportunity to improve their quality of life.\n\nKeywords: cancer cure; cancer survivors; life expectancy; time to cur",
         "relevant":true
      },
      {
         "id":"10.3389/fmicb.2019.01956",
         "title":"Impacts of HIV Cure Interventions on Viral Reservoirs in Tissues",
         "abstract":"HIV reservoirs persist in infected individuals despite combination antiretroviral therapy and can be identified in secondary lymphoid tissues, in intestinal tissues, in the central nervous system as well as in blood. Clinical trials have begun to explore effects of small molecule interventions to perturb the latent viral infection, but only limited information is available regarding the impacts of HIV cure-related clinical interventions on viral reservoirs found in tissues. Of the 14 HIV cure-related clinical trials since 2012 that have evaluated the effects of small molecule interventions in vivo, four trials have examined the impacts of the interventions in peripheral blood as well as other tissues that harbor persistent HIV. The additional tissues examined include cerebral spinal fluid, intestines and lymph nodes. We provide a comparison contrast analyses of the data across anatomical compartments tested in these studies to reveal where peripheral blood analyses reflect outcomes in other tissues as well as where the data reveal differences between tissue outcomes. We also summarize the current knowledge on these topics and highlight key open questions that need to be addressed experimentally to move the HIV cure research field closer to the development of an intervention strategy capable of eliciting long-term antiretroviral free remission of HIV disease.",
         "relevant":true
      },
      {
         "id":27168271,
         "title":"Onychomycosis: Does Cure Equate to Treatment Success?",
         "abstract":"Background: There is no general agreement as to what constitutes cure or treatment success in onychomycosis. Regulatory guidelines differ in the United States and Europe, and outcomes reported in clinical trials do not consistently report secondary endpoints.<br/>\n\nMethods: We reviewed definitions of onychomycosis cure to develop a less stringent and more practical approach to assess improvement and treatment success.<br/>\n\nResults: Complete cure (totally clear nail and mycologic cure) remains an important regulatory standard. Mycologic cure (negative fungal culture and negative potassium hydroxide) is the only consistently reported outcome in clinical trials, however the potential for discrepancies between microscopy and culture can be problematic. We propose a more practical approach to assessing improvement in infected nails that relies on both physician and patient input in a similar fashion to other skin diseases.<br/>\n\nConclusions: Treatment success should be based on both physician and patient assessment of improvement in the affected toenails and negative fungal culture. <br /><br /> <em>J Drugs Dermatol. </em>2016;15(5):626-632.",
         "relevant":true
      },
      {
         "id":"10.1016/S0140-6736(00)02799-9",
         "title":"Adverse drug reactions: definitions, diagnosis, and management",
         "abstract":"We define an adverse drug reaction as \"an appreciably harmful or unpleasant reaction, resulting from an intervention related to the use of a medicinal product, which predicts hazard from future administration and warrants prevention or specific treatment, or alteration of the dosage regimen, or withdrawal of the product.\" Such reactions are currently reported by use of WHO's Adverse Reaction Terminology, which will eventually become a subset of the International Classification of Diseases. Adverse drug reactions are classified into six types (with mnemonics): dose-related (Augmented), non-dose-related (Bizarre), dose-related and time-related (Chronic), time-related (Delayed), withdrawal (End of use), and failure of therapy (Failure). Timing, the pattern of illness, the results of investigations, and rechallenge can help attribute causality to a suspected adverse drug reaction. Management includes withdrawal of the drug if possible and specific treatment of its effects. Suspected adverse drug reactions should be reported. Surveillance methods can detect reactions and prove associations.",
         "relevant":true
      },
      {
         "id":"10.1016/j.revmed.2013.02.023",
         "title":"[Drug-induced fever: a diagnosis to remember]",
         "abstract":"Drug fever (DF) is a febrile reaction induced by a drug without additional clinical features like skin eruption. This adverse drug reaction is probably common but under diagnosed. While its outcome is generally favourable, DF generates unnecessary diagnostic procedures as well as hospitalisations or hospitalisation prolongations. Clinical presentation and biological findings are not specific. Fever is generally well tolerated but may be accompanied by general symptoms mimicking sepsis. Moderate biological disorders could be expected, including elevation or decrease in white blood cell count, eosinophilia, liver cytolysis, and increased C-reactive protein. An infection should be systematically ruled out. Clinical or biological signs of severity should question DF diagnosis. When DF is suspected, the involved drug(s) should be stopped after a reliable assessment of imputability. Antibiotics represent the most often implicated drugs. Fever disappearance after discontinuing the suspected drug is the cornerstone of DF diagnosis. Before stopping the administration of the suspected drug(s), a risk/benefit ratio assessment is necessary. Consistently, it may be complicated to stop an antimicrobial drug when treating an infection or an immunosuppressive drug if required.",
         "relevant":true
      },
      {
         "id":"10.1007/s00264-018-3909-8",
         "title":"Antibiotic-induced fever in orthopaedic patients-a diagnostic challenge",
         "abstract":"Introduction: Antibiotic-induced fever is a probably underestimated complication, which may be misdiagnosed as new infection. In this study, characteristics, diagnostic approach, and outcome of antibiotic-induced fever in patients treated for musculoskeletal infections are described.\n\nMethods: We retrospectively reviewed all patients with antibiotic-induced fever after surgery treated at our institution from 2014 to 2017. Antibiotic-induced fever was diagnosed, if the following criteria were fulfilled: (i) central (ear) body temperature > 38.0 °C; (ii) intravenous antibiotics for > three days; (iii) exclusion of infectious or other non-infectious causes of fever; and (iv) defervescence after discontinuation of antibiotics.\n\nResults: We included 11 patients (median age 51 years) treated for infection after fracture fixation (n = 5), periprosthetic joint infections (n = 3), infection after spinal instrumentation (n = 1), and soft tissue infection (n = 2). The suspected antibiotics inducing fever were beta-lactam antibiotics (n = 9), vancomycin (n = 3), daptomycin (n = 2), clindamycin, and meropenem (n = 1 each). Additional clinical findings were reduced general condition, generalized exanthema, and rigors, whereas five patients were asymptomatic apart from a fever. Leukopenia was observed in nine patients and increase of C-reactive protein value in ten patients. Fever occurred after a median of 20 days of antibiotic treatment and resolved after a median of one day after discontinuation of the suspected antibiotic.\n\nConclusions: Antibiotics should be considered as the possible cause of fever in orthopaedic patients receiving antimicrobial treament whenever clinical signs of new or persisting infection are lacking. Important hints suggestive for antibiotic-induced fever are good general condition despite high temperature and progressive leukopenia. Discontinuation or change to another substance leads to prompt defervescence, preventing unnecessary diagnostic procedures and antibiotic treatment.\n\nKeywords: Adverse event; Antibiotic; Fever; Musculoskeletal infections.",
         "relevant":true
      },
      {
         "id":21229871,
         "title":"Preclinical drug development",
         "abstract":"Life sciences provide reasonably sound prognosis for a number and nature of therapeutic targets on which drug design could be based, and search for new chemical entities--future new drugs, is now more than ever based on scientific principles. Nevertheless, current very long and incredibly costly drug discovery and development process is very inefficient, with attrition rate spanning from many thousands of new chemical structures, through a handful of validated drug leads, to single successful new drug launches, achieved in average after 13 years, with compounded cost estimates from hundreds of thousands to over one billion US dollars. Since radical pharmaceutical innovation is critically needed, number of new research projects concerning this area is steeply rising outside of big pharma industry--both in academic environment and in small private companies. Their prospective success will critically depend on project management, which requires combined knowledge of scientific, technical and legal matters, comprising regulations concerning admission of new drug candidates to be subjects of clinical studies. This paper attempts to explain basic rules and requirements of drug development within preclinical study period, in case of new chemical entities of natural or synthetic origin, which belong to low molecular weight category.",
         "relevant":true
      },
      {
         "id":10387985,
         "title":"Anticancer Drug Development: The Way Forward",
         "abstract":"Cancer chemotherapy celebrated its fiftieth anniversary last year. It was in 1945 that wartime research on the nitrogen mustards, which uncovered their potential use in the treatment of leukaemias and other cancers, was first made public. Fifty years later, more than sixty drugs have been registered in the USA for the treatment of cancer, but there are still lessons to be learnt. One problem, paradoxically, is that many anticancer agents produce a response in several different classes of the disease. This means that once a new agent has been shown to be effective in one cancer, much effort is devoted to further investigations of the same drug in various combinations for different disorders. While this approach has led to advances in the treatment of many childhood cancers and some rare diseases, a plethora of studies on metastatic colon cancer, for example, has yielded little benefit. 5-fluorouracil continues to be used in trials, yet there is no evidence for an increase in survival. The lesson to be learnt is that many common cancers are not adequately treated by present-day chemotherapy, and most trials of this sort are a waste of time. Significant increases in survival will only occur if the selectivity of present-day anticancer agents can be increased or new classes of more selective agents can be discovered. There are two fundamental problems in drug development: a lack of suitable laboratory tests and the difficulty of conducting early clinical trials. Firstly, no existing laboratory method can accurately predict which chemical will be effective against a particular class of human cancer. At best, tests can demonstrate a general 'anticancer' property. This is well exemplified by the discovery of cisplatin. The fact that cisplatin caused regression in a number of transplanted rodent tumours created no great excitement amongst chemotherapists. It was only later when it was tested clinically against ovarian cancer that results were sufficiently positive to encourage others to investigate. Only then was it discovered that metastatic teratoma was extraordinarily sensitive to the drug. This finding was made as a result of phase II trials and no laboratory model could have predicted it. The lesson to be learnt is that new drugs should be tested extensively in phase II trials before they are discarded. The second problem concerns early clinical trials. Because new drugs can only be tested against advanced and usually heavily pretreated disease, it is unlikely that dramatic responses will occur. The methods used to detect responses in solid tumours and metastases are crude, and it is likely that many useful drugs are missed. New techniques are needed to detect small but important responses. In addition to these technical problems, clinical trials are expensive and the time required for preclinical pharmacology and toxicology is lengthy. In the early days, drugs could enter clinical trials after fairly simple toxicological studies. The thalidomide disaster in the 1960s, however, led to the setting up of regulatory bodies to scrutinize drugs before clinical trials. This proved detrimental for cancer drug development because a series of fairly long-term tests is now required. These must be carried out in both rodents and one other species, usually the dog. This approach was probably a good thing for most medicines where a large margin of safety is required between the therapeutic dose and the dose which causes side effects, but was inappropriate for anticancer agents which are tested at the maximum possible dose which gives manageable side effects. These new regulations meant that the cost of one clinical trial after the 1970s was equivalent to the cost of ten before that time. Solutions to these problems are available, although to put them into practice would require the cooperation of government regulatory authorities, the pharmaceutical industry and other organisations such as the US National Cancer Institute (NCI), the UK Cancer Research Campaign (CRC) and the European Organisation for Research and Treatment of Cancer (EORTC). Firstly, it is important to switch from clinical trials of analogues and combinations of standard drugs to trials of new classes of anticancer agents. Further, an international effort should be launched whereby these new agents can be rapidly tested in phase II trials against common solid cancers using new techniques to detect small but significant tumour responses. Lead chemicals discovered in this way could then be taken back to the laboratory for further development. There is no shortage of new drugs which act by mechanisms quite different from present-day agents, and new approaches can greatly increase the amount of cytotoxic agents delivered to solid tumours. As long ago as 1980, the CRC introduced protocols which enabled early clinical trials to be carried out rapidly and with minimal cost. These procedures used short-term tests only in rodents to determine a safe starting dose. The test can be completed within six months and around fifty clinical trials using this protocol have been successfully carried out in collaboration with the EORTC. Despite this, the American Food and Drug Administration (FDA), regulatory authorities in many other countries and many drug companies still insist on using a second animal species before a phase I clinical trial is permitted instead of using the money spent to develop several agents with minimal toxicology testing. The EORTC and CRC also plan to introduce positron emission tomographic scanning into early clinical trials as a highly sensitive method of measuring tumour response. Cancer mortality has changed little over the past forty years, mainly because of our failure to develop curative chemotherapy for the common solid cancers. The way forward is to carry out extensive phase I and II clinical trials of the many new types of anticancer agent that have become available as a result of increased knowledge about cancer cells and how they differ from normal tissues. In order to do this, the regulatory authorities must recognize that minimal toxicology protocols are adequate, and drug companies must be persuaded to give more emphasis to the search for new chemotherapeutic agents. A coordinated effort to achieve these aims would be a wonderful way to mark the fiftieth anniversary of modern chemotherapy. Unfortunately the regulatory authorities find it less risky to stick with extensive safety testing rather than to use shortcuts, however well-validated clinically. Many but not all drug companies, mindful of profits, prefer the easy way out and concentrate on analogues, while most clinicians opt for trials of combinations of known agents, being aware that they are worth a publication or two. Reprinted with permission from Helix, Volume V, Issue 1, 1996, pp. 20-21.",
         "relevant":true
      },
      {
         "id":16445119,
         "title":"Informed toxicity assessment in drug discovery: systems-based toxicology",
         "abstract":"Technological advances in the biological, chemical and in silico sciences have transformed many scientific disciplines, including toxicology. A vast new palate of toxicity testing tools is now available to investigators, enabling the generation of enormous amounts of data using only small amounts of test sample and at relatively low cost. In addition to these tools, the pharmaceutical industry has an urgent need for toxicity testing earlier in the process, based on the recognition that safety issues are the single largest cause of drug candidate attrition from development portfolios and the marketplace. However, along with the opportunity provided by new testing tools comes the dilemma of deciding which tools to use and, equally as important, when and why to use them. It may well be unwise to apply a new toxicity test or screening system simply because one can, as both false positive and false negative outcomes can quickly negate the value of a toxicity test system and may even have a net negative impact on drug discovery productivity. This can be true even of test systems that are considered to be 'validated' in the traditional sense. How then is an investigator or drug discovery organization to decide which of the new tools to use, and when to use them? Proposed herein is a strategy for identifying high-value toxicity testing systems and strategies based on program knowledge and informed decision-making. The decision to apply a certain toxicity testing system in this strategy is informed by knowledge of the pharmacological target, the chemical features of molecules active at the pharmacological target, and existing public domain or institutional learning. This 'fit-for-purpose' approach limits non-targeted or 'uninformed' toxicity screening to only those few test systems with high specificity, strong outcome concordance and molecular relevance to frequently encountered toxicity risks (eg, genotoxicity). Additional toxicity testing and screening is then conducted to address specific known or potential toxicity risks, based on existing knowledge of the target pharmacology and secondary pharmacology or chemical attributes with known or suspect risk, and by active 'interrogation' of both the target and active chemical moieties during the drug discovery process. This model for toxicity testing decision-making is illustrated by two case studies from recent experience.",
         "relevant":true
      },
      {
         "id":17240921,
         "title":"[Development of antituberculous drugs: current status and future prospects]",
         "abstract":"Worldwide, tuberculosis (TB) remains the most frequent and important infectious disease causing morbidity and death. One-third of the world's population is infected with Mycobacterium tuberculosis (MTB), the etiologic agent of TB. The World Health Organization estimates that about eight to ten million new TB cases occur annually worldwide and the incidence of TB is currently increasing. In this context, TB is in the top three, with malaria and HIV being the leading causes of death from a single infectious agent, and approximately two million deaths are attributable to TB annually. In particular, pulmonary TB, the most common form of TB, is a highly contagious and life-threatening infection. Moreover, enhanced susceptibility to TB in HIV-infected populations is another serious health problem throughout the world. In addition, multidrug-resistant TB (MDR-TB) has been increasing in incidence in many areas, not only in developing countries but industrialized countries as well, during the past decade. These situations, particularly the global resurgence of TB and the rapid emergence of MDR-TB, underscore the importance of the development of new antituberculous drugs and new protocols for efficacious clinical control of TB patients using ordinary antimycobacterial drugs. Concerning the development of new antituberculous drugs, the following points are of particular importance. (1) Development of drugs which display lasting antimycobacterial activity in vivo is desirable, since they can be administered with long intervals and consequently facilitate directly observed therapy and enhance patient compliance. (2) Development of novel antituberculosis compounds to combat MDR-TB is urgently needed. (3) The eradication of slowly metabolizing and, if possible, dormant populations of MTB organisms that cause relapse, using new classes of anti-TB drugs is very promising for prevention of TB incidence, because it will markedly reduce the incidence of active TB from persons who are latently infected with MTB. Unfortunately, no new drugs except rifabutin and rifapentine has been marketed for TB in the US and other countries during the 40 years after release of rifampicin. There are a number of constraints that have deterred companies from investing in new anti-TB drugs. The research is expensive, slow and difficult, and requires specialized facilities for handling MTB. There are few animal models that closely mimic the human TB disease. Development time of any anti-TB drug will be long. In fact, clinical trials will require the minimum six-month therapy, with a follow-up period of one year or more. In addition, it is hard to demonstrate obvious benefit of a new anti-TB agents over pre-existing drugs, since clinical trials involve multidrug combination therapy using highly effective ordinary anti-TB drugs. Finaly, there is the perceived lack of commercial return to companies engaged in the development of new anti-TB drugs, because over 95% of TB cases worldwide are in developing countries. In this symposium, we reviewed the following areas. 1. Critical new information on the entire genome of MTB recently obtained and increasing knowledge of various mycobacterial virulence genes are greatly promoting the identification of genes that code for new drug targets. In this context, Dr. Namba reviewed the status of new types of compounds which are being developed as anti-TB drug. He also discussed the development of new antimycobacterial drugs according to new and potential pharmacological targets and the best clinical development plans for new-TB drugs in relation to corporate strategy. 2. Using such findings for mycobacterial genomes, bioinformatics/genomics/proteomics-based drug design and drug development using quantitative structure-activity relationships may be possible in the near future. In this context, Dr. Suwa and Dr. Suzuki reviewed the usefulness of chemical genomics in searching novel drug targets for development of new antituberculous drugs. The authors reviewed (1) the history and present status of chemical genomics that is defined as the systemic search for a selective small molecular modulator for each function of all gene products, (2) recent studies of the authors on profiles of the interactions between various kinds of human proteins and small molecule modulators using the new technology devised by Reverse Proteomics Research Institute, and (3) future prospects of the development of new antituberculous drugs based on chemical genomics. 3. It appears also promising to develop new types of drug administration systems using drug vehicles, which enable efficacious drug delivery to their target in vivo. Dr. Izumikawa, Dr. Ohno and Dr. Kohno reviewed the usefulness of liposome- and polymer-based technologies, which enable efficacious delivery of encapsulated drugs at required doses for prolonged periods of time with only a single shot without toxicity, and also enable highly targeted delivery of drugs to their target in vivo. They indicated that the applications of drug delivery system using conventional anti-mycobacterial agents are challenging to improve the compliance of treatment and better clinical outcome. 4. Immunoadjunctive therapy appears to be promising in improving outcome of clinical control of refractory mycobacterial infections, including MDR-TB and M. avium complex infection. Dr. Shimizu, Dr. Sato and Dr. Tomioka reviewed the present status of immunotherapy of mycobacterial infections in combination with antimycobacterial drugs. They indicated that the development of new classes of immunomodulators other than cytokines (IL-2, IFN-gamma, GM-CSF, IL-12, etc.) particularly those with no severe side-effects, are urgently needed. Their review dealed with some promising immunoadjunctive agents, especially ATP and its analogues, which potentiate macrophage antimycobacterial activity via purinergic P2 receptors. The aim of this symposium is to address the future prospects of the development of new drugs and drug regimens for anti-TB chemotherapy. There are a number of difficulties in drug-design for the development of new drug formulations with increased potential for antimycobacterial effects, excellent pharmacokinetics, and tolerability. It should be emphasized that the most urgent goal of chemotherapy of TB and MAC infections, especially that associated with HIV infection, is to develop highly active, low-cost drugs which can be used not only in industrialized countries but also in developing countries, since the incidences of AIDS-associated intractable TB and MAC infections are rapidly increasing in the latter. We strongly wish a great advance of fundametal and practical studies in developing such kinds of new anti-TB drugs in the near future. 1. Prospects for non-clinical or clinical development of new antituberculous drugs in relation to corporate strategy: Kenji NAMBA (New Product Research Laboratories I, Daiichi Pharmaceutical Co., Ltd.) Tuberculosis (TB) remains one of the deadliest threats to public health. No new anti-TB drugs have been brought into the clinic in the past 40 years. Current non-clinical works with progressed technology and Global Alliance for TB Drug Development, a non-profit organization established in 2000, accelerate research and development of faster-acting anti-TB compounds. We reviewed the status of new types of compounds which are being developed as anti-TB drug, such as diarylquinoline (TMC 207), nitroimidazole (PA-824 and OPC-67683), and moxifloxacin (MFLX). We also discussed the best clinical development plans for new-TB drugs in relation to corporate strategy. 2. Exploring novel drug targets through the chemical genomics approach and its possible application to the development of anti-tuberculosis drugs: Yorimasa SUWA (Reverse Proteomics Research Institute Co., Ltd.), Yohji SUZUKI (Teijin Ltd.) Recently, chemical genomics approach has been focused as an emerging technology for the drug discovery. In advance to a very large scale national project in US started last year, Reverse Proteomics Research Institute Co., Ltd. (REPRORI) has developed the core technologies for chemical genomics. Here we describe the outline of chemical genomics study, especially that of REPRORI, and discuss about its possible application to the development of anti-tuberculosis drugs. 3. Anti-mycobacterial agents and drug delivery: Koichi IZUMIKAWA, Hideaki OHNO, Shigeru KOHNO (Second Department of Internal Medicine, Nagasaki University School of Medicine) Mycobacterium infection is a major clinical concern in whole world. Since the newly developed anti-mycobacterial agents are few and still unavailable in clinical settings, the applications of drug delivery system using conventional anti-mycobacterial agents are challenging to improve the compliance of treatment and better efficacy. The efficacy of anti-mycobacterial agents modified by liposome or polymer based technology have been investigated and reported using various animal models. Drug delivery system increased and prolonged the drug concentrations at the blood and targeted organs and the duration of sustained drug release, respectively. These effects lead to decrease in the frequency of drug administrations dramatically and better efficacy rates. The studies, however, were performed only in animal models, the further investigations and evaluations in human are required for practical use. 4. Adjunctive immunotherapy of mycobacterial infections: Toshiaki SHIMIZU, Katsumasa SATO, Haruaki TOMIOKA (Department of Microbiology and Immunology, Shimane University School of Medicine) There is an urgent need to develop new antimicrobials and protocols for the administration of drugs that are potently efficacious against intractable mycobacterial infections. Unfortunately, development of the new drugs for solving this problem is not progressing. (ABSTRACT TRUNCATED)",
         "relevant":true
      },
      {
         "id":"10.1038/nrd.2017.221",
         "title":"Drug development for neurodevelopmental disorders: lessons learned from fragile X syndrome",
         "abstract":"Neurodevelopmental disorders such as fragile X syndrome (FXS) result in lifelong cognitive and behavioural deficits and represent a major public health burden. FXS is the most frequent monogenic form of intellectual disability and autism, and the underlying pathophysiology linked to its causal gene, FMR1, has been the focus of intense research. Key alterations in synaptic function thought to underlie this neurodevelopmental disorder have been characterized and rescued in animal models of FXS using genetic and pharmacological approaches. These robust preclinical findings have led to the implementation of the most comprehensive drug development programme undertaken thus far for a genetically defined neurodevelopmental disorder, including phase IIb trials of metabotropic glutamate receptor 5 (mGluR5) antagonists and a phase III trial of a GABAB receptor agonist. However, none of the trials has been able to unambiguously demonstrate efficacy, and they have also highlighted the extent of the knowledge gaps in drug development for FXS and other neurodevelopmental disorders. In this Review, we examine potential issues in the previous studies and future directions for preclinical and clinical trials. FXS is at the forefront of efforts to develop drugs for neurodevelopmental disorders, and lessons learned in the process will also be important for such disorders.",
         "relevant":true
      },
      {
         "id":"10.1089/cap.2016.0200",
         "title":"Review of Salient Investigational Drugs for the Treatment of Fragile X Syndrome",
         "abstract":"Objectives: Fragile X syndrome (FXS) is the most common inherited cause of intellectual disability, in addition to being the commonest diagnosable cause of autism. The identification of the biochemical mechanism underlying this disorder has provided amenable targets for therapy. This review aims to provide an overview of investigational drug therapies for FXS.\n\nMethods: The authors carried out a search of clinical and preclinical trials for FXS in PubMed and on the U.S. National Institutes of Health index of clinical trials ( www.clinicaltrials.gov ). We limited our review to Phase II trials or more preliminary and reviewed the associated publications for these studies, complemented by a review of the literature on PubMed.\n\nResults: The review of the preclinical, Phase I, and Phase II trials of agents with therapeutic potential in FXS revolves around an understanding of the putative pathways in the pathogenesis of FXS. While there is significant overlap between some of these pathways, the agents can be categorized as modulators of the metabotropic glutamate receptor system, GABAergic agents, and miscellaneous modulators affecting other pathways.\n\nConclusion: As trials involving agents targeting different aspects of the molecular biology proceed, common themes have emerged. With the great hope came great disappointment as the initial trials failed to demonstrate sufficient significance. In particular, the differences in outcome between the animal models and humans have highlighted the unique challenges of carrying out trials in these cognitively and behaviorally challenged individuals, as well as a dearth of clinically relevant outcome measures for use in medication trials. However, in reviewing and reframing the studies of the last decade, many important lessons have been learned, which will ultimately have a greater impact on therapeutic research in the field of developmental delay as a whole.",
         "relevant":true
      },
      {
         "id":"10.1146/annurev-pharmtox-010715-103429",
         "title":"Model-Informed Drug Development for Malaria Therapeutics",
         "abstract":"Malaria is a critical public health problem resulting in substantial morbidity and mortality, particularly in developing countries. Owing to the development of resistance toward current therapies, novel approaches to accelerate the development efforts of new malaria therapeutics are urgently needed. There have been significant advancements in the development of in vitro and in vivo experiments that generate data used to inform decisions about the potential merit of new compounds. A comprehensive disease-drug model capable of integrating discrete data from different preclinical and clinical components would be a valuable tool across all stages of drug development. This could have an enormous impact on the otherwise slow and resource-intensive process of traditional clinical drug development.",
         "relevant":true
      },
      {
         "id":"10.1016/s0891-5520(05)70400-1",
         "title":"Drug resistance among malaria and other parasites",
         "abstract":"Recent decades have witnessed the emergence and spread of parasites resistant to standard drug therapies, particularly malaria. Chloroquine-resistant Plasmodium falciparum has now spread to most malarial areas, and resistance to other antimalarial drugs, including mefloquine and sulfadoxine-pyrimethamine, have become significant problems in some parts of Southeast Asia and South America. Chloroquine-resistant P. vivax is well established in Papua New Guinea and Indonesia and has been reported in other areas. Trichomonas and Giardia infections resistant to metronidazole have also been documented. This article reviews the current status of drug resistance among parasites, particularly malaria, and offers strategies for managing patients with these infections.\n\nPIP: Reviewed in this article is the research literature on issues related to the development, spread, and impact of drug resistant malaria and current recommendations on chemoprophylaxis for the prevention of malaria. Also examined is metronidazole resistance in both Giardia and Trichomonas and the possibility of ivermectin resistance in human filariasis. Consistent themes in the literature on drug resistance include widespread and uncontrolled use of drugs, heavy reliance on a small number of drugs, use of single drug therapy, poor compliance with recommended treatment regimens, and slow development of new therapeutic alternatives. The way existing drugs are administered must be improved so their usefulness can be sustained. New drugs should not be introduced before they are needed to delay selection of resistant parasite strains. Mass drug administration should be used cautiously. Where possible, treatment should be based on a specific diagnosis. Efforts are needed to improve patient compliance with multiple-dose treatments. Although multidrug therapy has been proposed as an effective way to limit resistance, it poses problems in terms of increased cost and complexity. Finally, vaccines, appropriate and sustainable vector avoidance or elimination strategies, environmental control, and human behavior modification are important to reduce the need for drug therapy.",
         "relevant":true
      },
      {
         "id":"10.1056/NEJM199609123351107",
         "title":"The treatment of malaria",
         "abstract":"PIP: Increasing drug resistance in Plasmodium falciparum and a resurgence of malaria in tropical areas have effected a change in treatment of malaria in the last two decades. Symptoms of malaria are fever, chills, headache, and malaise. The prognosis worsens as the parasite counts, counts of mature parasites, and counts of neutrophils containing pigment increase. Treatment depends on severity, age of patient, degree of background immunity, likely pattern of susceptibility to antimalarial drugs, and the cost and availability of drugs. Chloroquine should be used for P. vivax, P. malariae, and P. ovale. P. vivax has shown high resistance to chloroquine in Oceania, however. Primaquine may be needed to treat P. vivax and P. ovale to rid the body of hypnozoites that survive in the liver. Chloroquine can treat P. falciparum infections acquired in North Africa, Central America north of the Panama Canal, Haiti, or the Middle East but not in most of Africa and some parts of Asia and South America. In areas of low grade resistance to chloroquine, amodiaquine can be used to effectively treat falciparum malaria. A combination of sulfadoxine-pyrimethamine is responsive to falciparum infections with high grade resistance to chloroquine. Mefloquine, halofantrine, or quinine with tetracycline can be used to treat multidrug-resistant P. falciparum. Derivatives of artemisinin obtained from qinghao or sweet wormwood developed as pharmaceuticals in China are the most rapidly acting of all antimalarial drugs. Children tend to tolerate antimalarial drugs well. Children who weigh less than 15 kg should not be given mefloquine. Health workers should not prescribe primaquine to pregnant women or newborns due to the risk of hemolysis. Chloroquine, sulfadoxine-pyrimethamine, quinine, and quinidine can be safely given in therapeutic doses throughout pregnancy. Clinical manifestations of severe malaria are hypoglycemia, convulsions, severe anemia, acute renal failure, jaundice, pulmonary edema, cerebral malaria, shock, and acidosis. Health workers should be prepared to treat these symptoms accordingly.",
         "relevant":true
      },
      {
         "id":"10.1016/j.drudis.2018.05.030",
         "title":"Drug metabolism and pharmacokinetic strategies for oligonucleotide- and mRNA-based drug development",
         "abstract":"Oligonucleotide and modified mRNA therapeutics have great potential to treat diseases that are currently challenging to cure and are expanding into global and chronic disease areas such as cancer and various cardiovascular diseases. Advanced drug delivery systems or ligand-drug conjugates are utilized to achieve 'the right dose to the right target' to benefit efficacy and safety in patients. Chemistry and ADME characteristics distinguish these therapeutics from small molecules. Understanding the scalability and translatability between species and compound properties is crucial for robust nonclinical PKPD predictions to support clinical study design. Although the field has been developing for three decades, there is still room for innovation but also a need for nonclinical regulatory guidance to address these new modalities.",
         "relevant":true
      },
      {
         "id":"10.1177/0022034520914246",
         "title":"Coronavirus Disease 2019 (COVID-19): Emerging and Future Challenges for Dental and Oral Medicine",
         "abstract":"The epidemic of coronavirus disease 2019 (COVID-19), originating in Wuhan, China, has become a major public health challenge for not only China but also countries around the world. The World Health Organization announced that the outbreaks of the novel coronavirus have constituted a public health emergency of international concern. As of February 26, 2020, COVID-19 has been recognized in 34 countries, with a total of 80,239 laboratory-confirmed cases and 2,700 deaths. Infection control measures are necessary to prevent the virus from further spreading and to help control the epidemic situation. Due to the characteristics of dental settings, the risk of cross infection can be high between patients and dental practitioners. For dental practices and hospitals in areas that are (potentially) affected with COVID-19, strict and effective infection control protocols are urgently needed. This article, based on our experience and relevant guidelines and research, introduces essential knowledge about COVID-19 and nosocomial infection in dental settings and provides recommended management protocols for dental practitioners and students in (potentially) affected areas.",
         "relevant":true
      },
      {
         "id":"10.1016/j.jmii.2020.03.034",
         "title":"Treatment options for COVID-19: The reality and challenges",
         "abstract":"An outbreak related to the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was first reported in Wuhan, China in December 2019. An extremely high potential for dissemination resulted in the global coronavirus disease 2019 (COVID-19) pandemic in 2020. Despite the worsening trends of COVID-19, no drugs are validated to have significant efficacy in clinical treatment of COVID-19 patients in large-scale studies. Remdesivir is considered the most promising antiviral agent; it works by inhibiting the activity of RNA-dependent RNA polymerase (RdRp). A large-scale study investigating the clinical efficacy of remdesivir (200 mg on day 1, followed by 100 mg once daily) is on-going. The other excellent anti-influenza RdRp inhibitor favipiravir is also being clinically evaluated for its efficacy in COVID-19 patients. The protease inhibitor lopinavir/ritonavir (LPV/RTV) alone is not shown to provide better antiviral efficacy than standard care. However, the regimen of LPV/RTV plus ribavirin was shown to be effective against SARS-CoV in vitro. Another promising alternative is hydroxychloroquine (200 mg thrice daily) plus azithromycin (500 mg on day 1, followed by 250 mg once daily on day 2-5), which showed excellent clinical efficacy on Chinese COVID-19 patients and anti-SARS-CoV-2 potency in vitro. The roles of teicoplanin (which inhibits the viral genome exposure in cytoplasm) and monoclonal and polyclonal antibodies in the treatment of SARS-CoV-2 are under investigation. Avoiding the prescription of non-steroidal anti-inflammatory drugs, angiotensin converting enzyme inhibitors, or angiotensin II type I receptor blockers is advised for COVID-19 patients.",
         "relevant":true
      },
      {
         "id":"10.1128/AAC.00399-20",
         "title":"Compounds with Therapeutic Potential against Novel Respiratory 2019 Coronavirus",
         "abstract":"Currently, the expansion of the novel human respiratory coronavirus (known as SARS-CoV-2 [severe acute respiratory syndrome coronavirus 2], COVID-2019 [coronavirus disease 2019], or 2019-nCoV [2019 novel coronavirus]) has stressed the need for therapeutic alternatives to alleviate and stop this new epidemic. The previous epidemics of infections by high-morbidity human coronaviruses, such as SARS-CoV in 2003 and the Middle East respiratory syndrome coronavirus (MERS-CoV) in 2012, prompted the characterization of compounds that could be potentially active against the currently emerging novel coronavirus, SARS-CoV-2. The most promising compound is remdesivir (GS-5734), a nucleotide analog prodrug currently in clinical trials for treating Ebola virus infections. Remdesivir inhibited the replication of SARS-CoV and MERS-CoV in tissue cultures, and it displayed efficacy in nonhuman animal models. In addition, a combination of the human immunodeficiency virus type 1 (HIV-1) protease inhibitors lopinavir/ritonavir and interferon beta (LPV/RTV-IFN-β) was shown to be effective in patients infected with SARS-CoV. LPV/RTV-IFN-β also improved clinical parameters in marmosets and mice infected with MERS-CoV. Remarkably, the therapeutic efficacy of remdesivir appeared to be superior to that of LPV/RTV-IFN-β against MERS-CoV in a transgenic humanized mouse model. The relatively high mortality rates associated with these three novel human coronavirus infections, SARS-CoV, MERS-CoV, and SARS-CoV-2, have suggested that proinflammatory responses might play a role in the pathogenesis. It remains unknown whether the generated inflammatory state should be targeted. Therapeutics that target the coronavirus alone might not be able to reverse highly pathogenic infections. This minireview aims to provide a summary of therapeutic compounds that have shown potential in fighting SARS-CoV-2 infections.",
         "relevant":true
      },
      {
         "id":"10.21873/invivo.11949",
         "title":"Potential Antiviral Drugs for SARS-Cov-2 Treatment: Preclinical Findings and Ongoing Clinical Research",
         "abstract":"Severe acute respiratory syndrome coronavirus 2 (SARS-Cov-2), initially termed 2019-new CoV (2019-nCoV), is a novel coronavirus responsible for the severe respiratory illness currently ongoing worldwide from the beginning of December 2019. This beta gene virus, very close to bat coronaviruses (bat-CoV-RaTG13) and bat-SL-CoVZC45, causes a severe disease, similar to those caused by Middle East respiratory syndrome (MERS)-CoV and SARS-CoV viruses, featured by low to moderate mortality rate. Unfortunately, the antiviral drugs commonly used in clinical practice to treat viral infections, are not applicable to SARS-Cov-2 and no vaccine is available. Thus, it is extremely necessary to identify new drugs suitable for the treatment of the 2019-nCoV outbreak. Different preclinical studies conducted on other coronaviruses suggested that promising clinical outcomes for 2019-nCoV should be obtained by using alpha-interferon, chloroquine phosphate, arabinol, remdesivir, lopinavir/ritonavir, and anti-inflammatory drugs. Moreover, clinical trials with these suitable drugs should be performed on patients affected by SARS-Cov-2 to prove their efficacy and safety. Finally, a very promising therapeutic drug, tocilizumab, is discussed; it is currently used to treat patients presenting COVID-19 pneumonia. Herein, we recapitulate these experimental studies to highlight the use of antiviral drugs for the treatment of SARS-Cov-2 disease.",
         "relevant":true
      },
      {
         "id":"10.1038/s41467-019-13940-6",
         "title":"Comparative therapeutic efficacy of remdesivir and combination lopinavir, ritonavir, and interferon beta against MERS-CoV",
         "abstract":"Middle East respiratory syndrome coronavirus (MERS-CoV) is the causative agent of a severe respiratory disease associated with more than 2468 human infections and over 851 deaths in 27 countries since 2012. There are no approved treatments for MERS-CoV infection although a combination of lopinavir, ritonavir and interferon beta (LPV/RTV-IFNb) is currently being evaluated in humans in the Kingdom of Saudi Arabia. Here, we show that remdesivir (RDV) and IFNb have superior antiviral activity to LPV and RTV in vitro. In mice, both prophylactic and therapeutic RDV improve pulmonary function and reduce lung viral loads and severe lung pathology. In contrast, prophylactic LPV/RTV-IFNb slightly reduces viral loads without impacting other disease parameters. Therapeutic LPV/RTV-IFNb improves pulmonary function but does not reduce virus replication or severe lung pathology. Thus, we provide in vivo evidence of the potential for RDV to treat MERS-CoV infections.",
         "relevant":true
      },
      {
         "id":"10.3389/fimmu.2018.01963",
         "title":"New Vaccine Technologies to Combat Outbreak Situations",
         "abstract":"Ever since the development of the first vaccine more than 200 years ago, vaccinations have greatly decreased the burden of infectious diseases worldwide, famously leading to the eradication of small pox and allowing the restriction of diseases such as polio, tetanus, diphtheria, and measles. A multitude of research efforts focuses on the improvement of established and the discovery of new vaccines such as the HPV (human papilloma virus) vaccine in 2006. However, radical changes in the density, age distribution and traveling habits of the population worldwide as well as the changing climate favor the emergence of old and new pathogens that bear the risk of becoming pandemic threats. In recent years, the rapid spread of severe infections such as HIV, SARS, Ebola, and Zika have highlighted the dire need for global preparedness for pandemics, which necessitates the extremely rapid development and comprehensive distribution of vaccines against potentially previously unknown pathogens. What is more, the emergence of antibiotic resistant bacteria calls for new approaches to prevent infections. Given these changes, established methods for the identification of new vaccine candidates are no longer sufficient to ensure global protection. Hence, new vaccine technologies able to achieve rapid development as well as large scale production are of pivotal importance. This review will discuss viral vector and nucleic acid-based vaccines (DNA and mRNA vaccines) as new approaches that might be able to tackle these challenges to global health.",
         "relevant":true
      },
      {
         "id":"10.1016/j.cvsm.2017.10.002",
         "title":"Recent Advances in Vaccine Technologies",
         "abstract":"This brief review discusses some recent advances in vaccine technologies with particular reference to their application within veterinary medicine. It highlights some of the key inactivated/killed approaches to vaccination, including natural split-product and subunit vaccines, recombinant subunit and protein vaccines, and peptide vaccines. It also covers live/attenuated vaccine strategies, including modified live marker/differentiating infected from vaccinated animals vaccines, live vector vaccines, and nucleic acid vaccines.",
         "relevant":true
      },
      {
         "id":"10.2174/1570162X17666190618160608",
         "title":"Updated Studies on the Development of HIV Therapeutic Vaccine",
         "abstract":"Background: Among the various types of pharmaceuticals, vaccines have a special place. However, in the case of HIV, nearly after 40 years of its discovery, an effective vaccine still is not available. The reason lies in several facts mainly the variability and smartness of HIV as well as the complexity of the interaction between HIV and immune responses. A robust, effective, and longterm immunity is undoubtedly what a successful preventive vaccine should induce in order to prevent the infection of HIV. Failure of human trials to this end has led to the idea of developing therapeutic vaccines with the purpose of curing already infected patients by boosting their immune responses against the virus. Nevertheless, the exceptional ability of the virus to escape the immune system based on the genetically diverse envelope and variable protein products have made it difficult to achieve an efficient therapeutic vaccine.\n\nObjective: We aimed at studying and comparing different approaches to HIV therapeutic vaccines.\n\nMethods: In this review, we summarized the human trials undergoing on HIV therapeutic vaccination which are registered in the U.S. clinical trial database (clinicaltrials.gov). These attempts are divided into different tables, according to the type of formulation and application in order to classify and compare their results.\n\nResult/conclusion: Among several methods applied in studied clinical trials which are mainly divided into DNA, Protein, Peptide, Viral vectors, and Dendritic cell-based vaccines, protein vaccine strategy is based on Tat protein-induced anti-Tat Abs in 79% HIV patients. However, the studies need to be continued to achieve a durable efficient immune response against HIV-1.",
         "relevant":true
      },
      {
         "id":"https://id.org/10.1103/PhysRevD.54.1",
         "title":"Review of Particle Physics",
         "abstract":"This biennial review summarizes much of Particle Physics. Using data from previous editions, plus 1900 new measurements from 700 papers, we list, evaluate, and average measured properties of gauge bosons, leptons, quarks, mesons, and baryons. We also summarize searches for hypothetical particles such as Higgs bosons, heavy neutrinos, and supersymmetric particles. All the particle properties and search limits are listed in Summary Tables. We also give numerous tables, figures, formulae, and reviews of topics such as the Standard Model, particle detectors, probability, and statistics. A booklet is available containing the Summary Tables and abbreviated versions of some of the other sections of this full Review.",
         "relevant":false
      },
      {
         "id":"10.1016/j.ccc.2013.08.004",
         "title":"Ultrasound physics",
         "abstract":"Bedside ultrasound has become an important modality for obtaining critical information in the acute care of patients. It is important to understand the physics of ultrasound in order to perform and interpret images at the bedside. The physics of both continuous wave and pulsed wave sound underlies diagnostic ultrasound. The instrumentation, including transducers and image processing, is important in the acquisition of appropriate sonographic images. Understanding how these concepts interplay with each other enables practitioners to obtain the best possible images.",
         "relevant":false
      },
      {
         "id":"10.1088/1361-6633/aa995b",
         "title":"Perspectives on theory at the interface of physics and biology",
         "abstract":"Theoretical physics is the search for simple and universal mathematical descriptions of the natural world. In contrast, much of modern biology is an exploration of the complexity and diversity of life. For many, this contrast is prima facie evidence that theory, in the sense that physicists use the word, is impossible in a biological context. For others, this contrast serves to highlight a grand challenge. I am an optimist, and believe (along with many colleagues) that the time is ripe for the emergence of a more unified theoretical physics of biological systems, building on successes in thinking about particular phenomena. In this essay I try to explain the reasons for my optimism, through a combination of historical and modern examples.",
         "relevant":false
      },
      {
         "id":"10.1016/j.plrev.2017.11.021",
         "title":"Physics of mind: Experimental confirmations of theoretical predictions",
         "abstract":"What is common among Newtonian mechanics, statistical physics, thermodynamics, quantum physics, the theory of relativity, astrophysics and the theory of superstrings? All these areas of physics have in common a methodology, which is discussed in the first few lines of the review. Is a physics of the mind possible? Is it possible to describe how a mind adapts in real time to changes in the physical world through a theory based on a few basic laws? From perception and elementary cognition to emotions and abstract ideas allowing high-level cognition and executive functioning, at nearly all levels of study, the mind shows variability and uncertainties. Is it possible to turn psychology and neuroscience into so-called \"hard\" sciences? This review discusses several established first principles for the description of mind and their mathematical formulations. A mathematical model of mind is derived from these principles. This model includes mechanisms of instincts, emotions, behavior, cognition, concepts, language, intuitions, and imagination. We clarify fundamental notions such as the opposition between the conscious and the unconscious, the knowledge instinct and aesthetic emotions, as well as humans' universal abilities for symbols and meaning. In particular, the review discusses in length evolutionary and cognitive functions of aesthetic emotions and musical emotions. Several theoretical predictions are derived from the model, some of which have been experimentally confirmed. These empirical results are summarized and we introduce new theoretical developments. Several unsolved theoretical problems are proposed, as well as new experimental challenges for future research.",
         "relevant":false
      },
      {
         "id":"10.1098/rsta.2015.0152",
         "title":"Pragmatic information in biology and physics",
         "abstract":"I will show how an objective definition of the concept of information and the consideration of recent results about information processing in the human brain help clarify some fundamental aspects of physics and biology. Rather than attempting to define information ab initio, I introduce the concept of interaction between material bodies as a primary concept. Two distinct categories can be identified: (i) interactions which can always be reduced to a superposition of physical interactions (forces) between elementary constituents; and (ii) interactions between complex bodies which cannot be expressed as a superposition of interactions between parts, and in which patterns and forms (in space and/or time) play the determining role. Pragmatic information is then defined as the link between a given pattern and the ensuing pattern-specific change. I will show that pragmatic information is a biological concept; it plays no active role in the purely physical domain-it only does so when a living organism intervenes. The consequences for physics (including foundations of quantum mechanics) and biology (including brain function) will be discussed. This will include speculations about three fundamental transitions, from the quantum to the classical domain, from natural inanimate to living systems, and from subhuman to human brain information-processing operations, introduced here in their direct connection with the concept of pragmatic information.",
         "relevant":false
      },
      {
         "id":"10.1016/j.ijpsycho.2015.02.016",
         "title":"Quantum neurophysics: From non-living matter to quantum neurobiology and psychopathology",
         "abstract":"The concepts of quantum brain, quantum mind and quantum consciousness have been increasingly gaining currency in recent years, both in scientific papers and in the popular press. In fact, the concept of the quantum brain is a general framework. Included in it are basically four main sub-headings. These are often incorrectly used interchangeably. The first of these and the one which started the quantum mind/consciousness debate was the place of consciousness in the problem of measurement in quantum mechanics. Debate on the problem of quantum measurement and about the place of the conscious observer has lasted almost a century. One solution to this problem is that the participation of a conscious observer in the experiment will radically change our understanding of the universe and our relationship with the outside world. The second topic is that of quantum biology. This topic has become a popular field of research, especially in the last decade. It concerns whether or not the rules of quantum physics operate in biological structures. It has been shown in the latest research on photosynthesis, the sense of smell and magnetic direction finding in animals that the laws of quantum physics may operate in warm-wet-noisy biological structures. The third sub-heading is quantum neurobiology. This topic has not yet gained wide acceptance and is still in its early stages. Its primary purpose is directed to understand whether the laws of quantum physics are effective in the biology of the nervous system or not. A further step in brain neurobiology, toward the understanding of consciousness formation, is the research of quantum laws effects upon neural network functions. The fourth and final topic is quantum psychopathology. This topic takes its basis and its support from quantum neurobiology. It comes from the idea that if quantum physics is involved in the normal working of the brain, diseased conditions of the brain such as depression, anxiety, dementia, schizophrenia and hallucinations can be explained by quantum physical pathology. In this article, these topics will be reviewed in a general framework, and for the first time a general classification will be made for the quantum brain theory.",
         "relevant":false
      },
      {
         "id":15648213,
         "title":"The definitions of information and meaning two possible boundaries between physics and biology",
         "abstract":"The standard approach to the definition of the physical quantities has not produced satisfactory results with the concepts of information and meaning. In the case of information we have at least two unrelated definitions, while in the case of meaning we have no definition at all. Here it is shown that both information and meaning can be defined by operative procedures, but it is also pointed out that we need to recognize them as a new type of natural entities. They are not quantities (neither fundamental nor derived) because they cannot be measured, and they are not qualities because are not subjective features. Here it is proposed to call them nominable entities, i.e., entities which can be specified only by naming their components in their natural order. If the genetic code is not a linguistic metaphor but a reality, we must conclude that information and meaning are real natural entities, and now we must also conclude that they are not equivalent to the quantities and qualities of our present theoretical framework. This gives us two options. One is to extend the definition of physics and say that the list of its fundamental entities must include information and meaning. The other is to say that physics is the science of quantities only, and in this case information and meaning become the exclusive province of biology. The boundary between physics and biology, in short, is a matter of convention, but the existence of information and meaning is not. We can decide to study them in the framework of an extended physics or in a purely biological framework, but we cannot avoid studying them for what they are, i.e., as fundamental components of the fabric of Nature.",
         "relevant":false
      },
      {
         "id":"10.1111/nyas.12860",
         "title":"The unification of physics: the quest for a theory of everything",
         "abstract":"The holy grail of physics has been to merge each of its fundamental branches into a unified \"theory of everything\" that would explain the functioning and existence of the universe. The last step toward this goal is to reconcile general relativity with the principles of quantum mechanics, a quest that has thus far eluded physicists. Will physics ever be able to develop an all-encompassing theory, or should we simply acknowledge that science will always have inherent limitations as to what can be known? Should new theories be validated solely on the basis of calculations that can never be empirically tested? Can we ever truly grasp the implications of modern physics when the basic laws of nature do not always operate according to our standard paradigms? These and other questions are discussed in this paper.",
         "relevant":false
      },
      {
         "id":"10.1111/nyas.12859",
         "title":"The origins of the universe: why is there something rather than nothing?",
         "abstract":"Perhaps the greatest mystery is why the universe exists in the first place. How is it possible for something to emerge from nothing, or has a universe in some form always existed? This question of origins-both of the universe as a whole and of the fundamental laws of physics-raises profound scientific, philosophical, and religious questions, culminating in the most basic existential question of all: Why are we here? Discussion of this and related questions is presented in this paper.",
         "relevant":false
      },
      {
         "id":"10.1111/nyas.12861",
         "title":"Transcending matter: physics and ultimate meaning",
         "abstract":"From the discovery of new galaxies and nearly undetectable dark energy to the quantum entanglement of particles across the universe, new findings in physics naturally elicit a sense of awe and wonder. For the founders of modern physics-from Einstein and Bohr to Heisenberg, Pauli, and Bohm-a fascination with deeper questions of meaning and ultimate reality led some of them to explore esoteric traditions and metaphysics. More recently, however, physicists have largely shunned such philosophical and spiritual associations. What can contemporary physics offer us in the quest to understand our place in the universe? Has physics in some ways become a religion unto itself that rejects the search for existential meaning? Discussion of these and related questions is presented in this paper.",
         "relevant":false
      },
      {
         "id":"10.1111/nyas.12877",
         "title":"Physics, philosophy, and the nature of reality",
         "abstract":"Both science and philosophy have been characterized as seeking to understand the nature of reality. They are sometimes even pitted against each other, suggesting that the success of science undermines the relevance of philosophy. But attending to the sort of understanding or explanation being sought offers a different picture: contemporary physics as practiced sometimes fails to provide a clear physical account of the world. This lies at the root of the dissatisfaction with standard quantum theory expressed by Einstein, Schrödinger, and John Bell. As an example, close consideration of Schrödinger's famous cat example suggests that physicists often have missed his point. What a philosophical disposition can contribute is not alternative physics, but rather the sort of careful attention to argument needed to extract a physical picture from a mathematical formalism.",
         "relevant":false
      },
      {
         "id":"10.1016/j.pbiomolbio.2015.06.008",
         "title":"Why natural science needs phenomenological philosophy",
         "abstract":"Through an exploration of theoretical physics, this paper suggests the need for regrounding natural science in phenomenological philosophy. To begin, the philosophical roots of the prevailing scientific paradigm are traced to the thinking of Plato, Descartes, and Newton. The crisis in modern science is then investigated, tracking developments in physics, science's premier discipline. Einsteinian special relativity is interpreted as a response to the threat of discontinuity implied by the Michelson-Morley experiment, a challenge to classical objectivism that Einstein sought to counteract. We see that Einstein's efforts to banish discontinuity ultimately fall into the \"black hole\" predicted in his general theory of relativity. The unavoidable discontinuity that haunts Einstein's theory is also central to quantum mechanics. Here too the attempt has been made to manage discontinuity, only to have this strategy thwarted in the end by the intractable problem of quantum gravity. The irrepressible discontinuity manifested in the phenomena of modern physics proves to be linked to a merging of subject and object that flies in the face of Cartesian philosophy. To accommodate these radically non-classical phenomena, a new philosophical foundation is called for: phenomenology. Phenomenological philosophy is elaborated through Merleau-Ponty's concept of depth and is then brought into focus for use in theoretical physics via qualitative work with topology and hypercomplex numbers. In the final part of this paper, a detailed summary is offered of the specific application of topological phenomenology to quantum gravity that was systematically articulated in The Self-Evolving Cosmos (Rosen, 2008a).",
         "relevant":false
      },
      {
         "id":"10.1111/nyas.14026",
         "title":"The mystery of our mathematical universe",
         "abstract":"Why is it that fundamental laws discovered through pure mathematics have been able to describe the behavior of our physical world with such precision? Given that the physical universe is composed of mathematical properties, some have posited that mathematics is the language of the universe, whose laws reveal what appears to be a hidden order in the natural world. Physicist S. James Gates, Jr. and science writer Margaret Wertheim explore the uncanny ability of mathematics to reveal the laws of nature.",
         "relevant":false
      },
      {
         "id":"10.1152/ajpregu.1980.238.5.R277",
         "title":"Physical causality and brain theories",
         "abstract":"The history of deterministic theories in physics is reviewed, and four levels of determinism are found: 1) absolute, 2) asymptotic, 3) probabilistic, and 4) absolute indeterminism. Nagel's view that all causal laws are deterministic in the frame of the state descriptions to which they refer is acknowledged, but the inevitability of macroscopic measurement noise may hint that dynamical laws are innately noisy. Quantum mechanical effects are not the noise source. Symmetry and broken symmetry are introduced as physical concepts that can account both for lawfulness, and for the hierarchical nature of the universe. Physical ideas are chosen over those of formal systems with indirect self-reference as the basis of a global theory of brains. By exclusion it is concluded that only a statistical thermodynamics, combined with nonlinear mechanics, has the features needed for theorizing about brains in a physical sense. Quantum mechanics is judged not to be relevant. New statistical thermodynamic theories are briefly described, and their strengths and weaknesses noted. The question, \"Why should neuroscience look to physics for its theories?\" is raised and answered. Some concrete objectives for a program of theoretical research are stated.",
         "relevant":false
      },
      {
         "id":"10.1098/rsta.2017.0313",
         "title":"Causality re-established",
         "abstract":"Causality has never gained the status of a 'law' or 'principle' in physics. Some recent literature has even popularized the false idea that causality is a notion that should be banned from theory. Such misconception relies on an alleged universality of the reversibility of the laws of physics, based either on the determinism of classical theory, or on the multiverse interpretation of quantum theory, in both cases motivated by mere interpretational requirements for realism of the theory. Here, I will show that a properly defined unambiguous notion of causality is a theorem of quantum theory, which is also a falsifiable proposition of the theory. Such a notion of causality appeared in the literature within the framework of operational probabilistic theories. It is a genuinely theoretical notion, corresponding to establishing a definite partial order among events, in the same way as we do by using the future causal cone on Minkowski space. The notion of causality is logically completely independent of the misidentified concept of 'determinism', and, being a consequence of quantum theory, is ubiquitous in physics. In addition, as classical theory can be regarded as a restriction of quantum theory, causality holds also in the classical case, although the determinism of the theory trivializes it. I then conclude by arguing that causality naturally establishes an arrow of time. This implies that the scenario of the 'block Universe' and the connected 'past hypothesis' are incompatible with causality, and thus with quantum theory: they are both doomed to remain mere interpretations and, as such, are not falsifiable, similar to the hypothesis of 'super-determinism'.This article is part of a discussion meeting issue 'Foundations of quantum mechanics and their impact on contemporary society'.",
         "relevant":false
      },
      {
         "id":"10.1016/j.tics.2015.09.003",
         "title":"The Social Regulation of Emotion: An Integrative, Cross-Disciplinary Model",
         "abstract":"Research in emotion regulation has largely focused on how people manage their own emotions, but there is a growing recognition that the ways in which we regulate the emotions of others also are important. Drawing on work from diverse disciplines, we propose an integrative model of the psychological and neural processes supporting the social regulation of emotion. This organizing framework, the 'social regulatory cycle', specifies at multiple levels of description the act of regulating another person's emotions as well as the experience of being a target of regulation. The cycle describes the processing stages that lead regulators to attempt to change the emotions of a target person, the impact of regulation on the processes that generate emotions in the target, and the underlying neural systems.",
         "relevant":false
      },
      {
         "id":"10.1177/1534582304267187",
         "title":"The functional architecture of human empathy",
         "abstract":"Empathy accounts for the naturally occurring subjective experience of similarity between the feelings expressed by self and others without loosing sight of whose feelings belong to whom. Empathy involves not only the affective experience of the other person's actual or inferred emotional state but also some minimal recognition and understanding of another's emotional state. In light of multiple levels of analysis ranging from developmental psychology, social psychology, cognitive neuroscience, and clinical neuropsychology, this article proposes a model of empathy that involves parallel and distributed processing in a number of dissociable computational mechanisms. Shared neural representations, self-awareness, mental flexibility, and emotion regulation constitute the basic macrocomponents of empathy, which are underpinned by specific neural systems. This functional model may be used to make specific predictions about the various empathy deficits that can be encountered in different forms of social and neurological disorders.",
         "relevant":false
      },
      {
         "id":"10.1016/j.drugpo.2018.06.001",
         "title":"Over and under-regulation in the Colorado Cannabis industry - A data-analytic perspective",
         "abstract":"With the State of California legalizing recreational cannabis sales on January 1, 2018, the regulatory process is once more in the forefront of cannabis research. Colorado, often held up as a model of legalization policy, was the first state to implement retail sale of recreational cannabis on January 1st, 2014. However, a combination of subsequent under-regulation and over-regulation, inconsistently applied across issues such as retail licencing, chemical testing, cannabis derivatives, municipality approval for growers, and financing, have not only held back the industry in Colorado but also negatively impacted public health, oversight, and have potentially increased the availability of illegal cannabis. We argue that a data-analytic approach to the industry is potentially the most effective way to resolve these concerns, since in the absence of consistent and reliable data, policymakers are apt to satisfy individual policy concerns without considering the industry as a whole. In this paper we present a data-analytic framework for the cannabis industry, offering a theoretically-driven justification for our approach, and describe implications for research on drug and information policy. The framework may serve as a model for other states or countries contemplating cannabis legalisation. As four new states legalised recreational cannabis in 2016, the implications of this research for policymakers has dramatically increased.",
         "relevant":false
      },
      {
         "id":"10.1016/S0140-6736(19)31789-1",
         "title":"Public health implications of legalising the production and sale of cannabis for medicinal and recreational use",
         "abstract":"We assess the current and describe possible future public health impacts of the legalisation of cannabis production, sale, and use in the Americas. First, we describe global patterns of cannabis use and their most probable adverse health effects. Second, we summarise evidence regarding the effectiveness of cannabinoids for medicinal use and describe approaches that have been used to regulate the use of medicinal cannabis and how these approaches might have affected medicinal and recreational use and harms (eg, road crashes). Third, we describe how jurisdictions that have legalised recreational use have regulated production and sale of cannabis. Fourth, we evaluate the effects of cannabis legalisation on cannabis use and harms and on the use of alcohol, tobacco, and other drugs. Fifth, we use alcohol and tobacco policy examples to identify possible long-term public health effects of cannabis legalisation. Finally, we outline policy approaches that could minimise harms to public health arising from the legalisation of a commercial cannabis industry.\n",
         "relevant":false
      },
      {
         "id":"10.1080/00952990.2019.1569669",
         "title":"Six policy lessons relevant to cannabis legalization",
         "abstract":"Background: Cannabis (marijuana) has been legalized for recreational and/or medicinal use in many US states, despite remaining a Schedule-I drug at the federal level. As legalization regimes are established in multiple countries, public health professionals should leverage decades of knowledge from other policy areas (e.g., alcohol and tobacco regulation) to inform cannabis policy.Objectives: Identify policy lessons from other more established policy areas that can inform cannabis policy in the United States, Canada, and any other nations that legalize recreational cannabis.Methods: Narrative review of policy and public health literature.Results: We identified six key lessons to guide cannabis policy. To avoid the harms of \"a medical system only in name,\" medical cannabis programs should either be regulated like medicine or combined with the recreational market. Capping potency of cannabis products can reduce the harms of the drug, including addiction. Pricing policies that promote public health may include minimum unit pricing or taxation by weight. Protecting science and public health from corporate interest can prevent the scenarios we have seen with soda and tobacco lobbies funding studies to report favorable results about their products. Legalizing states can go beyond reducing possession arrests (which can be accomplished without legalization) by expunging prior criminal records of cannabis-related convictions. Finally, facilitating rigorous research can differentiate truth from positive and negative hype about cannabis' effects.Conclusion: Scientists and policymakers can learn from the successes and failures of alcohol and tobacco policy to regulate cannabis products, thereby mitigating old harms of cannabis prohibition while reducing new harms from legalization.",
         "relevant":false
      },
      {
         "id":"10.1111/add.13428",
         "title":"Evaluating the public health impacts of legalizing recreational cannabis use in the United States",
         "abstract":"Background and aims: Since 2012 four US states have legalized the retail sale of cannabis for recreational use by adults, and more are likely to follow. This report aimed to (1) briefly describe the regulatory regimes so far implemented; (2) outline their plausible effects on cannabis use and cannabis-related harm; and (3) suggest what research is needed to evaluate the public health impact of these policy changes.\n\nMethod: We reviewed the drug policy literature to identify: (1) plausible effects of legalizing adult recreational use on cannabis price and availability; (2) factors that may increase or limit these effects; (3) pointers from studies of the effects of legalizing medical cannabis use; and (4) indicators of cannabis use and cannabis-related harm that can be monitored to assess the effects of these policy changes.\n\nResults: Legalization of recreational use will probably increase use in the long term, but the magnitude and timing of any increase is uncertain. It will be critical to monitor: cannabis use in household and high school surveys; cannabis sales; the number of cannabis plants legally produced; and the tetrahydrocannabinol (THC) content of cannabis. Indicators of cannabis-related harms that should be monitored include: car crash fatalities and injuries; emergency department presentations; presentations to addiction treatment services; and the prevalence of regular cannabis use among young people in mental health services and the criminal justice system.\n\nConclusions: Plausible effects of legalizing recreational cannabis use in the United States include substantially reducing the price of cannabis and increasing heavy use and some types of cannabis-related harm among existing users. In the longer term it may also increase the number of new users.",
         "relevant":false
      },
      {
         "id":"10.1002/wps.20735",
         "title":"Assessing the public health impacts of legalizing recreational cannabis use: the US experience",
         "abstract":"The sale of cannabis for adult recreational use has been made legal in nine US states since 2012, and nationally in Uruguay in 2013 and Canada in 2018. We review US research on the effects of legalization on cannabis use among adults and adolescents and on cannabis-related harms; the impact of legalizing adult recreational use on cannabis price, availability, potency and use; and regulatory policies that may increase or limit adverse effects of legalization. The legalization of recreational cannabis use in the US has substantially reduced the price of cannabis, increased its potency, and made cannabis more available to adult users. It appears to have increased the frequency of cannabis use among adults, but not so far among youth. It has also increased emergency department attendances and hospitalizations for some cannabis-related harms. The relatively modest effects on cannabis use to date probably reflect restrictions on the number and locations of retail cannabis outlets and the constraints on commercialization under a continued federal prohibition of cannabis. Future evaluations of legalization should monitor: cannabis sales volumes, prices and content of tetrahydrocannabinol; prevalence and frequency of cannabis use among adolescents and adults in household and high school surveys; car crash fatalities and injuries involving drivers who are cannabis-impaired; emergency department presentations related to cannabis; the demand for treatment of cannabis use disorders; and the prevalence of regular cannabis use among vulnerable young people in mental health services, schools and the criminal justice system. Governments that propose to legalize and regulate cannabis use need to fund research to monitor the impacts of these policy changes on public health, and take advantage of this research to develop ways of regulating can-nabis use that minimize adverse effects on public health.",
         "relevant":false
      },
      {
         "id":"10.1111/add.13845",
         "title":"The diverging trajectories of cannabis and tobacco policies in the United States: reasons and possible implications",
         "abstract":"Aim: To examine briefly the (i) rationales for two policy proposals in the United States to make it mandatory for cigarettes to contain very low levels of nicotine and to legalize cannabis for recreational use by adults; and (ii) possible lessons that participants in each policy debate may learn from each other.\n\nMethod: We briefly describe the diverging policies towards cannabis and tobacco in the United States, explain and critically analyse their rationales and discuss possible policy lessons.\n\nResults: Advocates of cannabis legalization have argued that prohibition has been an ineffective and expensive policy that penalizes ethnic minority users unjustly of a drug that is far less harmful than alcohol. The prohibition of traditional tobacco cigarettes has been advocated as a way to eliminate cigarette smoking. These proposals embody very different attitudes towards the harms of recreational adult drug use. Advocates of nicotine prohibition demand that alternative methods of nicotine delivery must be shown to be completely safe before adults are allowed to use them. Advocates of tobacco prohibition ignore evidence that smokers may not use these products and the likelihood of expanding the illicit tobacco market. Advocates of legalizing and regulating recreational cannabis ignore the need to tax and regulate sales in order to minimize the harms of heavy use.\n\nConclusions: It is not clear that the prohibition of adult use has a useful role to play in the regulation of either cannabis or tobacco. If both products remain legal, the goals of regulating tobacco and cannabis products should be to restrict youth access, promote the use of the least harmful products, provide users with evidence-based information on both absolute and differential product risks of use and use differential taxes and marketing controls to promote ways of using these products that cause the least harm to their users.",
         "relevant":false
      },
      {
         "id":"10.1016/j.ypmed.2017.07.008",
         "title":"Cannabis use, attitudes, and legal status in the U.S.: A review",
         "abstract":"Cannabis is widely used among adolescents and adults. In the U.S., marijuana laws have been changing, and Americans increasingly favor legalizing cannabis for medical and recreational uses. While some can use cannabis without harm, others experience adverse consequences. The objective of this review is to summarize information on the legal status of cannabis, perceptions regarding cannabis, prevalence and time trends in use and related adverse consequences, and evidence on the relationship of state medical (MML) and recreational (RML) marijuana laws to use and attitudes. Twenty-nine states now have MMLs, and eight of these have RMLs. Since the early 2000s, adult and adolescent perception of cannabis use as risky has decreased. Over the same time, the prevalence of adolescent cannabis use has changed little. However, adult cannabis use, disorders, and related consequences have increased. Multiple nationally representative studies indicate that MMLs have had little effect on cannabis use among adolescents. However, while MML effects have been less studied in adults, available evidence suggests that MMLs increase use and cannabis use disorders in adults. While data are not yet available to evaluate the effect of RMLs, they are likely to lower price, increase availability, and thereby increase cannabis use. More permissive marijuana laws may accomplish social justice aims (e.g., reduce racial disparities in law enforcement) and generate tax revenues. However, such laws may increase cannabis-related adverse health and psychosocial consequences by increasing the population of users. Dissemination of balanced information about the potential health harms of cannabis use is needed.",
         "relevant":false
      },
      {
         "id":"10.1016/j.compbiomed.2017.06.003",
         "title":"Tutoring math platform accessible for visually impaired people",
         "abstract":"Background: There are many problems with teaching and assessing impaired students in higher education, especially in technical science, where the knowledge is represented mostly by structural information like: math formulae, charts, graphs, etc. Developing e-learning platform for distance education solves this problem only partially due to the lack of accessibility for the blind.\n\nMethod: The proposed method is based on the decomposition of the typical mathematical exercise into a sequence of elementary sub-exercises. This allows for interactive resolving of math exercises and assessment of the correctness of exercise solutions at every stage. The presented methods were prepared and evaluated by visually impaired people and students.\n\nResults: The article presents the accessible interactive tutoring platform for math teaching and assessment, and experience in exploring it. The results of conducted research confirm good understanding of math formulae described according to elaborated rules. Regardless of the level of complexity of the math formulae the level of math formulae understanding is higher for alternative structural description.\n\nConclusions: The proposed solution enables alternative descriptions of math formulae. Based on the research results, the tool for computer-aided interactive learning of mathematics adapted to the needs of the blind has been designed, implemented and deployed as a platform for on-site and online and distance learning. The designed solution can be very helpful in overcoming many barriers that occur while teaching impaired students.",
         "relevant":false
      },
      {
         "id":"10.1177/002221949703000101",
         "title":"Mathematics education and students with learning disabilities: introduction to the special series",
         "abstract":"The prevalence of students with mathematics learning disabilities has triggered an interest among special education researchers and practitioners in developing an understanding of the needs of this group of students, and in identifying effective instructional programming to foster their mathematical performance during the school years and into adulthood. Research into the characteristics of students with mathematics learning disabilities is being approached from different perspectives, including developmental, neurological and neuropsychological, and educational. This diversity helps us develop a broader understanding of students' learning needs and difficulties. Special education assessment practices encompass a variety of approaches, including norm-referenced, criterion-referenced, and nonstandardized procedures, depending on the specific assessment questions professionals seek to answer. Students' mathematical knowledge and conceptual understanding must be examined to determine their strengths and weaknesses, curriculum-based progress, and use of cognitive strategies to arrive at mathematical solutions. Research findings have identified empirically validated interventions for teaching mathematics curricula to students with mathematics learning disabilities. Research studies have been grounded in behavioral theory and cognitive psychology, with an emergent interest in the constructivist approach. Although research studies have focused primarily on computational performance, more work is being conducted in the areas of story-problem solving and technology. These areas as well as other math curricular skills require further study. Additionally, the needs of adults with math LD have spurred educators to examine the elementary and secondary math curricula and determine ways to infuse them with life skills instruction accordingly. As the field of mathematics special education continues to evolve, special educators must remain cognizant of the developments in and influences on the field of mathematics education. Reform efforts have shaped the field significantly since the 1950s, contributing to the curriculum offered in mathematics textbooks and the pedagogical practices taught in higher education courses. Mathematics educators continue to search for a better understanding of how children learn mathematics; this process is shaped by the prevailing theoretical orientations and research methodologies. This special series in mathematics special education provides readers with information about the characteristics of students with mathematics learning disabilities, assessment procedures, mathematics programming, teacher preparation, and future directions for the field. The series originated as a result of discussions with Dr. Lee Wiederholt and Dr. Judith K. Voress, who saw a need for the compilation of recent research and best practices in mathematics special education. I thank them for their support of and thoughtful insights about the development of this series. I also appreciate the support of Dr. George Hynd and his editorial assistant, Kathryn Black, in finalizing the details for publication. Finally, I am most appreciative of the authors' contributions to this series; their work continues to significantly influence the development of the field of mathematics special education and programming for students with mathematics learning disabilities.",
         "relevant":false
      },
      {
         "id":"10.1177/002221949602900407",
         "title":"Using hypermedia to improve the mathematics problem-solving skills of students with learning disabilities",
         "abstract":"This article discusses current knowledge about teaching problem solving to students with learning disabilities (LD), using computers for teaching math to students with LD, and using computers for teaching problem solving to students with learning problems. Building upon identified effective learning strategies, direct instruction procedures, and principles of effective instructional design, the case is presented for the use of hypermedia in helping students with learning disabilities to improve their mathematics problem-solving abilities. Specific ideas and suggestions for applying hypermedia to cognitive strategy instruction and the graduated word problem sequence are given. Several cautions regarding hypermedia use are presented.",
         "relevant":false
      },
      {
         "id":"10.1080/10400435.2020.1734112",
         "title":"An interactive math braille learning application to assist blind students in Bangladesh",
         "abstract":"Due to the lack of affordable assistive tools for learning mathematics, blind students in Bangladesh still use outdated learning tools like Tailor Frame. Therefore, demand for a low-cost technological tool is there that will help the blind students to learn math braille and calculate numbers more easily. To provide an effective and affordable assistive tool, this study begins with a needs assessment study to identify the basic needs of blind students in learning math braille and solving mathematical calculations. Afterward, a mobile phone based interactive assistive application is proposed to improve the learning facilities of math braille using Nemeth code for blind students in Bangladesh. Interfaces of the application are designed based on their needs. Besides, interaction methods (such as, hearing & touching) of blind students with the physical world were another criterion in designing very interactive interfaces that provide self-learning facilities. The interfaces are evaluated by the teachers, experts, and end users in order to identify the usability. The evaluation shows a promising result toward the acceptability of the designed application. Therefore, this application can be helpful for the blind students to learn math braille using Nemeth Code.",
         "relevant":false
      },
      {
         "id":"10.1044/2017_LSHSS-17-0050",
         "title":"Curriculum-Based Language Assessment With Culturally and Linguistically Diverse Students in the Context of Mathematics",
         "abstract":"Purpose: The purpose of this tutorial is to discuss the use of curriculum-based language assessment (CBLA) with students who are English language learners and students who speak nonmainstream varieties of English, such as African American English.\n\nMethod: The article begins with a discussion of the discourse of mathematics and the role of the speech-language pathologist (SLP), followed by a review of studies that includes those that examined the performance of English language learner and nonmainstream dialect-speaking students on word-based math items.\n\nResults: The literature review highlights the linguistic and content biases associated with word-based math problems. Useful strategies that SLPs and educators can incorporate in culturally and linguistically appropriate assessments are discussed. The tutorial ends with a discussion of CBLA as a viable assessment approach to use with culturally and linguistically diverse students.\n\nConclusions: Tests used at national, state, and school levels to assess students' math abilities have associated linguistic bias and content bias often leading to an inaccurate depiction of culturally and linguistically diverse students' math skills. CBLA as an assessment method can be used by school-based SLPs to gather valid and useful information about culturally and linguistically diverse students' language for learning math. By using CBLA, SLPs can help modify curricular tasks in broader contexts in an effort to make math, including high-level math, \"accessible and achievable for all\" students (American Speech-Language-Hearing Association, 2017).",
         "relevant":false
      },
      {
         "id":"10.1080/17483107.2020.1800116",
         "title":"Computer aided math learning as a tool to assess and increase motivation in learning math by visually impaired students",
         "abstract":"Purpose: Effective teaching and learning mathematics is important to achieve good results during an academic and professional career. This is especially difficult for visually impaired students because of difficulties in managing structural information included in maths formulae.\n\nMethods: The extended multimedia alternative method, including the problem of decomposition and knowledge vector, were presented and compared to the classical teaching method. A qualitative method to evaluate motivation during the process of teaching and learning maths for impaired students, which includes eleven detailed motivators, has been developed.\n\nResults: The alternative teaching method offers statistically significant improvements in four of the eleven proposed assessment categories: success in progress - adjusting the difficulties of learning, presentation of the material, approval: group/individual and alternative presentation of mathematic materials. The experiments carried out allowed the authors to increase the knowledge about the limitations and challenges occurring in the process of maths education among visually impaired students and their motivation.\n\nConclusions: Based on the experiments and research results, it can be concluded that applying the proposed method of decomposition together with evaluation procedure based on the vector of knowledge in the process of teaching and learning of mathematics by visually impaired may increase their motivation positively. Implications for Rehabilitation The key aspects of the alternative method of presenting math formulae are included in the bellow points: • It increases the availability of materials containing structured information (mathematical formulae) for the blind. • It limits the importance of communication barriers in math education: the possibility of self-study, reducing costs - by reducing the number of individual tutorials. • It contributes to the development of universal rules for the structural presentation of information • It increases the independence of the student from the teacher.\n\nKeywords: Alternative math presentation; interactive multimedia math presentation; math tutoring platform; mathematics; student motivation.",
         "relevant":false
      },
      {
         "id":"10.2466/11.PR0.109.6.775-784",
         "title":"The use of feedback in education: a complex instructional strategy",
         "abstract":"The use of feedback to improve students' academic performance involves complex instructional strategies. There are a number of instructional problems within these strategies that may be overlooked. These include the influence of emotional responses, interactions between teacher and student, and levels of cognitive processing involved. Feedback should be viewed as an interactive, empirically driven, problem-solving process.",
         "relevant":false
      },
      {
         "id":"10.3402/meo.v16i0.6035",
         "title":"Interprofessional collaboration: three best practice models of interprofessional education",
         "abstract":"Interprofessional education is a collaborative approach to develop healthcare students as future interprofessional team members and a recommendation suggested by the Institute of Medicine. Complex medical issues can be best addressed by interprofessional teams. Training future healthcare providers to work in such teams will help facilitate this model resulting in improved healthcare outcomes for patients. In this paper, three universities, the Rosalind Franklin University of Medicine and Science, the University of Florida and the University of Washington describe their training curricula models of collaborative and interprofessional education.The models represent a didactic program, a community-based experience and an interprofessional-simulation experience. The didactic program emphasizes interprofessional team building skills, knowledge of professions, patient centered care, service learning, the impact of culture on healthcare delivery and an interprofessional clinical component. The community-based experience demonstrates how interprofessional collaborations provide service to patients and how the environment and availability of resources impact one's health status. The interprofessional-simulation experience describes clinical team skills training in both formative and summative simulations used to develop skills in communication and leadership.One common theme leading to a successful experience among these three interprofessional models included helping students to understand their own professional identity while gaining an understanding of other professional's roles on the health care team. Commitment from departments and colleges, diverse calendar agreements, curricular mapping, mentor and faculty training, a sense of community, adequate physical space, technology, and community relationships were all identified as critical resources for a successful program. Summary recommendations for best practices included the need for administrative support, interprofessional programmatic infrastructure, committed faculty, and the recognition of student participation as key components to success for anyone developing an IPE centered program.",
         "relevant":false
      },
      {
         "id":"10.3109/13561820.2014.937483",
         "title":"Interprofessional Education and Practice Guide No. 1: developing faculty to effectively facilitate interprofessional education",
         "abstract":"With the growth of interprofessional education (IPE) and practice in health professional schools, faculty members are being asked to assume new roles in leading or delivering interprofessional curriculum. Many existing faculty members feel ill-prepared to face the challenges of this curricular innovation. From 2012-2013, University of Missouri - Columbia and University of Washington partnered with six additional academic health centers to pilot a faculty development course to prepare faculty leaders for IPE. Using a variety of techniques, including didactic teaching, small group exercises, immersion participation in interprofessional education, local implementation of new IPE projects, and peer learning, the program positioned each site to successfully introduce an interprofessional innovation. Participating faculty confirmed the value of the program, and suggested that more widespread similar efforts were worthwhile. This guide briefly describes this faculty development program and identifies key lessons learned from the initiative. Peer learning arising from a faculty development community, adaptation of curricula to fit local context, experiential learning, and ongoing coaching/mentoring, especially as it related to actual participation in IPE activities, were among the key elements of this successful faculty development activity.",
         "relevant":false
      },
      {
         "id":"10.1111/j.1442-2018.2009.00456.x",
         "title":"Perceptions of distance education among nursing faculty members in North America",
         "abstract":"A strategy to increase access to nursing education, train nurses for practice, and prepare future nurse educators is distance education. Faculty member shortages are cited as the main reason for not accepting qualified applicants. Faculty members are the core of nursing education. In order to address nursing faculty members' concerns regarding distance education and to assist in faculty member recruitment, retention, growth, and development in order to improve and enhance the quality of distance education, one must answer the question: What are nursing faculty members' perceptions of distance education in nursing? Utilizing a number of databases to locate research specific to this topic, this article provides an integrative review of the nursing literature to ascertain the faculty members' perspective of distance education. The research was analyzed, findings summarized, and limitations mentioned. Utilizing a brief supplementary review of the literature, the implications, recommendations, and need for future research are discussed.",
         "relevant":false
      },
      {
         "id":"10.1097/01.AACN.0000310752.31406.d5",
         "title":"An online dual-certification program in advanced practice nursing and nursing education: one university's response to faculty shortages",
         "abstract":"Educating nurses for the healthcare delivery workforce is stymied as qualified applicants to nursing programs are being turned away. Although applications to baccalaureate programs have increased, between 41,683 and 147,000 undergraduate and graduate applicants were turned away from nursing education programs in 2005 due largely to shortages of nursing faculty. In this article, the evidence-based rationale for the development of a dual-certification program for the preparation of clinical nurse specialists and nurse educators is described. Because faculty shortages are nationwide, we developed the program to be delivered, in its entirety, online. Standardized data collection methods for evaluating student progress and their achievement of competencies expected of clinical nurse specialists and nurse educators are provided. The program may be a model for preparing clinically competent nurse educators who prefer practice settings to full-time faculty positions.",
         "relevant":false
      },
      {
         "id":"10.1111/j.1744-6163.2006.00089.x",
         "title":"An introductory clinical core course in psychiatric management: an innovative lifespan course blending all nurse practitioner majors",
         "abstract":"Topic: The prevalence of anxiety, depression, substance abuse, and suicidal ideation is significant in primary care settings across the country. Nonpsychiatric nurse practitioners must be able to recognize symptoms of common psychiatric disorders, know how to treat less complex mental illnesses, and know when to refer to psychiatric mental health nurse practitioners (PMHNPs).\n\nPurpose: This article describes the course content, assignments, and teaching strategies used in a clinical core course in the nurse practitioner (NP) curriculum that is required for all NP majors at the University of Texas at Arlington. Psychiatric Management for Advanced Practice provides the foundation for later PMHNP major specific clinical courses.\n\nSources: Development of the course content was based on NONPF Domains and Competencies for the NP, input from graduate NP faculty using a modified Delphi approach, NP student feedback, review of curriculum from other schools, and review of the literature on depression, suicide, anxiety, and substance abuse disorders in primary care settings.\n\nConclusions: Since 1999, students from the eight different NP programs at the University of Texas at Arlington have been required to take this course. Student, faculty, and graduate feedback about this course have been consistently positive. Many NP students comment on feeling much more comfortable assessing for depression, suicide, and substance use.",
         "relevant":false
      },
      {
         "id":"10.5688/ajpe78483",
         "title":"Effectiveness of E-learning in pharmacy education",
         "abstract":"Over the past 2 decades, e-learning has evolved as a new pedagogy within pharmacy education. As learners and teachers increasingly seek e-learning opportunities for an array of educational and individual benefits, it is important to evaluate the effectiveness of these programs. This systematic review of the literature examines the quality of e-learning effectiveness studies in pharmacy, describes effectiveness measures, and synthesizes the evidence for each measure. E-learning in pharmacy education effectively increases knowledge and is a highly acceptable instructional format for pharmacists and pharmacy students. However, there is limited evidence that e-learning effectively improves skills or professional practice. There is also no evidence that e-learning is effective at increasing knowledge long term; thus, long-term follow-up studies are required. Translational research is also needed to evaluate the benefits of e-learning at patient and organizational levels.",
         "relevant":false
      },
      {
         "id":"10.1016/j.cptl.2016.08.046",
         "title":"Game on: The gamification of the pharmacy classroom",
         "abstract":"Background: Gamification is the use of game mechanics to promote engagement and enjoyment of problem-solving in non-game situations. Gamification has been used widely in recent years in industry and academia as a tool for training and education.\n\nObjective: The aims of this paper are to provide an overview of gamification and digital game-based learning (DGBL), review the use of digital games in health professional education, and provide suggestions for future use in pharmacy curricula.\n\nDiscussion: Many examples of game-based learning in pharmacy and other health professional curricula have been published, however the body of literature on DGBL is less developed. Overall, evaluations of these techniques show that students find them engaging and enjoyable. A recent meta-analysis of studies comparing DGBL to non-game based learning in primary, secondary, post-secondary education found that DGBL significantly enhances learning. Challenges to implementing game-based learning are financial, cultural, and technological.\n\nConclusion: Many areas of the pharmacy curriculum could be appropriate for digital gamification. With more students entering pharmacy school familiar with video games and game-based living the time has come for pharmacy educators to explore how these instructional technologies could benefit a new generation of pharmacy students. As serious games are developed and researched in pharmacy curricula, test scores, student confidence in knowledge and skills, and retention of knowledge and skills are all outcomes that, if published, will help advance the adoption of DGBL into the pharmacy school classroom.\n\nKeywords: Active learning; Game-based learning; Gamification; Pharmacy education.",
         "relevant":false
      },
      {
         "id":"10.1111/bjep.12151",
         "title":"Math anxiety and its relationship with basic arithmetic skills among primary school children",
         "abstract":"Background: Children have been found to report and demonstrate math anxiety as early as the first grade. However, previous results concerning the relationship between math anxiety and performance are contradictory, with some studies establishing a correlation between them while others do not. These contradictory results might be related to varying operationalizations of math anxiety.\n\nAims: In this study, we aimed to examine the prevalence of math anxiety and its relationship with basic arithmetic skills in primary school children, with explicit focus on two aspects of math anxiety: anxiety about failure in mathematics and anxiety in math-related situations.\n\nSample: The participants comprised 1,327 children at grades 2-5.\n\nMethods: Math anxiety was assessed using six items, and basic arithmetic skills were assessed using three assessment tasks.\n\nResults: Around one-third of the participants reported anxiety about being unable to do math, one-fifth about having to answer teachers' questions, and one tenth about having to do math. Confirmatory factor analysis indicated that anxiety about math-related situations and anxiety about failure in mathematics are separable aspects of math anxiety. Structural equation modelling suggested that anxiety about math-related situations was more strongly associated with arithmetic fluency than anxiety about failure. Anxiety about math-related situations was most common among second graders and least common among fifth graders.\n\nConclusions: As math anxiety, particularly about math-related situations, was related to arithmetic fluency even as early as the second grade, children's negative feelings and math anxiety should be identified and addressed from the early primary school years.",
         "relevant":false
      },
      {
         "id":"10.1177/0956797615592630",
         "title":"Intergenerational Effects of Parents' Math Anxiety on Children's Math Achievement and Anxiety",
         "abstract":"A large field study of children in first and second grade explored how parents' anxiety about math relates to their children's math achievement. The goal of the study was to better understand why some students perform worse in math than others. We tested whether parents' math anxiety predicts their children's math achievement across the school year. We found that when parents are more math anxious, their children learn significantly less math over the school year and have more math anxiety by the school year's end-but only if math-anxious parents report providing frequent help with math homework. Notably, when parents reported helping with math homework less often, children's math achievement and attitudes were not related to parents' math anxiety. Parents' math anxiety did not predict children's reading achievement, which suggests that the effects of parents' math anxiety are specific to children's math achievement. These findings provide evidence of a mechanism for intergenerational transmission of low math achievement and high math anxiety.",
         "relevant":false
      },
      {
         "id":"10.1037/spq0000276",
         "title":"Math and science motivation in internationally adopted adolescents",
         "abstract":"Despite prior studies documenting learning difficulties among internationally adopted youth (IAY), none has explored academic motivation within this population. The current study addressed this gap by examining expectancies for success and task values in math and science among internationally adopted, domestically adopted, and nonadopted high-school students. Differences in students' math achievement and parents' beliefs about their ability were also explored. A subsample of 7,420 11th-grade students was selected from the High School Longitudinal Study data set (Ingels et al., 2011). After controlling for prior motivation, achievement, demographics, and clustering, analyses revealed that IAY held less adaptive motivational beliefs in math. Compared with their nonadopted peers, internationally adopted students reported lower expectancies for success, attainment value, and intrinsic value in math. Few differences in science motivation emerged, and no differences in math achievement were observed. (PsycINFO Database Record",
         "relevant":false
      },
      {
         "id":"10.1007/s10964-016-0439-9",
         "title":"Promotive and Corrosive Factors in African American Students' Math Beliefs and Achievement",
         "abstract":"Framed by expectancy-value theory (which posits that beliefs about and the subjective valuation of a domain predict achievement and decision-making in that domain), this study examined the relationships among teacher differential treatment and relevant math instruction on African American students' self-concept of math ability, math task value, and math achievement. These questions were examined by applying structural equation modeling to 618 African American youth (45.6 % female) followed from 7th to 11th grade in the Maryland Adolescent Development in Context Study. While controlling for gender and prior math achievement, relevant math instruction promoted and teacher differential treatment corroded students' math beliefs and achievement over time. Further, teacher discrimination undermined students' perceptions of their teachers, a mediating process under-examined in previous inquiry. These findings suggest policy and practice levers to narrow opportunity gaps, as well as foster math achievement and science, technology, engineering and math success.",
         "relevant":false
      },
      {
         "id":"10.1126/science.aac7427",
         "title":"Math at home adds up to achievement in school",
         "abstract":"With a randomized field experiment of 587 first-graders, we tested an educational intervention designed to promote interactions between children and parents relating to math. We predicted that increasing math activities at home would increase children's math achievement at school. We tested this prediction by having children engage in math story time with their parents. The intervention, short numerical story problems delivered through an iPad app, significantly increased children's math achievement across the school year compared to a reading (control) group, especially for children whose parents are habitually anxious about math. Brief, high-quality parent-child interactions about math at home help break the intergenerational cycle of low math achievement.",
         "relevant":false
      },
      {
         "id":"10.1037/spq0000198",
         "title":"Multidimensional assessment of self-regulated learning with middle school math students",
         "abstract":"This study examined the convergent and predictive validity of self-regulated learning (SRL) measures situated in mathematics. The sample included 100 eighth graders from a diverse, urban school district. Four measurement formats were examined including, 2 broad-based (i.e., self-report questionnaire and teacher ratings) and 2 task-specific measures (i.e., SRL microanalysis and behavioral traces). Convergent validity was examined across task-difficulty, and the predictive validity was examined across 3 mathematics outcomes: 2 measures of mathematical problem solving skill (i.e., practice session math problems, posttest math problems) and a global measure of mathematical skill (i.e., standardized math test). Correlation analyses were used to examine convergent validity and revealed medium correlations between measures within the same category (i.e., broad-based or task-specific). Relations between measurement classes were not statistically significant. Separate regressions examined the predictive validity of the SRL measures. While controlling all other predictors, a SRL microanalysis metacognitive-monitoring measure emerged as a significant predictor of all 3 outcomes and teacher ratings accounted for unique variance on 2 of the outcomes (i.e., posttest math problems and standardized math test). Results suggest that a multidimensional assessment approach should be considered by school psychologists interested in measuring SRL. (PsycINFO Database Record",
         "relevant":false
      },
      {
         "id":"10.1111/cdev.12662",
         "title":"Early Math Trajectories: Low-Income Children's Mathematics Knowledge From Ages 4 to 11",
         "abstract":"Early mathematics knowledge is a strong predictor of later academic achievement, but children from low-income families enter school with weak mathematics knowledge. An early math trajectories model is proposed and evaluated within a longitudinal study of 517 low-income American children from ages 4 to 11. This model includes a broad range of math topics, as well as potential pathways from preschool to middle grades mathematics achievement. In preschool, nonsymbolic quantity, counting, and patterning knowledge predicted fifth-grade mathematics achievement. By the end of first grade, symbolic mapping, calculation, and patterning knowledge were the important predictors. Furthermore, the first-grade predictors mediated the relation between preschool math knowledge and fifth-grade mathematics achievement. Findings support the early math trajectories model among low-income children.",
         "relevant":false
      },
      {
         "id":"10.1038/s41586-019-0912-1",
         "title":"Deep learning and process understanding for data-driven Earth system science",
         "abstract":"Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning.",
         "relevant":false
      },
      {
         "id":"10.1007/s11914-018-0445-9",
         "title":"The New Possibilities from \"Big Data\" to Overlooked Associations Between Diabetes, Biochemical Parameters, Glucose Control, and Osteoporosis",
         "abstract":"Purpose of review: To review current practices and technologies within the scope of \"Big Data\" that can further our understanding of diabetes mellitus and osteoporosis from large volumes of data. \"Big Data\" techniques involving supervised machine learning, unsupervised machine learning, and deep learning image analysis are presented with examples of current literature.\n\nRecent findings: Supervised machine learning can allow us to better predict diabetes-induced osteoporosis and understand relative predictor importance of diabetes-affected bone tissue. Unsupervised machine learning can allow us to understand patterns in data between diabetic pathophysiology and altered bone metabolism. Image analysis using deep learning can allow us to be less dependent on surrogate predictors and use large volumes of images to classify diabetes-induced osteoporosis and predict future outcomes directly from images. \"Big Data\" techniques herald new possibilities to understand diabetes-induced osteoporosis and ascertain our current ability to classify, understand, and predict this condition.",
         "relevant":false
      },
      {
         "id":"10.1038/s41576-019-0122-6",
         "title":"Deep learning: new computational modelling techniques for genomics",
         "abstract":"As a data-driven science, genomics largely utilizes machine learning to capture dependencies in data and derive novel biological hypotheses. However, the ability to extract new insights from the exponentially increasing volume of genomics data requires more expressive machine learning models. By effectively leveraging large data sets, deep learning has transformed fields such as computer vision and natural language processing. Now, it is becoming the method of choice for many genomics modelling tasks, including predicting the impact of genetic variation on gene regulatory mechanisms such as DNA accessibility and splicing.",
         "relevant":false
      },
      {
         "id":"10.1007/978-1-4939-7899-1_5",
         "title":"Machine Learning Methods in Computational Toxicology",
         "abstract":"Various methods of machine learning, supervised and unsupervised, linear and nonlinear, classification and regression, in combination with various types of molecular descriptors, both \"handcrafted\" and \"data-driven,\" are considered in the context of their use in computational toxicology. The use of multiple linear regression, variants of naïve Bayes classifier, k-nearest neighbors, support vector machine, decision trees, ensemble learning, random forest, several types of neural networks, and deep learning is the focus of attention of this review. The role of fragment descriptors, graph mining, and graph kernels is highlighted. The application of unsupervised methods, such as Kohonen's self-organizing maps and related approaches, which allow for combining predictions with data analysis and visualization, is also considered. The necessity of applying a wide range of machine learning methods in computational toxicology is underlined.",
         "relevant":false
      },
      {
         "id":"10.1016/j.neunet.2014.09.007",
         "title":"Deep learning of support vector machines with class probability output networks",
         "abstract":"Deep learning methods endeavor to learn features automatically at multiple levels and allow systems to learn complex functions mapping from the input space to the output space for the given data. The ability to learn powerful features automatically is increasingly important as the volume of data and range of applications of machine learning methods continues to grow. This paper proposes a new deep architecture that uses support vector machines (SVMs) with class probability output networks (CPONs) to provide better generalization power for pattern classification problems. As a result, deep features are extracted without additional feature engineering steps, using multiple layers of the SVM classifiers with CPONs. The proposed structure closely approaches the ideal Bayes classifier as the number of layers increases. Using a simulation of classification problems, the effectiveness of the proposed method is demonstrated.",
         "relevant":false
      },
      {
         "id":"10.1080/10962247.2016.1248303",
         "title":"Increasing the Use of Earth Science Data and Models in Air Quality Management",
         "abstract":"In 2010, the U.S. National Aeronautics and Space Administration (NASA) initiated the Air Quality Applied Science Team (AQAST) as a 5-year, $17.5-million award with 19 principal investigators. AQAST aims to increase the use of Earth science products in air quality-related research and to help meet air quality managers' information needs. We conducted a Web-based survey and a limited number of follow-up interviews to investigate federal, state, tribal, and local air quality managers' perspectives on usefulness of Earth science data and models, and on the impact AQAST has had. The air quality managers we surveyed identified meeting the National Ambient Air Quality Standards for ozone and particulate matter, emissions from mobile sources, and interstate air pollution transport as top challenges in need of improved information. Most survey respondents viewed inadequate coverage or frequency of satellite observations, data uncertainty, and lack of staff time or resources as barriers to increased use of satellite data by their organizations. Managers who have been involved with AQAST indicated that the program has helped build awareness of NASA Earth science products, and assisted their organizations with retrieval and interpretation of satellite data and with application of global chemistry and climate models. AQAST has also helped build a network between researchers and air quality managers with potential for further collaborations.",
         "relevant":false
      },
      {
         "id":"10.1080/10962247.2015.1040526",
         "title":"Air quality and climate connections",
         "abstract":"Multiple linkages connect air quality and climate change. Many air pollutant sources also emit carbon dioxide (CO2), the dominant anthropogenic greenhouse gas (GHG). The two main contributors to non-attainment of U.S. ambient air quality standards, ozone (O3) and particulate matter (PM), interact with radiation, forcing climate change. PM warms by absorbing sunlight (e.g., black carbon) or cools by scattering sunlight (e.g., sulfates) and interacts with clouds; these radiative and microphysical interactions can induce changes in precipitation and regional circulation patterns. Climate change is expected to degrade air quality in many polluted regions by changing air pollution meteorology (ventilation and dilution), precipitation and other removal processes, and by triggering some amplifying responses in atmospheric chemistry and in anthropogenic and natural sources. Together, these processes shape distributions and extreme episodes of O3 and PM. Global modeling indicates that as air pollution programs reduce SO2 to meet health and other air quality goals, near-term warming accelerates due to \"unmasking\" of warming induced by rising CO2. Air pollutant controls on CH4, a potent GHG and precursor to global O3 levels, and on sources with high black carbon (BC) to organic carbon (OC) ratios could offset near-term warming induced by SO2 emission reductions, while reducing global background O3 and regionally high levels of PM. Lowering peak warming requires decreasing atmospheric CO2, which for some source categories would also reduce co-emitted air pollutants or their precursors. Model projections for alternative climate and air quality scenarios indicate a wide range for U.S. surface O3 and fine PM, although regional projections may be confounded by interannual to decadal natural climate variability. Continued implementation of U.S. NOx emission controls guards against rising pollution levels triggered either by climate change or by global emission growth. Improved accuracy and trends in emission inventories are critical for accountability analyses of historical and projected air pollution and climate mitigation policies.",
         "relevant":false
      },
      {
         "id":"10.1177/1010539515592951",
         "title":"Impact of Climate Change on Air Quality and Public Health in Urban Areas",
         "abstract":"This review discusses how climate undergo changes and the effect of climate change on air quality as well as public health. It also covers the inter relationship between climate and air quality. The air quality discussed here are in relation to the 5 criteria pollutants; ozone (O3), carbon dioxide (CO2), nitrogen dioxide (NO2), sulfur dioxide (SO2), and particulate matter (PM). Urban air pollution is the main concern due to higher anthropogenic activities in urban areas. The implications on health are also discussed. Mitigating measures are presented with the final conclusion.",
         "relevant":false
      },
      {
         "id":"10.1039/c2cs35095e",
         "title":"Global air quality and climate",
         "abstract":"Emissions of air pollutants and their precursors determine regional air quality and can alter climate. Climate change can perturb the long-range transport, chemical processing, and local meteorology that influence air pollution. We review the implications of projected changes in methane (CH(4)), ozone precursors (O(3)), and aerosols for climate (expressed in terms of the radiative forcing metric or changes in global surface temperature) and hemispheric-to-continental scale air quality. Reducing the O(3) precursor CH(4) would slow near-term warming by decreasing both CH(4) and tropospheric O(3). Uncertainty remains as to the net climate forcing from anthropogenic nitrogen oxide (NO(x)) emissions, which increase tropospheric O(3) (warming) but also increase aerosols and decrease CH(4) (both cooling). Anthropogenic emissions of carbon monoxide (CO) and non-CH(4) volatile organic compounds (NMVOC) warm by increasing both O(3) and CH(4). Radiative impacts from secondary organic aerosols (SOA) are poorly understood. Black carbon emission controls, by reducing the absorption of sunlight in the atmosphere and on snow and ice, have the potential to slow near-term warming, but uncertainties in coincident emissions of reflective (cooling) aerosols and poorly constrained cloud indirect effects confound robust estimates of net climate impacts. Reducing sulfate and nitrate aerosols would improve air quality and lessen interference with the hydrologic cycle, but lead to warming. A holistic and balanced view is thus needed to assess how air pollution controls influence climate; a first step towards this goal involves estimating net climate impacts from individual emission sectors. Modeling and observational analyses suggest a warming climate degrades air quality (increasing surface O(3) and particulate matter) in many populated regions, including during pollution episodes. Prior Intergovernmental Panel on Climate Change (IPCC) scenarios (SRES) allowed unconstrained growth, whereas the Representative Concentration Pathway (RCP) scenarios assume uniformly an aggressive reduction, of air pollutant emissions. New estimates from the current generation of chemistry-climate models with RCP emissions thus project improved air quality over the next century relative to those using the IPCC SRES scenarios. These two sets of projections likely bracket possible futures. We find that uncertainty in emission-driven changes in air quality is generally greater than uncertainty in climate-driven changes. Confidence in air quality projections is limited by the reliability of anthropogenic emission trajectories and the uncertainties in regional climate responses, feedbacks with the terrestrial biosphere, and oxidation pathways affecting O(3) and SOA.",
         "relevant":false
      },
      {
         "id":"10.1186/s12940-017-0325-2",
         "title":"Climate change impacts on human health over Europe through its effect on air quality",
         "abstract":"This review examines the current literature on the effects of future emissions and climate change on particulate matter (PM) and O3 air quality and on the consequent health impacts, with a focus on Europe. There is considerable literature on the effects of climate change on O3 but fewer studies on the effects of climate change on PM concentrations. Under the latest Intergovernmental Panel on Climate Change (IPCC) 5th assessment report (AR5) Representative Concentration Pathways (RCPs), background O3 entering Europe is expected to decrease under most scenarios due to higher water vapour concentrations in a warmer climate. However, under the extreme pathway RCP8.5 higher (more than double) methane (CH4) abundances lead to increases in background O3 that offset the O3 decrease due to climate change especially for the 2100 period. Regionally, in polluted areas with high levels of nitrogen oxides (NOx), elevated surface temperatures and humidities yield increases in surface O3 - termed the O3 climate penalty - especially in southern Europe. The O3 response is larger for metrics that represent the higher end of the O3 distribution, such as daily maximum O3. Future changes in PM concentrations due to climate change are much less certain, although several recent studies also suggest a PM climate penalty due to high temperatures and humidity and reduced precipitation in northern mid-latitude land regions in 2100.A larger number of studies have examined both future climate and emissions changes under the RCP scenarios. Under these pathways the impact of emission changes on air quality out to the 2050s will be larger than that due to climate change, because of large reductions in emissions of O3 and PM pollutant precursor emissions and the more limited climate change response itself. Climate change will also affect climate extreme events such as heatwaves. Air pollution episodes are associated with stagnation events and sometimes heat waves. Air quality during the 2003 heatwave over Europe has been examined in numerous studies and mechanisms for enhancing O3 have been identified.There are few studies on health effects associated with climate change impacts alone on air quality, but these report higher O3-related health burdens in polluted populated regions and greater PM2.5 health burdens in these emission regions. Studies that examine the combined impacts of climate change and anthropogenic emissions change under the RCP scenarios report reductions in global and European premature O3-respiratory related and PM mortalities arising from the large decreases in precursor emissions. Under RCP 8.5 the large increase in CH4 leads to global and European excess O3-respiratory related mortalities in 2100. For future health effects, besides uncertainty in future O3 and particularly PM concentrations, there is also uncertainty in risk estimates such as effect modification by temperature on pollutant-response relationships and potential future adaptation that would alter exposure risk.",
         "relevant":false
      },
      {
         "id":"10.1371/journal.pmed.1002598",
         "title":"Future ozone-related acute excess mortality under climate and population change scenarios in China: A modeling study",
         "abstract":"Background: Climate change is likely to further worsen ozone pollution in already heavily polluted areas, leading to increased ozone-related health burdens. However, little evidence exists in China, the world's largest greenhouse gas emitter and most populated country. As China is embracing an aging population with changing population size and falling age-standardized mortality rates, the potential impact of population change on ozone-related health burdens is unclear. Moreover, little is known about the seasonal variation of ozone-related health burdens under climate change. We aimed to assess near-term (mid-21st century) future annual and seasonal excess mortality from short-term exposure to ambient ozone in 104 Chinese cities under 2 climate and emission change scenarios and 6 population change scenarios.\n\nMethods and findings: We collected historical ambient ozone observations, population change projections, and baseline mortality rates in 104 cities across China during April 27, 2013, to October 31, 2015 (2013-2015), which included approximately 13% of the total population of mainland China. Using historical ozone monitoring data, we performed bias correction and spatially downscaled future ozone projections at a coarse spatial resolution (2.0° × 2.5°) for the period April 27, 2053, to October 31, 2055 (2053-2055), from a global chemistry-climate model to a fine spatial resolution (0.25° × 0.25°) under 2 Intergovernmental Panel on Climate Change Representative Concentration Pathways (RCPs): RCP4.5, a moderate global warming and emission scenario where global warming is between 1.5°C and 2.0°C, and RCP8.5, a high global warming and emission scenario where global warming exceeds 2.0°C. We then estimated the future annual and seasonal ozone-related acute excess mortality attributable to both climate and population changes using cause-specific, age-group-specific, and season-specific concentration-response functions (CRFs). We used Monte Carlo simulations to obtain empirical confidence intervals (eCIs), quantifying the uncertainty in CRFs and the variability across ensemble members (i.e., 3 predictions of future climate and air quality from slightly different starting conditions) of the global model. Estimates of future changes in annual ozone-related mortality are sensitive to the choice of global warming and emission scenario, decreasing under RCP4.5 (-24.0%) due to declining ozone precursor emissions but increasing under RCP8.5 (10.7%) due to warming climate in 2053-2055 relative to 2013-2015. Higher ambient ozone occurs under the high global warming and emission scenario (RCP8.5), leading to an excess 1,476 (95% eCI: 898 to 2,977) non-accidental deaths per year in 2053-2055 relative to 2013-2015. Future ozone-related acute excess mortality from cardiovascular diseases was 5-8 times greater than that from respiratory diseases. Ozone concentrations increase by 15.1 parts per billion (10-9) in colder months (November to April), contributing to a net yearly increase of 22.3% (95% eCI: 7.7% to 35.4%) in ozone-related mortality under RCP8.5. An aging population, with the proportion of the population aged 65 years and above increased from 8% in 2010 to 24%-33% in 2050, will substantially amplify future ozone-related mortality, leading to a net increase of 23,838 to 78,560 deaths (110% to 363%). Our analysis was mainly limited by using a single global chemistry-climate model and the statistical downscaling approach to project ozone changes under climate change.\n\nConclusions: Our analysis shows increased future ozone-related acute excess mortality under the high global warming and emission scenario RCP8.5 for an aging population in China. Comparison with the lower global warming and emission scenario RCP4.5 suggests that climate change mitigation measures are needed to prevent a rising health burden from exposure to ambient ozone pollution in China.",
         "relevant":false
      },
      {
         "id":22838153,
         "title":"Assessment and statistical modeling of the relationship between remotely sensed aerosol optical depth and PM2.5 in the eastern United States",
         "abstract":"Research in scientific, public health, and policy disciplines relating to the environment increasingly makes use of high-dimensional remote sensing and the output of numerical models in conjunction with traditional observations. Given the public health and resultant public policy implications of the potential health effects of particulate matter (PM*) air pollution, specifically fine PM with an aerodynamic diameter < or = 2.5 pm (PM2.5), there has been substantial recent interest in the use of remote-sensing information, in particular aerosol optical depth (AOD) retrieved from satellites, to help characterize variability in ground-level PM2.5 concentrations in space and time. While the United States and some other developed countries have extensive PM monitoring networks, gaps in data across space and time necessarily occur; the hope is that remote sensing can help fill these gaps. In this report, we are particularly interested in using remote-sensing data to inform estimates of spatial patterns in ambient PM2.5 concentrations at monthly and longer time scales for use in epidemiologic analyses. However, we also analyzed daily data to better disentangle spatial and temporal relationships. For AOD to be helpful, it needs to add information beyond that available from the monitoring network. For analyses of chronic health effects, it needs to add information about the concentrations of long-term average PM2.5; therefore, filling the spatial gaps is key. Much recent evidence has shown that AOD is correlated with PM2.5 in the eastern United States, but the use of AOD in exposure analysis for epidemiologic work has been rare, in part because discrepancies necessarily exist between satellite-retrieved estimates of AOD, which is an atmospheric-column average, and ground-level PM2.5. In this report, we summarize the results of a number of empirical analyses and of the development of statistical models for the use of proxy information, in particular satellite AOD, in predicting PM2.5 concentrations in the eastern United States. We analyzed the spatiotemporal structure of the relationship between PM2.5 and AOD, first using simple correlations both before and after calibration based on meteorology, as well as large-scale spatial and temporal calibration to account for discrepancies between AOD and PM2.5. We then used both raw and calibrated AOD retrievals in statistical models to predict PM2.5 concentrations, accounting for AOD in two ways: primarily as a separate data source contributing a second likelihood to a Bayesian statistical model, as well as a data source on which we could directly regress. Previous consideration of satellite AOD has largely focused on the National Aeronautics and Space Administration (NASA) moderate resolution imaging spectroradiometer (MODIS) and multiangle imaging spectroradiometer (MISR) instruments. One contribution of our work is more extensive consideration of AOD derived from the Geostationary Operational Environmental Satellite East Aerosol/Smoke Product (GOES GASP) AOD and its relationship with PM2.5. In addition to empirically assessing the spatiotemporal relationship between GASP AOD and PM2.5, we considered new statistical techniques to screen anomalous GOES reflectance measurements and account for background surface reflectance. In our statistical work, we developed a new model structure that allowed for more flexible modeling of the proxy discrepancy than previous statistical efforts have had, with a computationally efficient implementation. We also suggested a diagnostic for assessing the scales of the spatial relationship between the proxy and the spatial process of interest (e.g., PM2.5). In brief, we had little success in improving predictions in our eastern-United States domain for use in epidemiologic applications. We found positive correlations of AOD with PM2.5 over time, but less correlation for long-term averages over space, unless we used calibration that adjusted for large-scale discrepancy between AOD and PM2.5 (see sections 3, 4, and 5). Statistical models that combined AOD, PM2.5 observations, and land-use and meteorologic variables were highly predictive of PM2.5 observations held out of the modeling, but AOD added little information beyond that provided by the other sources (see sections 5 and 6). When we used PM2.5 data estimates from the Community Multiscale Air Quality model (CMAQ) as the proxy instead of using AOD, we similarly found little improvement in predicting held-out observations of PM2.5, but when we regressed on CMAQ PM2.5 estimates, the predictions improved moderately in some cases. These results appeared to be caused in part by the fact that large-scale spatial patterns in PM2.5 could be predicted well by smoothing the monitor values, while small-scale spatial patterns in AOD appeared to weakly reflect the variation in PM2.5 inferred from the observations. Using a statistical model that allowed for potential proxy discrepancy at both large and small spatial scales was an important component of our modeling. In particular, when our models did not include a component to account for small-scale discrepancy, predictive performance decreased substantially. Even long-term averages of MISR AOD, considered the best, albeit most sparse, of the AOD products, were only weakly correlated with measured PM2.5 (see section 4). This might have been partly related to the fact that our analysis did not account for spatial variation in the vertical profile of the aerosol. Furthermore, we found evidence that some of the correlation between raw AOD and PM2.5 might have been a function of surface brightness related to land use, rather than having been driven by the detection of aerosol in the AOD retrieval algorithms (see sections 4 and 7). Difficulties in estimating the background surface reflectance in the retrieval algorithms likely explain this finding. With regard to GOES, we found moderate correlations of GASP AOD and PM2.5. The higher correlations of monthly and yearly averages after calibration reflected primarily the improved large-scale correlation, a necessary result of the calibration procedure (see section 3). While the results of this study's GOES reflectance screening and surface reflection correction appeared sensible, correlations of our proposed reflectance-based proxy with PM2.5 were no better than GASP AOD correlations with PM2.5 (see section 7). We had difficulty improving spatial prediction of monthly and yearly average PM2.5 using AOD in the eastern United States, which we attribute to the spatial discrepancy between AOD and measured PM2.5, particularly at smaller scales. This points to the importance of paying attention to the discrepancy structure of proxy information, both from remote-sensing and deterministic models. In particular, important statistical challenges arise in accounting for the discrepancy, given the difficulty in the face of sparse observations of distinguishing the discrepancy from the component of the proxy that is informative about the process of interest. Associations between adverse health outcomes and large-scale variation in PM2.5 (e.g., across regions) may be confounded by unmeasured spatial variation in factors such as diet. Therefore, one important goal was to use AOD to improve predictions of PM2.5 for use in epidemiologic analyses at small-to-moderate spatial scales (within urban areas and within regions). In addition, large-scale PM2.5 variation is well estimated from the monitoring data, at least in the United States. We found little evidence that current AOD products are helpful for improving prediction at small-to-moderate scales in the eastern United States and believe more evidence for the reliability of AOD as a proxy at such scales is needed before making use of AOD for PM2.5 prediction in epidemiologic contexts. While our results relied in part on relatively complicated statistical models, which may be sensitive to modeling assumptions, our exploratory correlation analyses (see sections 3 and 5) and relatively simple regression-style modeling of MISR AOD (see section 4) were consistent with the more complicated modeling results. When assessing the usefulness of AOD in the context of studying chronic health effects, we believe efforts need to focus on disentangling the temporal from the spatial correlations of AOD and PM2.5 and on understanding the spatial scale of correlation and of the discrepancy structure. While our results are discouraging, it is important to note that we attempted to make use of smaller-scale spatial variation in AOD to distinguish spatial variations of relatively small magnitude in long-term concentrations of ambient PM2.5. Our efforts pushed the limits of current technology in a spatial domain with relatively low PM2.5 levels and limited spatial variability. AOD may hold more promise in areas with higher aerosol levels, as the AOD signal would be stronger there relative to the background surface reflectance. Furthermore, for developing countries with high aerosol levels, it is difficult to build statistical models based on PM2.5 measurements and land-use covariates, so AOD may add more incremental information in those contexts. More generally, researchers in remote sensing are involved in ongoing efforts to improve AOD products and develop new approaches to using AOD, such as calibration with model-estimated vertical profiles and the use of speciation information in MISR AOD; these efforts warrant continued investigation of the usefulness of remotely sensed AOD for public health research.",
         "relevant":false
      },
      {
         "id":19202993,
         "title":"Key scientific findings and policy- and health-relevant insights from the U.S. Environmental Protection Agency's Particulate Matter Supersites Program and related studies: an integration and synthesis of results",
         "abstract":"In 1998, the U.S. Environmental Protection Agency (EPA) initiated a major air quality program known as the Particulate Matter (PM) Supersites Program. The Supersites Program was a multiyear, $27 million air quality monitoring program consisting of eight regional air quality projects located throughout the United States, each with differing atmospheric pollution conditions resulting from variations in source emissions and meteorology. The overall goal of the program was to elucidate source-receptor relationships and atmospheric processes leading to PM accumulation on urban and regional scales; thus providing the scientific underpinning for modeling and data analysis efforts to support State Implementation Plans and more effective risk management approaches for PM. The program had three main objectives: (1) conduct methods development and evaluation, (2) characterize ambient PM, and (3) support health effects and exposure research. This paper provides a synthesis of key scientific findings from the Supersites Program and related studies. EPA developed 16 science/policy-relevant questions in conjunction with state and other federal agencies, Regional Planning Organizations, and the private sector. These questions were addressed to the extent possible, even given the vast amount of new information available from the Supersites Program, in a series of papers published as a special issue of the Journal of Air & Waste Management Association (February 2008). This synthesis also includes discussions of: (1) initial Supersites Program support for air quality management efforts in specific locations throughout the United States; (2) selected policy-relevant insights, based on atmospheric sciences findings, useful to air quality managers and decision makers planning emissions management strategies to address current and future PM National Ambient Air Quality Standards (NAAQS) and network planning and implementation; (3) selected health-relevant insights interpreted from atmospheric sciences findings in light of future directions for health and exposure scientists planning studies of the effects of PM on human health; and (4) selected knowledge gaps to guide future research. Finally, given the scope and depth of research and findings from the Supersites Program, this paper provides a reference source so readers can glean a general understanding of the overall research conducted and its policy-relevant insights. Supporting details for the results presented are available through the cited references. An annotated table of contents allows readers to easily find specific subject matter within the text.",
         "relevant":false
      },
      {
         "id":"10.2190/V541-6531-2614-0375",
         "title":"Silenced science: air pollution decision-making at the EPA threatens public health",
         "abstract":"The saga of the Environmental Protection Agency's new particulate matter (PM) rule is yet another example of this Administration's disregard for and disrespect of science and scientists--and may signal the beginning of a disturbing trend to reduce the role of science in protecting the quality of our air. Political interference in the PM case is clear. And more trouble may be in the wings when it comes to acceptable levels of ozone pollution and the process for setting the National Ambient Air Quality Standards (NAAQS). For several years, the Union of Concerned Scientists has been actively monitoring and documenting the misuse of science in public policy-making. Consider this a call to arms. Now is the time to engage your elected officials on these issues.",
         "relevant":false
      },
      {
         "id":"10.1080/10962247.2018.1459325",
         "title":"Adapting air quality management for a changing climate: Survey of local districts in California",
         "abstract":"Air quality can be affected by weather and thus is sensitive to a changing climate. Wildfire (influenced by weather), consecutive high temperature summer days, and other extreme events are projected to become more severe and frequent with climate change. These may create challenging conditions for managing air quality despite policy targets to reduce precursor and pollutant emissions. Although extreme events are becoming more intense and interest in climate adaptation is increasing among public health practitioners, little attention in scholarly literature and policy covers climate adaptation for air quality governance. Understanding the management and managers' perspectives at the local level provides insight about the needs for climate adaptation, including their adaptation status, perspectives, responsibilities, and roles. This study explores local manager perspectives and experiences of managing air quality within a changing climate as one puzzle piece to understand the gap in climate adaptation within the air quality sector. A broader goal is to contribute to the discussion of developing a multi-jurisdictional vision for reducing the impacts of air quality in a changing climate. In 2016 local air quality district managers in California were invited to participate in an online survey of 39 questions focused on extreme event impacts on air quality. The questionnaire focused on present air quality threats and extreme event challenges, adaptation status and strategies, adaptive capacities, perceived barriers to adaptation, and jurisdictional responsibilities and roles. Over 85 percent of the 35 local air districts in California participated in the survey, which represents 80 percent of the state's population. High awareness and knowledge of climate change among local managers indicates they are ready to adopt and take action on policies that would support climate adaptation, but barriers reported suggests they may need policies and adequate funding to take action and make necessary changes.",
         "relevant":false
      },
      {
         "id":"10.1371/journal.pone.0222807",
         "title":"Measuring sustainability of seed-funded earth science informatics projects",
         "abstract":"Short term funding is a common funding model for informatics projects. Funders are interested in maximizing the sustainability and accessibility of the outputs, but there are no commonly accepted practices to do so in the Earth sciences informatics field. We constructed and applied a framework for sustainability drawing from other disciplines that have more published work focusing on sustainability of projects. This framework had seven sustainability influences (outputs modified, code repository used, champion present, workforce stability, support from other organizations, collaboration/partnership, and integration with policy), and three ways of defining sustainability (at the individual-, organization-, and community-level). Using this framework, we evaluated outputs of projects funded by the U.S. Geological Survey's Community for Data Integration (CDI). We found that the various outputs are widely accessible, but not necessarily sustained or maintained. Projects with most sustainability influences often became institutionalized and met required needs of the community. Even if proposed outputs were not delivered or sustained, knowledge of lessons learned could be spread to build community capacity in a topic, which is another type of sustainability. We conclude by summarizing lessons for individuals applying for short-term funding, and for organizations managing programs that provide such funding, for maximizing sustainability of project outcomes.",
         "relevant":false
      },
      {
         "id":"10.1111/nph.13338",
         "title":"One physical system': Tansley's ecosystem as Earth's critical zone",
         "abstract":"Integrative concepts of the biosphere, ecosystem, biogeocenosis and, recently, Earth's critical zone embrace scientific disciplines that link matter, energy and organisms in a systems-level understanding of our remarkable planet. Here, we assert the congruence of Tansley's (1935) venerable ecosystem concept of 'one physical system' with Earth science's critical zone. Ecosystems and critical zones are congruent across spatial-temporal scales from vegetation-clad weathering profiles and hillslopes, small catchments, landscapes, river basins, continents, to Earth's whole terrestrial surface. What may be less obvious is congruence in the vertical dimension. We use ecosystem metabolism to argue that full accounting of photosynthetically fixed carbon includes respiratory CO₂ and carbonic acid that propagate to the base of the critical zone itself. Although a small fraction of respiration, the downward diffusion of CO₂ helps determine rates of soil formation and, ultimately, ecosystem evolution and resilience. Because life in the upper portions of terrestrial ecosystems significantly affects biogeochemistry throughout weathering profiles, the lower boundaries of most terrestrial ecosystems have been demarcated at depths too shallow to permit a complete understanding of ecosystem structure and function. Opportunities abound to explore connections between upper and lower components of critical-zone ecosystems, between soils and streams in watersheds, and between plant-derived CO₂ and deep microbial communities and mineral weathering.",
         "relevant":false
      },
      {
         "id":"10.1007/s11356-017-0127-7",
         "title":"Coupling population dynamics with earth system models: the POPEM model",
         "abstract":"Precise modeling of CO2 emissions is important for environmental research. This paper presents a new model of human population dynamics that can be embedded into ESMs (Earth System Models) to improve climate modeling. Through a system dynamics approach, we develop a cohort-component model that successfully simulates historical population dynamics with fine spatial resolution (about 1°×1°). The population projections are used to improve the estimates of CO2 emissions, thus transcending the bulk approach of existing models and allowing more realistic non-linear effects to feature in the simulations. The module, dubbed POPEM (from Population Parameterization for Earth Models), is compared with current emission inventories and validated against UN aggregated data. Finally, it is shown that the module can be used to advance toward fully coupling the social and natural components of the Earth system, an emerging research path for environmental science and pollution research.",
         "relevant":false
      },
      {
         "id":"10.1098/rsta.2008.0219",
         "title":"The computational future for climate and Earth system models: on the path to petaflop and beyond",
         "abstract":"The development of the climate and Earth system models has had a long history, starting with the building of individual atmospheric, ocean, sea ice, land vegetation, biogeochemical, glacial and ecological model components. The early researchers were much aware of the long-term goal of building the Earth system models that would go beyond what is usually included in the climate models by adding interactive biogeochemical interactions. In the early days, the progress was limited by computer capability, as well as by our knowledge of the physical and chemical processes. Over the last few decades, there has been much improved knowledge, better observations for validation and more powerful supercomputer systems that are increasingly meeting the new challenges of comprehensive models. Some of the climate model history will be presented, along with some of the successes and difficulties encountered with present-day supercomputer systems.",
         "relevant":false
      },
      {
         "id":"10.1371/journal.pone.0209311",
         "title":"Development of a measure to evaluate competence perceptions of natural and social science",
         "abstract":"Interdisciplinary scientific research teams are essential for responding to society's complex scientific and social issues. Perceptual barriers to collaboration can inhibit the productivity of teams crossing traditional disciplinary boundaries. To explore these perceptual barriers, survey measures related to perceived competence were developed and validated with a population of earth scientists (n = 449) ranging from undergraduates through professionals. Resulting competence scales included three factors that we labeled as Perceived Respect (PR), Perceived Methodological Rigor (PM), and Perceived Intelligence (Pi). A Mann-Whitney U test revealed that earth scientists perceived social science/scientists as significantly less competent than natural science/scientists. A multivariate multilevel analysis indicated that women perceived scientists as more intelligent than did men. Working with social scientists and holding an earth science PhD changed earth scientists' perceptions of social science on multiple scales. Our study indicates that competence in scientific disciplines is a multidimensional construct. Our results from earth scientists also indicate that perceptual barriers towards other scientific disciplines should be studied further as interdisciplinarity in scientific research continues to be encouraged as a solution to many socio-scientific problems.",
         "relevant":false
      },
      {
         "id":"10.1098/rstb.2016.0489",
         "title":"History and contemporary significance of the Rhynie cherts-our earliest preserved terrestrial ecosystem",
         "abstract":"The Rhynie cherts Unit is a 407 million-year old geological site in Scotland that preserves the most ancient known land plant ecosystem, including associated animals, fungi, algae and bacteria. The quality of preservation is astonishing, and the initial description of several plants 100 years ago had a huge impact on botany. Subsequent discoveries provided unparalleled insights into early life on land. These include the earliest records of plant life cycles and fungal symbioses, the nature of soil microorganisms and the diversity of arthropods. Today the Rhynie chert (here including the Rhynie and Windyfield cherts) takes on new relevance, especially in relation to advances in the fields of developmental genetics and Earth systems science. New methods and analytical techniques also contribute to a better understanding of the environment and its organisms. Key discoveries are reviewed, focusing on the geology of the site, the organisms and the palaeoenvironments. The plants and their symbionts are of particular relevance to understanding the early evolution of the plant life cycle and the origins of fundamental organs and tissue systems. The Rhynie chert provides remarkable insights into the structure and interactions of early terrestrial communities, and it has a significant role to play in developing our understanding of their broader impact on Earth systems.This article is part of a discussion meeting issue 'The Rhynie cherts: our earliest terrestrial ecosystem revisited'.",
         "relevant":false
      },
      {
         "id":"10.1126/science.aal3003",
         "title":"Probing the frontiers of particle physics with tabletop-scale experiments",
         "abstract":"The field of particle physics is in a peculiar state. The standard model of particle theory successfully describes every fundamental particle and force observed in laboratories, yet fails to explain properties of the universe such as the existence of dark matter, the amount of dark energy, and the preponderance of matter over antimatter. Huge experiments, of increasing scale and cost, continue to search for new particles and forces that might explain these phenomena. However, these frontiers also are explored in certain smaller, laboratory-scale \"tabletop\" experiments. This approach uses precision measurement techniques and devices from atomic, quantum, and condensed-matter physics to detect tiny signals due to new particles or forces. Discoveries in fundamental physics may well come first from small-scale experiments of this type.",
         "relevant":false
      },
      {
         "id":"10.1098/rsta.2018.0185",
         "title":"Particle physics experiments based on the AWAKE acceleration scheme",
         "abstract":"New particle acceleration schemes open up exciting opportunities, potentially providing more compact or higher-energy accelerators. The AWAKE experiment at CERN is currently taking data to establish the method of proton-driven plasma wakefield acceleration. A second phase aims to demonstrate that bunches of about 109 electrons can be accelerated to high energy, preserving emittance and that the process is scalable with length. With this, an electron beam of [Formula: see text](50 GeV) could be available for new fixed-target or beam-dump experiments searching for the hidden sector, like dark photons. The rate of electrons on target could be increased by a factor of more than 1000 compared to that currently available, leading to a corresponding increase in sensitivity to new physics. Such a beam could also be brought into collision with a high-power laser and thereby probe the completely unmeasured region of strong fields at values of the Schwinger critical field. An ultimate goal is to produce an electron beam of [Formula: see text](3 TeV) and collide with an Large Hadron Collider proton beam. This very high-energy electron-proton collider would probe a new regime in which the structure of matter is completely unknown. This article is part of the Theo Murphy meeting issue 'Directions in particle beam-driven plasma wakefield acceleration'.",
         "relevant":false
      },
      {
         "id":"10.1111/gcb.13475",
         "title":"Coarse climate change projections for species living in a fine-scaled world",
         "abstract":"Accurately predicting biological impacts of climate change is necessary to guide policy. However, the resolution of climate data could be affecting the accuracy of climate change impact assessments. Here, we review the spatial and temporal resolution of climate data used in impact assessments and demonstrate that these resolutions are often too coarse relative to biologically relevant scales. We then develop a framework that partitions climate into three important components: trend, variance, and autocorrelation. We apply this framework to map different global climate regimes and identify where coarse climate data is most and least likely to reduce the accuracy of impact assessments. We show that impact assessments for many large mammals and birds use climate data with a spatial resolution similar to the biologically relevant area encompassing population dynamics. Conversely, impact assessments for many small mammals, herpetofauna, and plants use climate data with a spatial resolution that is orders of magnitude larger than the area encompassing population dynamics. Most impact assessments also use climate data with a coarse temporal resolution. We suggest that climate data with a coarse spatial resolution is likely to reduce the accuracy of impact assessments the most in climates with high spatial trend and variance (e.g., much of western North and South America) and the least in climates with low spatial trend and variance (e.g., the Great Plains of the USA). Climate data with a coarse temporal resolution is likely to reduce the accuracy of impact assessments the most in the northern half of the northern hemisphere where temporal climatic variance is high. Our framework provides one way to identify where improving the resolution of climate data will have the largest impact on the accuracy of biological predictions under climate change.",
         "relevant":false
      },
      {
         "id":"10.1111/gcb.13629",
         "title":"Hydrologic refugia, plants, and climate change",
         "abstract":"Climate, physical landscapes, and biota interact to generate heterogeneous hydrologic conditions in space and over time, which are reflected in spatial patterns of species distributions. As these species distributions respond to rapid climate change, microrefugia may support local species persistence in the face of deteriorating climatic suitability. Recent focus on temperature as a determinant of microrefugia insufficiently accounts for the importance of hydrologic processes and changing water availability with changing climate. Where water scarcity is a major limitation now or under future climates, hydrologic microrefugia are likely to prove essential for species persistence, particularly for sessile species and plants. Zones of high relative water availability - mesic microenvironments - are generated by a wide array of hydrologic processes, and may be loosely coupled to climatic processes and therefore buffered from climate change. Here, we review the mechanisms that generate mesic microenvironments and their likely robustness in the face of climate change. We argue that mesic microenvironments will act as species-specific refugia only if the nature and space/time variability in water availability are compatible with the ecological requirements of a target species. We illustrate this argument with case studies drawn from California oak woodland ecosystems. We posit that identification of hydrologic refugia could form a cornerstone of climate-cognizant conservation strategies, but that this would require improved understanding of climate change effects on key hydrologic processes, including frequently cryptic processes such as groundwater flow.",
         "relevant":false
      },
      {
         "id":"10.1111/gcb.13653",
         "title":"Designing ecological climate change impact assessments to reflect key climatic drivers",
         "abstract":"Identifying the climatic drivers of an ecological system is a key step in assessing its vulnerability to climate change. The climatic dimensions to which a species or system is most sensitive - such as means or extremes - can guide methodological decisions for projections of ecological impacts and vulnerabilities. However, scientific workflows for combining climate projections with ecological models have received little explicit attention. We review Global Climate Model (GCM) performance along different dimensions of change and compare frameworks for integrating GCM output into ecological models. In systems sensitive to climatological means, it is straightforward to base ecological impact assessments on mean projected changes from several GCMs. Ecological systems sensitive to climatic extremes may benefit from what we term the 'model space' approach: a comparison of ecological projections based on simulated climate from historical and future time periods. This approach leverages the experimental framework used in climate modeling, in which historical climate simulations serve as controls for future projections. Moreover, it can capture projected changes in the intensity and frequency of climatic extremes, rather than assuming that future means will determine future extremes. Given the recent emphasis on the ecological impacts of climatic extremes, the strategies we describe will be applicable across species and systems. We also highlight practical considerations for the selection of climate models and data products, emphasizing that the spatial resolution of the climate change signal is generally coarser than the grid cell size of downscaled climate model output. Our review illustrates how an understanding of how climate model outputs are derived and downscaled can improve the selection and application of climatic data used in ecological modeling.",
         "relevant":false
      },
      {
         "id":"10.1073/pnas.1602817113",
         "title":"Achieving climate connectivity in a fragmented landscape",
         "abstract":"The contiguous United States contains a disconnected patchwork of natural lands. This fragmentation by human activities limits species' ability to track suitable climates as they rapidly shift. However, most models that project species movement needs have not examined where fragmentation will limit those movements. Here, we quantify climate connectivity, the capacity of landscape configuration to allow species movement in the face of dynamically shifting climate. Using this metric, we assess to what extent habitat fragmentation will limit species movements in response to climate change. We then evaluate how creating corridors to promote climate connectivity could potentially mitigate these restrictions, and we assess where strategies to increase connectivity will be most beneficial. By analyzing fragmentation patterns across the contiguous United States, we demonstrate that only 41% of natural land area retains enough connectivity to allow plants and animals to maintain climatic parity as the climate warms. In the eastern United States, less than 2% of natural area is sufficiently connected. Introducing corridors to facilitate movement through human-dominated regions increases the percentage of climatically connected natural area to 65%, with the most impactful gains in low-elevation regions, particularly in the southeastern United States. These climate connectivity analyses allow ecologists and conservation practitioners to determine the most effective regions for increasing connectivity. More importantly, our findings demonstrate that increasing climate connectivity is critical for allowing species to track rapidly changing climates, reconfiguring habitats to promote access to suitable climates.",
         "relevant":false
      },
      {
         "id":"10.1371/journal.pone.0165073",
         "title":"Assessing Mammal Exposure to Climate Change in the Brazilian Amazon",
         "abstract":"Human-induced climate change is considered a conspicuous threat to biodiversity in the 21st century. Species' response to climate change depends on their exposition, sensitivity and ability to adapt to novel climates. Exposure to climate change is however uneven within species' range, so that some populations may be more at risk than others. Identifying the regions most exposed to climate change is therefore a first and pivotal step on determining species' vulnerability across their geographic ranges. Here, we aimed at quantifying mammal local exposure to climate change across species' ranges. We identified areas in the Brazilian Amazon where mammals will be critically exposed to non-analogue climates in the future with different variables predicted by 15 global circulation climate forecasts. We also built a null model to assess the effectiveness of the Amazon protected areas in buffering the effects of climate change on mammals, using an innovative and more realistic approach. We found that 85% of species are likely to be exposed to non-analogue climatic conditions in more than 80% of their ranges by 2070. That percentage is even higher for endemic mammals; almost all endemic species are predicted to be exposed in more than 80% of their range. Exposure patterns also varied with different climatic variables and seem to be geographically structured. Western and northern Amazon species are more likely to experience temperature anomalies while northeastern species will be more affected by rainfall abnormality. We also observed an increase in the number of critically-exposed species from 2050 to 2070. Overall, our results indicate that mammals might face high exposure to climate change and that protected areas will probably not be efficient enough to avert those impacts.",
         "relevant":false
      },
      {
         "id":"10.1111/j.1095-8649.2009.02180.x",
         "title":"Implications of climate change for the fishes of the British Isles",
         "abstract":"Recent climatic change has been recorded across the globe. Although environmental change is a characteristic feature of life on Earth and has played a major role in the evolution and global distribution of biodiversity, predicted future rates of climatic change, especially in temperature, are such that they will exceed any that has occurred over recent geological time. Climate change is considered as a key threat to biodiversity and to the structure and function of ecosystems that may already be subject to significant anthropogenic stress. The current understanding of climate change and its likely consequences for the fishes of Britain and Ireland and the surrounding seas are reviewed through a series of case studies detailing the likely response of several marine, diadromous and freshwater fishes to climate change. Changes in climate, and in particular, temperature have and will continue to affect fish at all levels of biological organization: cellular, individual, population, species, community and ecosystem, influencing physiological and ecological processes in a number of direct, indirect and complex ways. The response of fishes and of other aquatic taxa will vary according to their tolerances and life stage and are complex and difficult to predict. Fishes may respond directly to climate-change-related shifts in environmental processes or indirectly to other influences, such as community-level interactions with other taxa. However, the ability to adapt to the predicted changes in climate will vary between species and between habitats and there will be winners and losers. In marine habitats, recent changes in fish community structure will continue as fishes shift their distributions relative to their temperature preferences. This may lead to the loss of some economically important cold-adapted species such as Gadus morhua and Clupea harengus from some areas around Britain and Ireland, and the establishment of some new, warm-adapted species. Increased temperatures are likely to favour cool-adapted (e.g. Perca fluviatilis) and warm-adapted freshwater fishes (e.g. roach Rutilus rutilus and other cyprinids) whose distribution and reproductive success may currently be constrained by temperature rather than by cold-adapted species (e.g. salmonids). Species that occur in Britain and Ireland that are at the edge of their distribution will be most affected, both negatively and positively. Populations of conservation importance (e.g.Salvelinus alpinus and Coregonus spp.) may decline irreversibly. However, changes in food-web dynamics and physiological adaptation, for example because of climate change, may obscure or alter predicted responses. The residual inertia in climate systems is such that even a complete cessation in emissions would still leave fishes exposed to continued climate change for at least half a century. Hence, regardless of the success or failure of programmes aimed at curbing climate change, major changes in fish communities can be expected over the next 50 years with a concomitant need to adapt management strategies accordingly.",
         "relevant":false
      },
      {
         "id":"10.1098/rsbl.2017.0218",
         "title":"Climate and sex ratio variation in a viviparous lizard",
         "abstract":"The extent to which key biological processes, such as sex determination, respond to environmental fluctuations is fundamental for assessing species' susceptibility to ongoing climate change. Few studies, however, address how climate affects offspring sex in the wild. We monitored two climatically distinct populations of the viviparous skink Niveoscincus ocellatus for 16 years, recording environmental temperatures, offspring sex and date of birth. We found strong population-specific effects of temperature on offspring sex, with female offspring more common in warm years at the lowland site but no effect at the highland site. In contrast, date of birth advanced similarly in response to temperature at both sites. These results suggest strong population-specific effects of temperature on offspring sex that are independent of climatic effects on other physiological processes. These results have significant implications for our understanding of the ecological and evolutionary consequences of variation in sex ratios under climate change.",
         "relevant":false
      },
      {
         "id":"10.1037/bul0000096",
         "title":"Emotion and the prefrontal cortex: An integrative review",
         "abstract":"The prefrontal cortex (PFC) plays a critical role in the generation and regulation of emotion. However, we lack an integrative framework for understanding how different emotion-related functions are organized across the entire expanse of the PFC, as prior reviews have generally focused on specific emotional processes (e.g., decision making) or specific anatomical regions (e.g., orbitofrontal cortex). Additionally, psychological theories and neuroscientific investigations have proceeded largely independently because of the lack of a common framework. Here, we provide a comprehensive review of functional neuroimaging, electrophysiological, lesion, and structural connectivity studies on the emotion-related functions of 8 subregions spanning the entire PFC. We introduce the appraisal-by-content model, which provides a new framework for integrating the diverse range of empirical findings. Within this framework, appraisal serves as a unifying principle for understanding the PFC's role in emotion, while relative content-specialization serves as a differentiating principle for understanding the role of each subregion. A synthesis of data from affective, social, and cognitive neuroscience studies suggests that different PFC subregions are preferentially involved in assigning value to specific types of inputs: exteroceptive sensations, episodic memories and imagined future events, viscero-sensory signals, viscero-motor signals, actions, others' mental states (e.g., intentions), self-related information, and ongoing emotions. We discuss the implications of this integrative framework for understanding emotion regulation, value-based decision making, emotional salience, and refining theoretical models of emotion. This framework provides a unified understanding of how emotional processes are organized across PFC subregions and generates new hypotheses about the mechanisms underlying adaptive and maladaptive emotional functioning. (PsycINFO Database Record",
         "relevant":false
      },
      {
         "id":"10.1016/B978-0-12-804281-6.00014-8",
         "title":"Emotion regulation across the life span",
         "abstract":"Being able to flexibly regulate one's emotions is critical for adaptive functioning across the life span. The importance of emotion regulation for human cognition has been reflected in the marked increase in the amount of psychologic research on emotion and its regulation in the past two decades. In this chapter, we review theoretical and empirical advances in this research, with a particular focus on the neural bases of emotion regulation. We begin with a brief overview of the field at present and provide a general primer on the behavioral and neuroimaging methods used to study emotion regulation. We then outline the brain regions involved in both triggering and modulating affect, and how they may change throughout development and into old age. Finally, we conclude with a roadmap for the future study of emotion regulation, in particular focusing on how to integrate measures with high ecologic validity (e.g., experience sampling, social emotion regulation) with neuroimaging techniques.",
         "relevant":false
      },
      {
         "id":"10.1016/bs.pbr.2019.02.007",
         "title":"Emotion regulation and emotion perception in aging: A perspective on age-related differences and similarities",
         "abstract":"The aim of this chapter is to review recent literature describing how developments in cognition may contribute to age-related changes in emotional processes, specifically emotion regulation and emotion perception. In general, older adults are more likely than young adults to report feeling positive. Prominent conceptual models of cognitive and emotional development in aging attempt to explain why the affective lives of older adults might not undergo similar age-related declines as other cognitive and physical systems. In this chapter, we will discuss predictions of cognitive and emotional development from several leading conceptual models of aging. We will then examine how closely the evidence from the fields of emotion regulation and emotion perception coincides with conceptual predictions. Finally, we will attempt to negotiate findings of age differences and age similarities in emotional processes as well as provide suggestions for future studies of emotion and cognition in aging.",
         "relevant":false
      },
      {
         "id":"10.1016/j.smrv.2015.12.006",
         "title":"Sleep and emotion regulation: An organizing, integrative review",
         "abstract":"A growing body of research suggests that disrupted sleep is a robust risk and maintenance factor for a range of psychiatric conditions. One explanatory mechanism linking sleep and psychological health is emotion regulation. However, numerous components embedded within this construct create both conceptual and empirical challenges to the study of emotion regulation. These challenges are reflected in most sleep-emotion research by way of poor delineation of constructs and insufficient distinction among emotional processes. Most notably, a majority of research has focused on emotions generated as a consequence of inadequate sleep rather than underlying regulatory processes that may alter these experiences. The current review utilizes the process model of emotion regulation as an organizing framework for examining the impact of sleep upon various aspects of emotional experiences. Evidence is provided for maladaptive changes in emotion at multiple stages of the emotion generation and regulation process. We conclude with a call for experimental research designed to clearly explicate which points in the emotion regulation process appear most vulnerable to sleep loss as well as longitudinal studies to follow these processes in relation to the development of psychopathological conditions.",
         "relevant":false
      },
      {
         "id":"10.3928/19382359-20180710-01",
         "title":"Music and Brain Development",
         "abstract":"When I was young, I remember my parents used to play the Beatles song \"Here Comes the Sun\" in the morning on the old record player. My mother helped hire the first band director at my middle school, and my father played opera in the car on my way to elementary school. I dabbled with different instruments, and played trumpet (cornet actually), flute, piccolo, oboe, English horn, and Ukulele. I also sang in several choirs and still sing today. What did all this musical influence do to my developing brain? This article reviews some of the existing evidence about music and brain development, to make a case for exposing children to music at an early age and supporting school music programs. [Pediatr Ann. 2018;47(8):e306-e308.].",
         "relevant":false
      },
      {
         "id":"10.1016/bs.pbr.2018.03.009",
         "title":"Reflections on music, affect, and sociality",
         "abstract":"Music is an important facet of and practice in human cultures, significantly related to its capacity to induce a range of intense and complex emotions. Studying the psychological and neurophysiological responses to music allows us to examine and uncover the neural mechanisms underlying the emotional impact of music. We provide an overview of different aspects of current research on how music listening produces emotions and the corresponding feelings, and consider the underlying neurophysiological mechanisms. We conclude with evidence to suggest that musical training may influence the ability to recognize the emotions of others.",
         "relevant":false
      },
      {
         "id":"10.1177/0898010117710855",
         "title":"The Effect of Music on the Spirituality of Patients: A Systematic Review",
         "abstract":"Purpose: Although some studies have suggested that music can positively affect physical and psychological variables, few have evaluated its effects on spirituality. This study aimed to evaluate the effects of musical interventions on the spirituality of patients, regardless of diagnoses.\n\nMethod: This was a systematic literature review that followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) recommendations conducted through a relevant search of terms in six databases (PubMed, Web of Science, CINAHL, PsycINFO, ScienceDirect, and LILACS) without temporal delimitation. Experimental or quasi-experimental studies were included, involving participants regardless of diagnoses, to assess the effect of music on spirituality, either through musical intervention as music medicine or through music therapy. The methodological quality of included studies was evaluated using the Physiotherapy Evidence Database scale.\n\nResults: A total of 147 studies were identified; 7 met the inclusion criteria. Five studies were randomized controlled trials involving six music therapists leading the musical intervention with the active participation of patients. The interventions used were heterogeneous. Three studies were associated with improved spirituality after the intervention. Four studies used measurements to evaluate spiritual well-being.\n\nConclusion: This review did not allow ascertaining the positive impact of music intervention on spirituality in patients, which motivates further research.",
         "relevant":false
      },
      {
         "id":"10.1371/journal.pone.0206531",
         "title":"The music that helps people sleep and the reasons they believe it works: A mixed methods analysis of online survey reports",
         "abstract":"Sleep loss is a widespread problem with serious physical and economic consequences. Music can impact upon physical, psychological and emotional states, which may explain anecdotal reports of its success as an everyday sleep aid. However, there is a lack of systematic data on how widely it is used, why people opt for music as a sleep aid, or what music works; hence the underlying drivers to music-sleep effects remain unclear. We investigated music as a sleep aid within the general public via a mixed methods data online survey (n = 651) that scored musicality, sleep habits, and open text responses on what music helps sleep and why. In total, 62% of respondents stated that they used music to help them sleep. They reported fourteen musical genres comprising 545 artists. Linear modelling found stress, age, and music use as significant predictors of sleep quality (PSQI) scores. Regression tree modelling revealed that younger people with higher musical engagement were significantly more likely to use music to aid sleep. Thematic analysis of the open text responses generated four themes that described why people believe music helps sleep: music offers unique properties that stimulate sleep (Provide), music is part of a normal sleep routine (Habit), music induces a physical or mental state conducive to sleep (State), and music blocks an internal or external stimulus that would otherwise disrupt sleep (Distract). This survey provides new evidence into the relationship between music and sleep in a population that ranged widely in age, musicality, sleep habits and stress levels. In particular, the results highlight the varied pathways of effect between music and sleep. Diversity was observed both in music choices, which reflected idiosyncratic preferences rather than any consistent musical structure, and in the reasons why music supports good sleep, which went far beyond simple physical/mental relaxation.",
         "relevant":false
      },
      {
         "id":"10.1098/rstb.2015.0539",
         "title":"Condition-dependent sex: who does it, when and why?",
         "abstract":"We review the phenomenon of condition-dependent sex-where individuals' condition affects the likelihood that they will reproduce sexually rather than asexually. In recent years, condition-dependent sex has been studied both theoretically and empirically. Empirical results in microbes, fungi and plants support the theoretical prediction that negative condition-dependent sex, in which individuals in poor condition are more likely to reproduce sexually, can be evolutionarily advantageous under a wide range of settings. Here, we review the evidence for condition-dependent sex and its potential implications for the long-term survival and adaptability of populations. We conclude by asking why condition-dependent sex is not more commonly observed, and by considering generalizations of condition-dependent sex that might apply even for obligate sexuals.This article is part of the themed issue 'Weird sex: the underappreciated diversity of sexual reproduction'.",
         "relevant":false
      },
      {
         "id":"10.1098/rstb.2015.0540",
         "title":"The frequency of sex in fungi",
         "abstract":"Fungi are a diverse group of organisms with a huge variation in reproductive strategy. While almost all species can reproduce sexually, many reproduce asexually most of the time. When sexual reproduction does occur, large variation exists in the amount of in- and out-breeding. While budding yeast is expected to outcross only once every 10 000 generations, other fungi are obligate outcrossers with well-mixed panmictic populations. In this review, we give an overview of the costs and benefits of sexual and asexual reproduction in fungi, and the mechanisms that evolved in fungi to reduce the costs of either mode. The proximate molecular mechanisms potentiating outcrossing and meiosis appear to be present in nearly all fungi, making them of little use for predicting outcrossing rates, but also suggesting the absence of true ancient asexual lineages. We review how population genetic methods can be used to estimate the frequency of sex in fungi and provide empirical data that support a mixed mode of reproduction in many species with rare to frequent sex in between rounds of mitotic reproduction. Finally, we highlight how these estimates might be affected by the fungus-specific mechanisms that evolved to reduce the costs of sexual and asexual reproduction.This article is part of the themed issue 'Weird sex: the underappreciated diversity of sexual reproduction'.",
         "relevant":false
      },
      {
         "id":"10.1007/s12064-009-0077-9",
         "title":"Diploidy and the selective advantage for sexual reproduction in unicellular organisms",
         "abstract":"This article develops mathematical models describing the evolutionary dynamics of both asexually and sexually reproducing populations of diploid unicellular organisms. The asexual and sexual life cycles are based on the asexual and sexual life cycles in Saccharomyces cerevisiae, Baker's yeast, which normally reproduces by asexual budding, but switches to sexual reproduction when stressed. The mathematical models consider three reproduction pathways: (1) Asexual reproduction, (2) self-fertilization, and (3) sexual reproduction. We also consider two forms of genome organization. In the first case, we assume that the genome consists of two multi-gene chromosomes, whereas in the second case, we consider the opposite extreme and assume that each gene defines a separate chromosome, which we call the multi-chromosome genome. These two cases are considered to explore the role that recombination has on the mutation-selection balance and the selective advantage of the various reproduction strategies. We assume that the purpose of diploidy is to provide redundancy, so that damage to a gene may be repaired using the other, presumably undamaged copy (a process known as homologous recombination repair). As a result, we assume that the fitness of the organism only depends on the number of homologous gene pairs that contain at least one functional copy of a given gene. If the organism has at least one functional copy of every gene in the genome, we assume a fitness of 1. In general, if the organism has l homologous pairs that lack a functional copy of the given gene, then the fitness of the organism is kappa(l). The kappa(l) are assumed to be monotonically decreasing, so that kappa(0) = 1 > kappa(1) > kappa(2) > cdots, three dots, centered > kappa(infinity) = 0. For nearly all of the reproduction strategies we consider, we find, in the limit of large N, that the mean fitness at mutation-selection balance is max{2e(-mu) - 1,0} where N is the number of genes in the haploid set of the genome, epsilon is the probability that a given DNA template strand of a given gene produces a mutated daughter during replication, and mu = Nepsilon. The only exception is the sexual reproduction pathway for the multi-chromosomed genome. Assuming a multiplicative fitness landscape where kappa(l) = alpha(l) for alpha in (0, 1), this strategy is found to have a mean fitness that exceeds the mean fitness of all the other strategies. Furthermore, while other reproduction strategies experience a total loss of viability due to the steady accumulation of deleterious mutations once mu exceeds [Formula: see text] no such transition occurs in the sexual pathway. Indeed, in the limit as alpha --> 1 for the multiplicative landscape, we can show that the mean fitness for the sexual pathway with the multi-chromosomed genome converges to e(-2mu), which is always positive. We explicitly allow for mitotic recombination in this study, which, in contrast to previous studies using different models, does not have any advantage over other asexual reproduction strategies. The results of this article provide a basis for understanding the selective advantage of the specific meiotic pathway that is employed by sexually reproducing organisms. The results of this article also suggest an explanation for why unicellular organisms such as Saccharomyces cerevisiae (Baker's yeast) switch to a sexual mode of reproduction when stressed. While the results of this article are based on modeling mutation-propagation in unicellular organisms, they nevertheless suggest that, in more complex organisms with significantly larger genomes, sex is necessary to prevent the loss of viability of a population due to genetic drift. Finally, and perhaps most importantly, the results of this article demonstrate a selective advantage for sexual reproduction with fewer and much less restrictive assumptions than those of previous studies.",
         "relevant":false
      },
      {
         "id":"10.1002/bies.20833",
         "title":"Constraints on the evolution of asexual reproduction",
         "abstract":"Sexual reproduction is almost ubiquitous among multicellular organisms even though it entails severe fitness costs. To resolve this apparent paradox, an extensive body of research has been devoted to identifying the selective advantages of recombination that counteract these costs. Yet, how easy is it to make the transition to asexual reproduction once sexual reproduction has been established for a long time? The present review approaches this question by considering factors that impede the evolution of parthenogenesis in animals. Most importantly, eggs need a diploid chromosome set in most species in order to develop normally. Next, eggs may need to be activated by sperm, and sperm may also contribute centrioles and other paternal factors to the zygote. Depending on how diploidy is achieved mechanistically, further problems may arise in offspring that stem from 'inbreeding depression' or inappropriate sex determination systems. Finally, genomic imprinting is another well-known barrier to the evolution of asexuality in mammals. Studies on species with occasional, deficient parthenogenesis indicate that the relative importance of these constraints may vary widely. The intimate evolutionary relations between haplodiploidy and parthenogenesis as well as implications for the clade selection hypothesis of the maintenance of sexual reproduction are also discussed.",
         "relevant":false
      },
      {
         "id":"10.1098/rspb.2017.2706",
         "title":"How oxygen gave rise to eukaryotic sex",
         "abstract":"How did full meiotic eukaryotic sex evolve and what was the immediate advantage allowing it to develop? We propose that the crucial determinant can be found in internal reactive oxygen species (ROS) formation at the start of eukaryotic evolution approximately 2 × 109 years ago. The large amount of ROS coming from a bacterial endosymbiont gave rise to DNA damage and vast increases in host genome mutation rates. Eukaryogenesis and chromosome evolution represent adaptations to oxidative stress. The host, an archaeon, most probably already had repair mechanisms based on DNA pairing and recombination, and possibly some kind of primitive cell fusion mechanism. The detrimental effects of internal ROS formation on host genome integrity set the stage allowing evolution of meiotic sex from these humble beginnings. Basic meiotic mechanisms thus probably evolved in response to endogenous ROS production by the 'pre-mitochondrion'. This alternative to mitosis is crucial under novel, ROS-producing stress situations, like extensive motility or phagotrophy in heterotrophs and endosymbiontic photosynthesis in autotrophs. In multicellular eukaryotes with a germline-soma differentiation, meiotic sex with diploid-haploid cycles improved efficient purging of deleterious mutations. Constant pressure of endogenous ROS explains the ubiquitous maintenance of meiotic sex in practically all eukaryotic kingdoms. Here, we discuss the relevant observations underpinning this model.",
         "relevant":false
      },
      {
         "id":"10.1093/gbe/evw136",
         "title":"Mitochondria, the Cell Cycle, and the Origin of Sex via a Syncytial Eukaryote Common Ancestor",
         "abstract":"Theories for the origin of sex traditionally start with an asexual mitosing cell and add recombination, thereby deriving meiosis from mitosis. Though sex was clearly present in the eukaryote common ancestor, the order of events linking the origin of sex and the origin of mitosis is unknown. Here, we present an evolutionary inference for the origin of sex starting with a bacterial ancestor of mitochondria in the cytosol of its archaeal host. We posit that symbiotic association led to the origin of mitochondria and gene transfer to host's genome, generating a nucleus and a dedicated translational compartment, the eukaryotic cytosol, in which-by virtue of mitochondria-metabolic energy was not limiting. Spontaneous protein aggregation (monomer polymerization) and Adenosine Tri-phosphate (ATP)-dependent macromolecular movement in the cytosol thereby became selectable, giving rise to continuous microtubule-dependent chromosome separation (reduction division). We propose that eukaryotic chromosome division arose in a filamentous, syncytial, multinucleated ancestor, in which nuclei with insufficient chromosome numbers could complement each other through mRNA in the cytosol and generate new chromosome combinations through karyogamy. A syncytial (or coenocytic, a synonym) eukaryote ancestor, or Coeca, would account for the observation that the process of eukaryotic chromosome separation is more conserved than the process of eukaryotic cell division. The first progeny of such a syncytial ancestor were likely equivalent to meiospores, released into the environment by the host's vesicle secretion machinery. The natural ability of archaea (the host) to fuse and recombine brought forth reciprocal recombination among fusing (syngamy and karyogamy) progeny-sex-in an ancestrally meiotic cell cycle, from which the simpler haploid and diploid mitotic cell cycles arose. The origin of eukaryotes was the origin of vertical lineage inheritance, and sex was required to keep vertically evolving lineages viable by rescuing the incipient eukaryotic lineage from Muller's ratchet. The origin of mitochondria was, in this view, the decisive incident that precipitated symbiosis-specific cell biological problems, the solutions to which were the salient features that distinguish eukaryotes from prokaryotes: A nuclear membrane, energetically affordable ATP-dependent protein-protein interactions in the cytosol, and a cell cycle involving reduction division and reciprocal recombination (sex).",
         "relevant":false
      },
      {
         "id":"10.1002/bies.201600074",
         "title":"Evolution of sex: Using experimental genomics to select among competing theories",
         "abstract":"Few topics have intrigued biologists as much as the evolution of sex. Understanding why sex persists despite its costs requires not just rigorous theoretical study, but also empirical data on related fundamental issues, including the nature of genetic variance for fitness, patterns of genetic interactions, and the dynamics of adaptation. The increasing feasibility of examining genomes in an experimental context is now shedding new light on these problems. Using this approach, McDonald et al. recently demonstrated that sex uncouples beneficial and deleterious mutations, allowing selection to proceed more effectively with sex than without. Here we discuss the insights provided by this study, along with other recent empirical work, in the context of the major theoretical models for the evolution of sex.",
         "relevant":false
      },
      {
         "id":"10.1021/acscombsci.8b00189",
         "title":"Early Years of High-Throughput Experimentation and Combinatorial Approaches in Catalysis and Materials Science",
         "abstract":"This is a report on the early years of combinatorial materials science and technology. High-throughput technologies (HTTs) are found in life- and materials-science laboratories. Although HTTs have long been the standard in life sciences in academia as well as in industry, HTTs in materials science have become the standard in industry but not in academia. In life science, successful drugs developed with HTTs have been reported, but there is no information on successful materials developed with HTTs that have made it to the market. Some initial development of HTTs in materials science is summarized, especially early applications of artificial intelligence. This outlook attempts to summarize the development of combinatorial materials sciences from the early years to today.",
         "relevant":false
      },
      {
         "id":"10.1002/advs.201900808",
         "title":"Data-Driven Materials Science: Status, Challenges, and Perspectives",
         "abstract":"Data-driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning-typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high-throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data-driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data-driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",
         "relevant":false
      },
      {
         "id":"10.1002/minf.201600047",
         "title":"Materials Informatics: Statistical Modeling in Material Science",
         "abstract":"Material informatics is engaged with the application of informatic principles to materials science in order to assist in the discovery and development of new materials. Central to the field is the application of data mining techniques and in particular machine learning approaches, often referred to as Quantitative Structure Activity Relationship (QSAR) modeling, to derive predictive models for a variety of materials-related \"activities\". Such models can accelerate the development of new materials with favorable properties and provide insight into the factors governing these properties. Here we provide a comparison between medicinal chemistry/drug design and materials-related QSAR modeling and highlight the importance of developing new, materials-specific descriptors. We survey some of the most recent QSAR models developed in materials science with focus on energetic materials and on solar cells. Finally we present new examples of material-informatic analyses of solar cells libraries produced from metal oxides using combinatorial material synthesis. Different analyses lead to interesting physical insights as well as to the design of new cells with potentially improved photovoltaic parameters.",
         "relevant":false
      },
      {
         "id":"10.1139/apnm-2018-0450",
         "title":"Total energy expenditure in elite open-water swimmers",
         "abstract":"This study aimed to examine the total energy expenditure (TEE) and physical activity level (PAL) of elite open-water swimmers. Our study group included 5 world-class competitive open-water swimmers. TEE was measured using the doubly labeled water method for 1 week. The TEE was 4549 ± 1185 kcal/day. The PAL was 3.22 ± 0.46. Our results may provide a reference to optimize energy requirement support.",
         "relevant":false
      },
      {
         "id":"10.3233/WOR-182796",
         "title":"Differences in posture kinematics between using a tablet, a laptop, and a desktop computer in sitting and in standing",
         "abstract":"Background: Alternative methods of accessing the internet and performing computing-related work tasks are becoming common, e.g., using tablets or standing workstations. Few studies examine postural differences while using these alternative methods.\n\nObjective: To assess neck and upper limb kinematics while using a tablet, laptop and desktop computer (sitting and standing).\n\nMethods: Differences in neck flexion/extension, lateral flexion, rotation; elbow flexion/extension; wrist flexion/extension, radial/ulnar deviation; and shoulder elevation in 30 participants were assessed in four conditions, three in sitting (tablet, laptop and desktop computer) and one in standing (desktop computer). Three-dimensional motion capture recorded posture variables during an editing task. Differences between variables were determined using one-way ANOVA with Bonferroni post-hoc tests.\n\nResults: Compared to the desktop (sitting), tablet and laptop use resulted in increased neck flexion (mean difference tablet 16.92°, 95% CI 12.79-21.04; laptop 10.92, 7.86-13.97, P < 0.001) and shoulder elevation (right; tablet 10.29, 5.27-15.11; laptop 7.36, 3.72-11.01, P < 0.001). There were no meaningful posture differences between the sitting and standing desktop.\n\nConclusions: These findings suggest that using a tablet or laptop may increase neck flexion, potentially increasing posture strain. Regular users of tablets/laptops should consider adjustments in their posture, however, further research is required to determine whether posture adjustments prevent or reduce musculoskeletal symptoms.",
         "relevant":false
      },
      {
         "id":30677807,
         "title":"Laptop computer-induced hyperpigmentation",
         "abstract":"A 25-year-old afebrile man presented with one year of worsening non-pruritic hyperpigmented non-blanchable reticulated patches and one erosion on his abdomen. He denied trauma, contact with new detergents, and recent travel. He was not taking medications and denied ever having similar skin findings. Further questioning revealed that he positioned his laptop computer directly on his abdomen for several hours every night. His progressive skin findings characterize erythema ab igne, which occurs after repetitive prolonged exposure to temperatures between 43 to 47 degrees Celsius. The hyperpigmentation can occur anywhere on unprotected skin and is an ongoing clinical problem in all demographics as heat sources evolve. Guided questioning of an unsuspecting patient can expedite diagnosis and prevent the development of erosions and ulcers, permanent skin discoloration, and even skin cancers.",
         "relevant":false
      },
      {
         "id":28144597,
         "title":"The Fundamental Reasons Why Laptop Computers should not be Used on Your Lap",
         "abstract":"As a tendency to use new technologies, gadgets such as laptop computers are becoming more popular among students, teachers, businessmen and office workers. Today laptops are a great tool for education and learning, work and personal multimedia. Millions of men, especially those in the reproductive age, are frequently using their laptop computers on the lap (thigh). Over the past several years, our lab has focused on the health effects of exposure to different sources of electromagnetic fields such as cellular phones, mobile base stations, mobile phone jammers, laptop computers, radars, dentistry cavitrons and Magnetic Resonance Imaging (MRI). Our own studies as well as the studies performed by other researchers indicate that using laptop computers on the lap adversely affects the male reproductive health. When it is placed on the lap, not only the heat from a laptop computer can warm men's scrotums, the electromagnetic fields generated by laptop's internal electronic circuits as well as the Wi-Fi Radiofrequency radiation hazards (in a Wi-Fi connected laptop) may decrease sperm quality. Furthermore, due to poor working posture, laptops should not be used on the lap for long hours.",
         "relevant":false
      },
      {
         "id":26396965,
         "title":"New Horizons in Enhancing the Proliferation and Differentiation of Neural Stem Cells Using Stimulatory Effects of the Short Time Exposure to Radiofrequency Radiation",
         "abstract":"Mobile phone use and wireless communication technology have grown explosively over the past decades. This rapid growth has caused widespread global concern about the potential detrimental effects of this technology on human health. Stem cells generate specialized cell types of the tissue in which they reside through normal differentiation pathways. Considering the undeniable importance of stem cells in modern medicine, numerous studies have been performed on the effects of ionizing and non-ionizing radiation on cellular processes such as: proliferation, differentiation, cell cycle and DNA repair processes. We have conducted extensive studies on beneficial (stimulatory) or detrimental biological effects of exposure to different sources of electromagnetic fields such as mobile phones, mobile phone base stations, mobile phone jammers, radar systems, magnetic resonance imaging (MRI) systems and dentistry cavitrons over the past years. In this article, recent studies on the biological effects of non-ionizing electromagnetic radiation in the range of radiofrequency (RF) on some important features of stem cells such as their proliferation and differentiation are reviewed. Studies reviewed in this paper indicate that the stimulatory or inhibitory effects of RF radiation on the proliferation and differentiation of stem cells depend on various factors such as the biological systems, experiment conditions, the frequency and intensity of RF and the duration of exposure.",
         "relevant":false
      },
      {
         "id":"10.1177/0956797614524581",
         "title":"The pen is mightier than the keyboard: advantages of longhand over laptop note taking",
         "abstract":"Taking notes on laptops rather than in longhand is increasingly common. Many researchers have suggested that laptop note taking is less effective than longhand note taking for learning. Prior studies have primarily focused on students' capacity for multitasking and distraction when using laptops. The present research suggests that even when laptops are used solely to take notes, they may still be impairing learning because their use results in shallower processing. In three studies, we found that students who took notes on laptops performed worse on conceptual questions than students who took notes longhand. We show that whereas taking more notes can be beneficial, laptop note takers' tendency to transcribe lectures verbatim rather than processing information and reframing it in their own words is detrimental to learning.",
         "relevant":false
      },
      {
         "id":"10.1016/j.tree.2011.09.016",
         "title":"The many costs of sex",
         "abstract":"Explaining the evolution of sex is challenging for biologists. A 'twofold cost' compared with asexual reproduction is often quoted. If a cost of this magnitude exists, the benefits of sex must be large for it to have evolved and be maintained. Focusing on benefits can be misleading, as this sidelines important questions about the cost of sex: what is the source of the twofold cost: males, genome dilution or both? Does the cost deviate from twofold? What other factors make sex costly? How should the costs of sex be empirically measured? The total cost of sex and how it varies in different contexts must be known to determine the benefits needed to account for the origin and maintenance of sex.",
         "relevant":false
      }
   ]
}